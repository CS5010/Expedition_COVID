{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f017fb00d9b849ffacf8e3d75545d6b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba2e4138179949e682eae8a4d2477110",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94c8788c68894931aa617a6756249e18",
              "IPY_MODEL_1b0cc4c9fabd48159a28f4d4e98c5088"
            ]
          }
        },
        "ba2e4138179949e682eae8a4d2477110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94c8788c68894931aa617a6756249e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_520c383d92fb4374a6ae834f84dd9e66",
            "_dom_classes": [],
            "description": "Predict loop:   3%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 396,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09dfb5208d004d0e83a8b633927c02e4"
          }
        },
        "1b0cc4c9fabd48159a28f4d4e98c5088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bdcaabc54eb34954ab3c994279548d2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/396 [00:07&lt;02:30,  2.54sequences/s, Call=13, TotalLoss=0.000759]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6f36177546b48448d179defff592b3a"
          }
        },
        "520c383d92fb4374a6ae834f84dd9e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09dfb5208d004d0e83a8b633927c02e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdcaabc54eb34954ab3c994279548d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6f36177546b48448d179defff592b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ-StzD1ZLEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dede261-0b31-4378-e4c1-801afcc108c0"
      },
      "source": [
        "# Set Runname\n",
        "RunName = 'CovidA21-LSTM4Analyze'\n",
        "RunComment = 'April 14 Covid 2021 Dataset; Old set of Properties; NO Validation; LSTM; Futures; 500 counties'\n",
        "\n",
        "startbold = \"\\033[1m\"\n",
        "resetfonts = \"\\033[0m\"\n",
        "startred = '\\033[31m'\n",
        "\n",
        "startpurple = '\\033[35m'\n",
        "startyellowbkg = '\\033[43m'\n",
        "\n",
        "!lscpu\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               85\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Stepping:            3\n",
            "CPU MHz:             2000.190\n",
            "BogoMIPS:            4000.38\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            1024K\n",
            "L3 cache:            39424K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Mon Jun  7 03:13:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ndvkB04ZZLE",
        "outputId": "665f42be-08f0-4b6e-bd2b-8b442df9755c"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tqdm.keras import TqdmCallback\n",
        "from tqdm import tnrange, notebook, tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import os\n",
        "import gc\n",
        "from csv import reader\n",
        "from csv import writer\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from textwrap import wrap\n",
        "import pandas as pd\n",
        "import io as io\n",
        "import string\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta,date,datetime\n",
        "\n",
        "!pip install cloudmesh-common -U\n",
        "from cloudmesh.common.StopWatch import StopWatch\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cloudmesh-common\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/b7/7ff2524917137a262c9809045ecce5f5bdd34b9123af315789d6d4f1c80b/cloudmesh_common-4.3.69-py2.py3-none-any.whl (81kB)\n",
            "\r\u001b[K     |████                            | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 18.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (0.8.9)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (2018.9)\n",
            "Collecting pyfiglet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/07/fcfdd7a2872f5b348953de35acce1544dab0c1e8368dca54279b1cde5c15/pyfiglet-0.8.post1-py2.py3-none-any.whl (865kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 16.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: humanize in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (0.5.1)\n",
            "Collecting python-hostlist\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/4f/f31dd4b4bf1a57a5c29599e1165d0df70dbdddcfa59a7c1d04ee2ff4ccbd/python-hostlist-1.21.tar.gz\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/04/377418ac1e530ce2a196b54c6552c018fdf1fe776718053efb1f216bffcd/simplejson-3.17.2-cp37-cp37m-manylinux2010_x86_64.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pathlib in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (1.0.1)\n",
            "Collecting oyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/37/aa/111610d8bf5b1bb7a295a048fc648cec346347a8b0be5881defd2d1b4a52/oyaml-1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->cloudmesh-common) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from oyaml->cloudmesh-common) (3.13)\n",
            "Building wheels for collected packages: python-hostlist\n",
            "  Building wheel for python-hostlist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-hostlist: filename=python_hostlist-1.21-cp37-none-any.whl size=38943 sha256=eb08687701058bb3a7675ee931ceb2c0c9fa7d278889e05063bf96cdc75e1932\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/5b/55/ddcf52288f0b10f4564ca1b2531594ff7ccc65f487ba8dc437\n",
            "Successfully built python-hostlist\n",
            "Installing collected packages: pyfiglet, colorama, python-hostlist, simplejson, oyaml, cloudmesh-common\n",
            "Successfully installed cloudmesh-common-4.3.69 colorama-0.4.4 oyaml-1.0 pyfiglet-0.8.post1 python-hostlist-1.21 simplejson-3.17.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6P2muCAV3CJ",
        "outputId": "1f641463-7a16-4c82-fd5c-6bf19acacab7"
      },
      "source": [
        "def wraptotext(textinput,size=None):\n",
        "  if size is None:\n",
        "    size = 120\n",
        "  textlist = wrap(textinput,size)\n",
        "  textresult = textlist[0]\n",
        "  for itext in range(1,len(textlist)):\n",
        "    textresult += '\\n'+textlist[itext]\n",
        "  return textresult\n",
        "\n",
        "def timenow():\n",
        "  now = datetime.now()\n",
        "  return now.strftime(\"%m/%d/%Y, %H:%M:%S\") + \" UTC\"\n",
        "\n",
        "def float32fromstrwithNaN(instr):\n",
        "  if instr == 'NaN':\n",
        "    return NaN\n",
        "  return np.float32(instr)\n",
        "\n",
        "def printexit(exitmessage):\n",
        "  print(exitmessage)\n",
        "  sys.exit()\n",
        "\n",
        "def strrnd(value):\n",
        "  return str(round(value,4))\n",
        "\n",
        "NaN = np.float32(\"NaN\")\n",
        "\n",
        "ReadJuly2020Covid = False\n",
        "ReadAugust2020Covid = False\n",
        "ReadJan2021Covid = False\n",
        "ReadApril2021Covid = False\n",
        "ScaleProperties = False\n",
        "ConvertDynamicPredictedQuantity = False\n",
        "ConvertDynamicProperties = True\n",
        "GenerateFutures = False\n",
        "GenerateSequences = False\n",
        "PredictionsfromInputs = False\n",
        "RereadMay2020 = False\n",
        "UseOLDCovariates = False\n",
        "Dropearlydata = 0\n",
        "NIHCovariates = False \n",
        "UseFutures = True\n",
        "Usedaystart = False \n",
        "PopulationNorm = False\n",
        "SymbolicWindows = False\n",
        "Hydrology = False\n",
        "Earthquake = False\n",
        "EarthquakeImagePlots = False\n",
        "AddSpecialstoSummedplots = False\n",
        "UseRealDatesonplots = False\n",
        "Dumpoutkeyplotsaspics = False\n",
        "OutputNetworkPictures = False\n",
        "CDSpecial = False\n",
        "NumpredbasicperTime = 2\n",
        "NumpredFuturedperTime = 2\n",
        "NumTimeSeriesCalculated = 0\n",
        "Dailyunit = 1\n",
        "TimeIntervalUnitName = 'Day'\n",
        "InitialDate = datetime(2000,1,1)\n",
        "NumberofTimeunits = 0\n",
        "Num_Time =0\n",
        "FinalDate = datetime(2000,1,1)\n",
        "GlobalTrainingLoss = 0.0\n",
        "GlobalValidationLoss = 0.0\n",
        "\n",
        "# Type of Testing\n",
        "LocationBasedValidation = False\n",
        "LocationValidationFraction = 0.0\n",
        "LocationTrainingfraction = 1.0\n",
        "RestartLocationBasedValidation = False\n",
        "\n",
        "global SeparateValandTrainingPlots\n",
        "SeparateValandTrainingPlots = True\n",
        "Plotsplitsize = -1 # if > 1 split time in plots\n",
        "\n",
        "GarbageCollect = True\n",
        "GarbageCollectionLimit = 0\n",
        "\n",
        "current_time = timenow()\n",
        "print(startbold + startred + current_time + ' ' +RunName + ' ' + RunComment + resetfonts)\n",
        "\n",
        "SubName = RunName[0:6]\n",
        "if SubName == 'BEST14' or SubName == 'BEST15' or SubName == 'BEST16':\n",
        "  UseOLDCovariates = False\n",
        "  ReadAugust2020Covid = True\n",
        "  ScaleProperties = True\n",
        "  ConvertDynamicPredictedQuantity = True\n",
        "  GenerateFutures = True\n",
        "  GenerateSequences = True\n",
        "  PredictionsfromInputs = True\n",
        "  NIHCovariates = True\n",
        "  ConvertDynamicProperties = True\n",
        "  Dropearlydata = 37\n",
        "  CDSpecial = True\n",
        "\n",
        "if SubName == 'CovidA':\n",
        "  UseOLDCovariates = False\n",
        "  ReadApril2021Covid = True\n",
        "  ScaleProperties = True\n",
        "  ConvertDynamicPredictedQuantity = True\n",
        "  GenerateFutures = True\n",
        "  UseFutures = True\n",
        "  GenerateSequences = True\n",
        "  PredictionsfromInputs = True\n",
        "  NIHCovariates = True\n",
        "  ConvertDynamicProperties = True\n",
        "  CDSpecial = True\n",
        "\n",
        "if SubName == 'C2021A' or SubName == 'C2021B':\n",
        "  UseOLDCovariates = False\n",
        "  ReadJan2021Covid = True\n",
        "  ScaleProperties = True\n",
        "  ConvertDynamicPredictedQuantity = True\n",
        "  GenerateFutures = True\n",
        "  GenerateSequences = True\n",
        "  PredictionsfromInputs = True\n",
        "  NIHCovariates = True\n",
        "  ConvertDynamicProperties = True\n",
        "  Dropearlydata = 0\n",
        "  CDSpecial = True\n",
        "\n",
        "\n",
        "if RunName == 'BEST10' or RunName == 'BEST13-10D' or RunName == 'BEST12-10' or RunName == 'BEST12-Test' or RunName == 'BEST13' or RunName == 'BEST13-10' or RunName == 'BEST13-10A' or RunName == 'BEST13-10C':\n",
        "  UseOLDCovariates = False\n",
        "  ReadAugust2020Covid = True\n",
        "  ScaleProperties = True\n",
        "  ConvertDynamicPredictedQuantity = True\n",
        "  GenerateFutures = True\n",
        "  GenerateSequences = True\n",
        "  PredictionsfromInputs = True\n",
        "  CDSpecial = True\n",
        "\n",
        "if RunName == 'BEST11' or RunName == 'BEST11A':\n",
        "  UseOLDCovariates = True\n",
        "  ReadAugust2020Covid = True\n",
        "  ScaleProperties = True\n",
        "  ConvertDynamicPredictedQuantity = True\n",
        "  GenerateFutures = True\n",
        "  GenerateSequences = True\n",
        "  PredictionsfromInputs = True\n",
        "  CDSpecial = True\n",
        "\n",
        "if RunName == 'BEST12':\n",
        "  UseOLDCovariates = True\n",
        "  RereadMay2020 = True\n",
        "  ReadAugust2020Covid = False\n",
        "  ScaleProperties = True\n",
        "  ConvertDynamicPredictedQuantity = True\n",
        "  GenerateFutures = True\n",
        "  GenerateSequences = True\n",
        "  PredictionsfromInputs = True\n",
        "  CDSpecial = True\n",
        "\n",
        "if RunName == 'BEST8' or RunName == 'BEST8A' or RunName == 'BEST12-LSTM-8':\n",
        "  ReadJuly2020Covid = True\n",
        "\n",
        "\n",
        "# read in science data \n",
        "COLABROOTDIR=\"/content/gdrive/My Drive/Colab Datasets\"\n",
        "os.environ[\"COLABROOTDIR\"] = COLABROOTDIR\n",
        "\n",
        "APPLDIR=os.path.join(COLABROOTDIR, \"COVIDJuly2020\")\n",
        "\n",
        "# Set up Checkpoints\n",
        "CHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + RunName + \"dir/\"\n",
        "try: \n",
        "    if not os.path.exists(CHECKPOINTDIR):\n",
        "      os.mkdir(CHECKPOINTDIR) \n",
        "except OSError as error: \n",
        "    print(error)\n",
        "print('Checkpoint set up in directory ' + CHECKPOINTDIR)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[31m06/07/2021, 03:14:14 UTC CovidA21-LSTM4Analyze April 14 Covid 2021 Dataset; Old set of Properties; NO Validation; LSTM; Futures; 500 counties\u001b[0m\n",
            "Checkpoint set up in directory /content/gdrive/My Drive/Colab Datasets/COVIDJuly2020/checkpoints/CovidA21-LSTM4Analyzedir/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7UmZjGVWWx3",
        "outputId": "9c784d41-1987-4213-fbc6-6164ffec5de2"
      },
      "source": [
        "if ReadApril2021Covid:\n",
        "  Dropearlydata = 40 # 3 more than needed by covariates so as to get \"round number of days\"\n",
        "  NIHCovariates = True\n",
        "  UseOLDCovariates = False\n",
        "  LengthFutures = 0\n",
        "\n",
        "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
        "  FinalDate = datetime(2021,4,14)\n",
        "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
        "  print(\"Total number of Days April 2021 Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
        "\n",
        "  DATASETDIR = APPLDIR + '/CovidApril14-2021'\n",
        "\n",
        "  CasesFile = DATASETDIR + '/' + 'US_daily_cumulative_cases_April14.csv'\n",
        "  DeathsFile = DATASETDIR + '/' + 'US_daily_cumulative_deaths_April14.csv'\n",
        "  LocationdataFile = DATASETDIR + '/Population.csv'\n",
        "  VotingdataFile = DATASETDIR + '/2020votes.csv'\n",
        "  AlaskaVotingdataFile = DATASETDIR + '/Alaskavoting2016.csv'\n",
        "\n",
        "  Nloc = 3142\n",
        "  NFIPS = 3142\n",
        "\n",
        "# Set up location information\n",
        "  Num_Time = NumberofTimeunits\n",
        "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
        "  Locationcolumns = [] # String version of FIPS\n",
        "  FIPSintegerlookup = {}\n",
        "  FIPSstringlookup = {}\n",
        "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
        "\n",
        "# Read in  cases Data into BasicInputTimeSeries\n",
        "  with open(CasesFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type Cases ' + Ftype)\n",
        "\n",
        "      iloc = 0    \n",
        "      for nextrow in csv_reader:\n",
        "        if (len(nextrow)< NumberofTimeunits + 1 + Dropearlydata):\n",
        "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "        # skip first entry\n",
        "        localfips = nextrow[0]\n",
        "        Locationcolumns.append(localfips)\n",
        "        Locationfips[iloc] = int(localfips)\n",
        "        FIPSintegerlookup[int(localfips)] = iloc\n",
        "        FIPSstringlookup[localfips] = iloc\n",
        "        for itime in range(0, NumberofTimeunits):\n",
        "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
        "          if Dropearlydata > 0:\n",
        "            floatlast = np.float(nextrow[Dropearlydata])\n",
        "            BasicInputTimeSeries[itime,iloc,0] = BasicInputTimeSeries[itime,iloc,0] - floatlast\n",
        "        iloc += 1\n",
        "# End Reading in cases data\n",
        "\n",
        "  if iloc != Nloc:\n",
        "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
        "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
        "\n",
        "# Read in deaths Data into BasicInputTimeSeries\n",
        "  with open(DeathsFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type Deaths ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        if (len(nextrow)<NumberofTimeunits + 1 + Dropearlydata):\n",
        "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "        localfips = nextrow[0]\n",
        "        if (Locationfips[iloc] != int(localfips)):\n",
        "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
        "        for itime in range(0, NumberofTimeunits):\n",
        "          BasicInputTimeSeries[itime,iloc,1] = nextrow[itime + 1 + Dropearlydata]\n",
        "          if Dropearlydata > 0:\n",
        "            floatlast = np.float(nextrow[Dropearlydata])\n",
        "            BasicInputTimeSeries[itime,iloc,1] = BasicInputTimeSeries[itime,iloc,1] - floatlast\n",
        "        iloc += 1\n",
        "# End Reading in deaths data\n",
        "\n",
        "  if iloc != Nloc:\n",
        "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
        "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
        "\n",
        "  Locationname = ['Empty'] * NFIPS\n",
        "  Locationstate = ['Empty'] * NFIPS\n",
        "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
        "  with open(LocationdataFile, 'r', encoding='latin1') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type Prop Data ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        localfips = int(nextrow[0])\n",
        "        if localfips in FIPSintegerlookup.keys():\n",
        "          jloc = FIPSintegerlookup[localfips]\n",
        "          Locationname[jloc] = nextrow[4]\n",
        "          Locationstate[jloc] = nextrow[3]\n",
        "          Locationpopulation[jloc] = int(nextrow[2])\n",
        "          iloc += 1 # just counting lines  \n",
        "        else:\n",
        "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
        "# END setting NFIPS location properties\n",
        "\n",
        "  DemVoting = np.full(NFIPS, -1.0, dtype=np.float32)\n",
        "  RepVoting = np.full(NFIPS, -1.0, dtype=np.float32)\n",
        "  with open(VotingdataFile, 'r', encoding='latin1') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'state_name':\n",
        "        printexit('EXIT: Wrong file type Voting Data ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        localfips = int(nextrow[1])\n",
        "        if localfips > 2900 and localfips < 2941: # Alaska not useful\n",
        "          continue\n",
        "        if localfips in FIPSintegerlookup.keys():\n",
        "          jloc = FIPSintegerlookup[localfips]\n",
        "          if DemVoting[jloc] >= 0.0:\n",
        "             printexit('EXIT Double Setting of FIPS ' +str(iloc) + ' ' + str(localfips))\n",
        "          DemVoting[jloc] = nextrow[8]\n",
        "          RepVoting[jloc] = nextrow[7]\n",
        "          iloc += 1 # just counting lines  \n",
        "        else:\n",
        "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
        "\n",
        "  with open(AlaskaVotingdataFile, 'r',encoding='utf-8-sig') as read_obj: # remove ufeff\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'SpecialAlaska':\n",
        "        printexit('EXIT: Wrong file type Alaska Voting Data ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        localfips = int(nextrow[1])\n",
        "        if localfips in FIPSintegerlookup.keys():\n",
        "          jloc = FIPSintegerlookup[localfips]\n",
        "          if DemVoting[jloc] >= 0.0:\n",
        "             printexit('EXIT Double Setting of FIPS ' +str(iloc) + ' ' + str(localfips))\n",
        "          DemVoting[jloc] = float(nextrow[2]) * 42.77/36.5\n",
        "          RepVoting[jloc] = float(nextrow[3]) * 52.83/51.3\n",
        "          iloc += 1 # just counting lines  \n",
        "        else:\n",
        "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))\n",
        "\n",
        "  for iloc in range(0,NFIPS):\n",
        "    if DemVoting[iloc] >= 0.0:\n",
        "      continue\n",
        "    print(str(iloc) + ' Missing Votes ' + str(Locationfips[iloc]) + ' ' + Locationname[iloc] + ' ' + Locationstate[iloc] + ' pop ' + str( Locationpopulation[iloc]))\n",
        "    DemVoting[iloc] = 0.5\n",
        "    RepVoting[iloc] = 0.5\n",
        "\n",
        "# Set Static Properties of the Nloc studied locations\n",
        "# Order is Static, Dynamic, Cases, Deaths\n",
        "# Voting added as 13th covariate\n",
        "  NpropperTimeDynamic = 13\n",
        "  NpropperTimeStatic = 0\n",
        "\n",
        "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
        "  InputPropertyNames = [] * NpropperTime\n",
        "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
        "\n",
        "\n",
        "\n",
        "# Read January 2021 Covid Data\n",
        "\n",
        "if ReadJan2021Covid:\n",
        "  Dropearlydata = 37\n",
        "  NIHCovariates = True\n",
        "  UseOLDCovariates = False\n",
        "\n",
        "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
        "  FinalDate = datetime(2021,1,26)\n",
        "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
        "  print(\"Total number of Days January 2021 Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
        "\n",
        "  DATASETDIR = APPLDIR + '/January2021'\n",
        "\n",
        "  CasesFile = DATASETDIR + '/' + 'US_daily_cumulative_cases.csv'\n",
        "  DeathsFile = DATASETDIR + '/' + 'US_daily_cumulative_deaths.csv'\n",
        "  LocationdataFile = DATASETDIR + '/Population.csv'\n",
        "\n",
        "  Nloc = 3142\n",
        "  NFIPS = 3142\n",
        "\n",
        "# Set up location information\n",
        "  Num_Time = NumberofTimeunits\n",
        "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
        "  Locationcolumns = [] # String version of FIPS\n",
        "  FIPSintegerlookup = {}\n",
        "  FIPSstringlookup = {}\n",
        "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
        "\n",
        "# Read in  cases Data into BasicInputTimeSeries\n",
        "  with open(CasesFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type Cases ' + Ftype)\n",
        "\n",
        "      iloc = 0    \n",
        "      for nextrow in csv_reader:\n",
        "        if (len(nextrow)< NumberofTimeunits + 1 + Dropearlydata):\n",
        "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "        # skip first entry\n",
        "        localfips = nextrow[0]\n",
        "        Locationcolumns.append(localfips)\n",
        "        Locationfips[iloc] = int(localfips)\n",
        "        FIPSintegerlookup[int(localfips)] = iloc\n",
        "        FIPSstringlookup[localfips] = iloc\n",
        "        for itime in range(0, NumberofTimeunits):\n",
        "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
        "          if Dropearlydata > 0:\n",
        "            floatlast = np.float(nextrow[Dropearlydata])\n",
        "            BasicInputTimeSeries[itime,iloc,0] = BasicInputTimeSeries[itime,iloc,0] - floatlast\n",
        "        iloc += 1\n",
        "# End Reading in cases data\n",
        "\n",
        "  if iloc != Nloc:\n",
        "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
        "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
        "\n",
        "# Read in deaths Data into BasicInputTimeSeries\n",
        "  with open(DeathsFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type Deaths ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        if (len(nextrow)<NumberofTimeunits + 1 + Dropearlydata):\n",
        "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "        localfips = nextrow[0]\n",
        "        if (Locationfips[iloc] != int(localfips)):\n",
        "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
        "        for itime in range(0, NumberofTimeunits):\n",
        "          BasicInputTimeSeries[itime,iloc,1] = nextrow[itime + 1 + Dropearlydata]\n",
        "          if Dropearlydata > 0:\n",
        "            floatlast = np.float(nextrow[Dropearlydata])\n",
        "            BasicInputTimeSeries[itime,iloc,1] = BasicInputTimeSeries[itime,iloc,1] - floatlast\n",
        "        iloc += 1\n",
        "# End Reading in deaths data\n",
        "\n",
        "  if iloc != Nloc:\n",
        "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
        "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
        "\n",
        "  Locationname = ['Empty'] * NFIPS\n",
        "  Locationstate = ['Empty'] * NFIPS\n",
        "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
        "  with open(LocationdataFile, 'r', encoding='latin1') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type Prop Data ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        localfips = int(nextrow[0])\n",
        "        if localfips in FIPSintegerlookup.keys():\n",
        "          jloc = FIPSintegerlookup[localfips]\n",
        "          Locationname[jloc] = nextrow[4]\n",
        "          Locationstate[jloc] = nextrow[3]\n",
        "          Locationpopulation[jloc] = int(nextrow[2])\n",
        "          iloc += 1 # just counting lines  \n",
        "        else:\n",
        "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
        "# END setting NFIPS location properties\n",
        "\n",
        "# Set Static Properties of the Nloc studied locations\n",
        "# Order is Static, Dynamic, Cases, Deaths\n",
        "  NpropperTimeDynamic = 12\n",
        "  NpropperTimeStatic = 0\n",
        "\n",
        "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
        "  InputPropertyNames = [' '] * NpropperTime\n",
        "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
        "\n",
        "\n",
        "\n",
        "# Finish this after NIH Covariate\n",
        "\n",
        "\n",
        "if ReadAugust2020Covid:\n",
        "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
        "  FinalDate = datetime(2020,8,13)\n",
        "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
        "  print(\"Total number of Days August Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
        "\n",
        "  DATASETDIR = APPLDIR +'/MidAugust2020Data'\n",
        "\n",
        "  CasesFile = DATASETDIR + '/' + 'covid-cases.csv'\n",
        "  DeathsFile = DATASETDIR + '/' + 'covid-deaths.csv'\n",
        "  CovariatesFile = DATASETDIR + '/' + 'PVI-31July2020.csv'\n",
        "  if RereadMay2020 or UseOLDCovariates:\n",
        "    CovariatesFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
        "  LocationdataFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
        "\n",
        "  Nloc = 314\n",
        "  NFIPS = 316\n",
        "\n",
        "if RereadMay2020:\n",
        "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
        "  FinalDate = datetime(2020,5,25)\n",
        "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
        "  print(\"Total number of Days May Dataset \" + str(NumberofTimeunits)  + ' Dropping at start ' + str(Dropearlydata))\n",
        "\n",
        "  DATASETDIR = APPLDIR +'/EndMay2020fromfiles'\n",
        "\n",
        "  CasesFile = DATASETDIR + '/' + 'Covid19-cases-110USCities.csv'\n",
        "  DeathsFile = DATASETDIR + '/' + 'Covid19-deaths-110USCities.csv'\n",
        "  CovariatesFile = DATASETDIR + '/' + 'PVI-31July2020.csv'\n",
        "  if UseOLDCovariates:\n",
        "    CovariatesFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
        "  LocationdataFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
        "\n",
        "  Nloc = 110\n",
        "  NFIPS = 112\n",
        "\n",
        "if ReadAugust2020Covid or RereadMay2020:\n",
        "\n",
        "# Set up location information\n",
        "  Num_Time = NumberofTimeunits\n",
        "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
        "  Locationcolumns = [] # String version of FIPS\n",
        "  FIPSintegerlookup = {}\n",
        "  FIPSstringlookup = {}\n",
        "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
        "\n",
        "# Read in  cases Data into BasicInputTimeSeries\n",
        "  with open(CasesFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      \n",
        "      for nextrow in csv_reader:\n",
        "        if (len(nextrow)!=NumberofTimeunits + 1 + Dropearlydata):\n",
        "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "        localfips = nextrow[0]\n",
        "        Locationcolumns.append(localfips)\n",
        "        Locationfips[iloc] = int(localfips)\n",
        "        FIPSintegerlookup[int(localfips)] = iloc\n",
        "        FIPSstringlookup[localfips] = iloc\n",
        "        for itime in range(0, NumberofTimeunits):\n",
        "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
        "        iloc += 1\n",
        "# End Reading in cases data\n",
        "\n",
        "  if iloc != Nloc:\n",
        "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
        "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
        "\n",
        "# Read in deaths Data into BasicInputTimeSeries\n",
        "  with open(DeathsFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        if (len(nextrow)!=NumberofTimeunits + 1 + Dropearlydata):\n",
        "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "        localfips = nextrow[0]\n",
        "        if (Locationfips[iloc] != int(localfips)):\n",
        "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
        "        for itime in range(0, NumberofTimeunits):\n",
        "          BasicInputTimeSeries[itime,iloc,1] = nextrow[itime + 1 + Dropearlydata]\n",
        "        iloc += 1\n",
        "# End Reading in deaths data\n",
        "\n",
        "  if iloc != Nloc:\n",
        "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
        "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
        "\n",
        "# START setting location properties -- there are NFIPS of these\n",
        "# NFIPS can be larger than Nloc. Any fips in studied group must be in fips property group\n",
        "# Add missing FIPS in 315 and not 314 set are 49057  and 49053\n",
        "# while 48203 is in 314 but not 315; 316 adds 48023\n",
        "  Locationfips[Nloc] = 49057\n",
        "  Locationfips[Nloc+1] = 49053\n",
        "  Locationcolumns.append(str(Locationfips[Nloc]))\n",
        "  FIPSintegerlookup[Locationfips[Nloc]] = Nloc\n",
        "  FIPSstringlookup[str(Locationfips[Nloc])] = Nloc\n",
        "  Locationcolumns.append(str(Locationfips[Nloc+1]))\n",
        "  FIPSintegerlookup[Locationfips[Nloc+1]] = Nloc+1\n",
        "  FIPSstringlookup[str(Locationfips[Nloc+1])] = Nloc+1\n",
        "\n",
        "  Locationname = ['Empty'] * NFIPS\n",
        "  Locationstate = ['Empty'] * NFIPS\n",
        "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
        "  with open(LocationdataFile, 'r') as read_obj:\n",
        "      csv_reader = reader(read_obj)\n",
        "      header = next(csv_reader)\n",
        "      Ftype = header[0]\n",
        "      if Ftype != 'FIPS':\n",
        "        printexit('EXIT: Wrong file type ' + Ftype)\n",
        "\n",
        "      iloc = 0\n",
        "      for nextrow in csv_reader:\n",
        "        localfips = int(nextrow[0])\n",
        "        if localfips in FIPSintegerlookup.keys():\n",
        "          jloc = FIPSintegerlookup[localfips]\n",
        "          Locationname[jloc] = nextrow[2]\n",
        "          Locationstate[jloc] = nextrow[1]\n",
        "          Locationpopulation[jloc] = int(nextrow[5])\n",
        "          iloc += 1 # just counting lines\n",
        "       \n",
        "  if iloc != Nloc+2:\n",
        "    printexit('EXIT Inconsistent old static data lengths ' +str(iloc) + ' ' + str(Nloc+2))\n",
        "  if 48203 in FIPSintegerlookup.keys():\n",
        "    iloc = FIPSintegerlookup[48203]\n",
        "    Locationname[iloc] = 'Harrison'\n",
        "    Locationstate[iloc] = 'Texas'\n",
        "    Locationpopulation[iloc] = 66553\n",
        "# END setting NFIPS location properties\n",
        "\n",
        "# Set Static Properties of the Nloc studied locations\n",
        "# Order is Static, Dynamic, Cases, Deaths\n",
        "  if NIHCovariates:\n",
        "      NpropperTimeDynamic = 11\n",
        "      NpropperTimeStatic = 0\n",
        "  else:\n",
        "      NpropperTimeDynamic = 0\n",
        "      NpropperTimeStatic = 12\n",
        "      if UseOLDCovariates:\n",
        "        NpropperTimeStatic = 26\n",
        "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
        "  InputPropertyNames = [] * NpropperTime\n",
        "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
        "\n",
        "  if not NIHCovariates:\n",
        "      BasicInputStaticProps = np.empty([Nloc,NpropperTimeStatic],dtype = np.float32)\n",
        "      \n",
        "      with open(CovariatesFile, 'r') as read_obj:\n",
        "          csv_reader = reader(read_obj)\n",
        "          header = next(csv_reader)\n",
        "          Ftype = header[0]\n",
        "          if Ftype != 'FIPS':\n",
        "              printexit('EXIT: Wrong file type ' + Ftype)\n",
        "          throwaway = 2\n",
        "          if UseOLDCovariates:\n",
        "            throwaway = 6\n",
        "          if ( len(header) != (throwaway+NpropperTimeStatic)):\n",
        "              printexit('EXIT: Incorrect property header length ' + str(len(header)) + ' ' +str(2+NpropperTimeStatic))\n",
        "          InputPropertyNames[:] = header[throwaway:]\n",
        "\n",
        "          iloc = 0\n",
        "          for nextrow in csv_reader:\n",
        "            if (len(nextrow)!= (throwaway+NpropperTimeStatic)):\n",
        "              printexit('EXIT: Incorrect row length ' + str(iloc) + ' ' + str(2+NpropperTimeStatic) + ' ' +str(len(nextrow)))\n",
        "            localfips = int(nextrow[0])\n",
        "            if not localfips in FIPSintegerlookup.keys():\n",
        "              continue\n",
        "    #           printexit('EXIT: Missing FIPS ' + str(localfips))\n",
        "            jloc = FIPSintegerlookup[localfips]\n",
        "            if jloc >= Nloc:\n",
        "              print('FIPS ' + str(localfips) + ' skipped in property read')\n",
        "              continue # skip this FIPS\n",
        "            BasicInputStaticProps[jloc,:] = np.asarray(nextrow[throwaway:], dtype=np.float32)\n",
        "            iloc += 1\n",
        "    # End Reading in Static Properties data\n",
        "\n",
        "      if iloc != Nloc:\n",
        "        printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
        "      print('Read Static Properties for locations ' + str(Nloc) + ' Properties ' + str(NpropperTimeStatic))\n",
        "\n",
        "  # August Covariates all intensive and no missing data\n",
        "  # May Coviates have intensive properties missing for Harrison TX\n",
        "      if UseOLDCovariates:\n",
        "        Property_is_Intensive[20] = False\n",
        "        Property_is_Intensive[21] = False\n",
        "        Property_is_Intensive[22] = False\n",
        "\n",
        "# Finish this after NIH Covariate"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Days April 2021 Dataset 409 Dropping at start 40\n",
            "Read Cases data locations 3142 Time Steps 409\n",
            "Read Deaths data locations 3142 Time Steps 409\n",
            "548 Missing Votes 15005 Kalawao County Hawaii pop 86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHLk_GQWXg6D",
        "outputId": "0dcd2231-fa62-45a1-f6fe-9659081edb44"
      },
      "source": [
        "#\n",
        "#Read and setup NIH Covariates August 2020 and January, April 2021 Data\n",
        "#new collection of time dependent covariates (even if constant).\n",
        "#cases and deaths and location property from previous data\n",
        "\n",
        "if NIHCovariates:\n",
        "  if ReadJan2021Covid:\n",
        "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Comorbidities.csv\",\"Demographics.csv\", \"Disease Spread.csv\", \n",
        "                     \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Intervention Testing.csv\", \"Mobility.csv\", \n",
        "                     \"Residential Density.csv\", \"Social Distancing.csv\",  \"Transmissible Cases.csv\"]\n",
        "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Demographics\", \"Disease Spread\", \n",
        "                 \"Health Disparities\", \"Hospital Beds\", \"Intervention Testing\", \"Mobility\", \"Residential Density\", \n",
        "                 \"Social Distancing\", \"Transmissible Cases\"]\n",
        "\n",
        "  elif ReadApril2021Covid:\n",
        "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Comorbidities.csv\",\"Demographics.csv\", \"Disease Spread.csv\", \n",
        "                     \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Mobility.csv\", \n",
        "                     \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\",\"NOFILE\"]\n",
        "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Demographics\", \"Disease Spread\", \n",
        "                 \"Health Disparities\", \"Hospital Beds\",  \"Mobility\", \"Residential Density\", \n",
        "                 \"Social Distancing\", \"Testing\",\"Transmissible Cases\",\"voting\"]\n",
        "  else:\n",
        "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Co-morbidities.csv\", \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Pop Demographics.csv\", \"Pop Mobility.csv\", \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\"]\n",
        "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\", \"Health Disparities\", \"Hospital Beds\", \"Pop Demographics\", \"Pop Mobility\", \"Residential Density\", \"Social Distancing\", \"Testing\", \"Transmissible Cases\"]\n",
        "  \n",
        "  NIHDATADIR = DATASETDIR + '/' \n",
        "  numberfiles = len(Propnames)\n",
        "  NpropperTimeStatic = 0\n",
        "  if NpropperTimeDynamic != numberfiles:\n",
        "    printexit('EXIT: Dynamic Properties set wrong ' + str(numberfiles) + ' ' + str(NpropperTimeDynamic))\n",
        "  DynamicPropertyTimeSeries = np.empty([Num_Time,Nloc,numberfiles],dtype = np.float32)\n",
        "  enddifference = NaN\n",
        "\n",
        "  for ifiles in range(0,numberfiles):\n",
        "    InputPropertyNames.append(Propnames[ifiles])\n",
        "    if Propfilenames[ifiles] == 'NOFILE': # Special case of Voting Data\n",
        "      for iloc in range(0,Nloc):\n",
        "        Demsize = DemVoting[iloc]\n",
        "        RepSize = RepVoting[iloc]\n",
        "        Votingcovariate = Demsize/(RepSize+Demsize)\n",
        "        DynamicPropertyTimeSeries[:,iloc,ifiles] = Votingcovariate\n",
        "      continue # over ifile loop\n",
        "\n",
        "    DynamicPropFile = NIHDATADIR + Propfilenames[ifiles]\n",
        "    if not (ReadJan2021Covid or ReadApril2021Covid):\n",
        "      DynamicPropFile = DATASETDIR + '/ThirdCovariates/' + Propfilenames[ifiles]\n",
        "\n",
        "    # Read in  Covariate Data into DynamicPropertyTimeSeries\n",
        "    with open(DynamicPropFile, 'r') as read_obj:\n",
        "        csv_reader = reader(read_obj)\n",
        "        header = next(csv_reader)\n",
        "        skip = 1\n",
        "        if ReadJan2021Covid or ReadApril2021Covid:\n",
        "          skip = 2\n",
        "          Ftype = header[0]\n",
        "          if Ftype != 'Name':\n",
        "            printexit('EXIT: Wrong file type ' + Ftype)\n",
        "        Ftype = header[skip-1]\n",
        "        if Ftype != 'FIPS':\n",
        "          printexit('EXIT: Wrong file type ' + Ftype)\n",
        "        # Check Date\n",
        "        hformat = '%m-%d-%Y'\n",
        "        if ReadJan2021Covid or ReadApril2021Covid:\n",
        "          hformat = '%Y-%m-%d'\n",
        "        firstdate = datetime.strptime(header[skip], hformat)\n",
        "        tdelta = (firstdate-InitialDate).days \n",
        "        if tdelta > 0:\n",
        "          printexit('Missing Covariate Data start -- adjust Dropearlydata ' + str(tdelta))\n",
        "        lastdate = datetime.strptime(header[len(header)-1], hformat)\n",
        "        enddifference1 = (FinalDate-lastdate).days\n",
        "        if math.isnan(enddifference):\n",
        "          enddifference = enddifference1\n",
        "          print('Missing days at the end ' + str(enddifference))\n",
        "        else:\n",
        "          if enddifference != enddifference1:\n",
        "            printexit('EXIT: Incorrect time length ' + Propnames[ifiles] + ' expected ' + str(enddifference) + ' actual ' +str(enddifference1))\n",
        "        iloc = 0\n",
        "        \n",
        "        for nextrow in csv_reader:\n",
        "          if (len(nextrow)!=NumberofTimeunits + skip -enddifference-tdelta):\n",
        "            printexit('EXIT: Incorrect row length ' + Propnames[ifiles] + ' Location ' + str(iloc) + ' ' +str(len(nextrow)))\n",
        "          localfips = nextrow[skip-1]\n",
        "          jloc = FIPSstringlookup[localfips] \n",
        "          for itime in range(0, NumberofTimeunits - enddifference):\n",
        "            DynamicPropertyTimeSeries[itime,jloc,ifiles] = nextrow[itime + skip - tdelta]\n",
        "        # Use previous week value for missing data at the end\n",
        "          for itime in range(NumberofTimeunits - enddifference, NumberofTimeunits):\n",
        "            DynamicPropertyTimeSeries[itime,jloc,ifiles] = DynamicPropertyTimeSeries[itime-7,jloc,ifiles]\n",
        "          iloc += 1\n",
        "# End Reading in dynamic property data\n",
        "\n",
        "    if iloc != Nloc:\n",
        "            printexit('EXIT Inconsistent location lengths ' + Propnames[ifiles] + str(iloc) + ' ' + str(Nloc))\n",
        "    print('Read ' + Propnames[ifiles] + ' data for locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time) + ' Days dropped at start ' + str(-tdelta))\n",
        "\n",
        "  if ReadApril2021Covid:\n",
        "    CovidPopulationCut = 0 # Use this if NumberCut = 0\n",
        "    NumberCut = 2642\n",
        "    uselocation = np.full(Nloc, True, dtype = np.bool)\n",
        "    if (CovidPopulationCut > 0) or (NumberCut > 0):\n",
        "      if NumberCut >0:\n",
        "        smalllocations = np.argsort(Locationpopulation)\n",
        "        for jloc in range(0,NumberCut):\n",
        "          uselocation[smalllocations[jloc]] = False\n",
        "        CovidPopulationCut = Locationpopulation[smalllocations[NumberCut]]\n",
        "      else:\n",
        "        NumberCut =0\n",
        "        for iloc in range(0,Nloc):\n",
        "          if Locationpopulation[iloc] < CovidPopulationCut:\n",
        "            uselocation[iloc] = False\n",
        "            NumberCut += 1\n",
        "      print(' Population Cut ' + str(CovidPopulationCut) + ' removes ' + str(NumberCut) + ' of ' + str(Nloc))\n",
        "    if(NumberCut > 0):\n",
        "      NewNloc = Nloc - NumberCut\n",
        "      NewNFIPS = NewNloc\n",
        "      NewLocationfips = np.empty(NewNFIPS, dtype=int) # integer version of FIPs\n",
        "      NewLocationcolumns = [] # String version of FIPS\n",
        "      NewFIPSintegerlookup = {}\n",
        "      NewFIPSstringlookup = {}\n",
        "      NewBasicInputTimeSeries = np.empty([Num_Time,NewNloc,2],dtype = np.float32)\n",
        "      NewLocationname = ['Empty'] * NewNFIPS\n",
        "      NewLocationstate = ['Empty'] * NewNFIPS\n",
        "      NewLocationpopulation = np.empty(NewNFIPS, dtype=int)\n",
        "      NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,numberfiles],dtype = np.float32) \n",
        "\n",
        "      Newiloc = 0\n",
        "      for iloc in range(0,Nloc):\n",
        "        if not uselocation[iloc]:\n",
        "          continue\n",
        "        NewBasicInputTimeSeries[:,Newiloc,:] = BasicInputTimeSeries[:,iloc,:]\n",
        "        NewDynamicPropertyTimeSeries[:,Newiloc,:] = DynamicPropertyTimeSeries[:,iloc,:]\n",
        "        localfips = Locationcolumns[iloc]\n",
        "        NewLocationcolumns.append(localfips)\n",
        "        NewLocationfips[Newiloc] = int(localfips)\n",
        "        NewFIPSintegerlookup[int(localfips)] = Newiloc\n",
        "        NewFIPSstringlookup[localfips] = Newiloc \n",
        "        NewLocationpopulation[Newiloc] = Locationpopulation[iloc]\n",
        "        NewLocationstate[Newiloc] = Locationstate[iloc]\n",
        "        NewLocationname[Newiloc] = Locationname[iloc]\n",
        "        Newiloc +=1\n",
        "\n",
        "      BasicInputTimeSeries = NewBasicInputTimeSeries\n",
        "      DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
        "      Locationname = NewLocationname\n",
        "      Locationstate = NewLocationstate\n",
        "      Locationpopulation = NewLocationpopulation\n",
        "      FIPSstringlookup = NewFIPSstringlookup\n",
        "      FIPSintegerlookup = NewFIPSintegerlookup\n",
        "      Locationcolumns = NewLocationcolumns\n",
        "      Locationfips = NewLocationfips\n",
        "      NFIPS = NewNFIPS\n",
        "      Nloc = NewNloc\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing days at the end 0\n",
            "Read Age Distribution data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Air Pollution data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Co-morbidities data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Demographics data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Disease Spread data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Health Disparities data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Hospital Beds data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Mobility data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Residential Density data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Social Distancing data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Testing data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            "Read Transmissible Cases data for locations 3142 Time Steps 409 Days dropped at start 3\n",
            " Population Cut 128331 removes 2642 of 3142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x2wLIc5X3L3",
        "outputId": "1122061a-848d-4dbb-948f-8378b4a20d44"
      },
      "source": [
        "# Convert Cumulative to Daily\n",
        "# Convert  cumulative to Daily. \n",
        "# Replace negative daily values by zero\n",
        "# remove daily to sqrt(daily)  and Then normalize maximum to 1\n",
        "if ConvertDynamicPredictedQuantity:\n",
        "  NewBasicInputTimeSeries = np.empty_like(BasicInputTimeSeries, dtype=np.float32)\n",
        "  Zeroversion = np.zeros_like(BasicInputTimeSeries, dtype=np.float32)\n",
        "  Rolleddata = np.roll(BasicInputTimeSeries, 1, axis=0)\n",
        "  Rolleddata[0,:,:] = Zeroversion[0,:,:]\n",
        "  NewBasicInputTimeSeries = np.maximum(np.subtract(BasicInputTimeSeries,Rolleddata),Zeroversion)\n",
        "  originalnumber = np.sum(BasicInputTimeSeries[NumberofTimeunits-1,:,:],axis=0)\n",
        "  newnumber = np.sum(NewBasicInputTimeSeries,axis=(0,1))\n",
        "  print('Original summed counts ' + str(originalnumber) + ' become ' + str(newnumber)+ ' Cases, Deaths')\n",
        "\n",
        "  BasicInputTimeSeries = NewBasicInputTimeSeries\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original summed counts [23553836.   416232.] become [23588870.   420675.] Cases, Deaths\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNLjATQ6X77g"
      },
      "source": [
        "#Static and Dynamic specials for COVID\n",
        "#except case where Romeo data read\n",
        "\n",
        "# Remove special status of Cases and Deaths\n",
        "if CDSpecial:\n",
        "\n",
        "  NewNpropperTimeDynamic = NpropperTimeDynamic + 2\n",
        "  NewNpropperTime = NpropperTimeStatic + NewNpropperTimeDynamic   \n",
        "\n",
        "  NewProperty_is_Intensive = np.full(NewNpropperTime, True, dtype = np.bool)\n",
        "  NewInputPropertyNames = []\n",
        "  NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,NewNpropperTimeDynamic],dtype = np.float32)\n",
        "\n",
        "  for casesdeaths in range(0,2):\n",
        "    NewDynamicPropertyTimeSeries[:,:,casesdeaths] = BasicInputTimeSeries[:,:,casesdeaths]\n",
        "  BasicInputTimeSeries = None\n",
        "\n",
        "  for iprop in range(0,NpropperTimeStatic):\n",
        "    NewInputPropertyNames.append(InputPropertyNames[iprop])\n",
        "    NewProperty_is_Intensive[iprop] = Property_is_Intensive[iprop]\n",
        "  NewProperty_is_Intensive[NpropperTimeStatic] = False\n",
        "  NewProperty_is_Intensive[NpropperTimeStatic+1] = False\n",
        "  NewInputPropertyNames.append('Cases')\n",
        "  NewInputPropertyNames.append('Deaths')\n",
        "  for ipropdynamic in range(0,NpropperTimeDynamic):\n",
        "    Newiprop = NpropperTimeStatic+2+ipropdynamic\n",
        "    iprop = NpropperTimeStatic+ipropdynamic\n",
        "    NewDynamicPropertyTimeSeries[:,:,Newiprop] = DynamicPropertyTimeSeries[:,:,iprop]\n",
        "    NewInputPropertyNames.append(InputPropertyNames[iprop])\n",
        "    NewProperty_is_Intensive[Newiprop] = Property_is_Intensive[iprop]\n",
        "  \n",
        "  NpropperTimeDynamic = NewNpropperTimeDynamic\n",
        "  NpropperTime = NewNpropperTime\n",
        "  DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
        "  InputPropertyNames = NewInputPropertyNames\n",
        "  Property_is_Intensive = NewProperty_is_Intensive\n",
        "\n",
        "\n",
        "# Execute under all COVID circumstances properties generated here\n",
        "if CDSpecial:\n",
        "  if NpropperTimeStatic > 0:\n",
        "    Num_Extensive = 0\n",
        "    for iprop in range(0,NpropperTimeStatic):\n",
        "      if not Property_is_Intensive[iprop]:\n",
        "        Num_Extensive +=1\n",
        "    print(startbold + startred + ' Number of Extensive parameters ' + str(Num_Extensive) + resetfonts)\n",
        "    for iprop in range(0,NpropperTimeStatic):\n",
        "      if not Property_is_Intensive[iprop]:\n",
        "        print(InputPropertyNames[iprop])\n",
        "\n",
        "    # Convert Extensive covariates to SQRT(Population normed)\n",
        "    # Replace negatives by mean of positives and zeroes\n",
        "    positivemean = np.zeros(NpropperTimeStatic, dtype = np.float32)\n",
        "    countvalidentries = np.zeros(NpropperTimeStatic, dtype = np.float32)\n",
        "    for iloc in range(0,Nloc):\n",
        "      for iprop in range(0,NpropperTimeStatic):\n",
        "        if not Property_is_Intensive[iprop]:\n",
        "          BasicInputStaticProps[iloc,iprop] = np.sqrt(BasicInputStaticProps[iloc,iprop]/Locationpopulation[iloc])\n",
        "        else:\n",
        "          if BasicInputStaticProps[iloc,iprop] >= 0:\n",
        "            positivemean[iprop] += BasicInputStaticProps[iloc,iprop]\n",
        "            countvalidentries[iprop] += 1.0\n",
        "\n",
        "    for iprop in range(0,NpropperTimeStatic):\n",
        "        if Property_is_Intensive[iprop]:\n",
        "          positivemean[iprop] /= countvalidentries[iprop]\n",
        "\n",
        "    for iloc in range(0,Nloc):\n",
        "      for iprop in range(0,NpropperTimeStatic):\n",
        "        if Property_is_Intensive[iprop]:\n",
        "          if BasicInputStaticProps[iloc,iprop] < 0:\n",
        "            BasicInputStaticProps[iloc,iprop] = positivemean[iprop]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O09JxXuqYEd7",
        "outputId": "f2fef287-ecdd-420d-ed00-ca4b8be38933"
      },
      "source": [
        "#Normalize All Static and Dynamic Properties\n",
        "#for Static Properties BasicInputStaticProps[Nloc,NpropperTimeStatic] converts to NormedInputStaticProps[Nloc,NpropperTimeStatic]\n",
        "def SetTakeroot(x,n):\n",
        "    if np.isnan(x):\n",
        "      return NaN   \n",
        "    if n == 3:\n",
        "      return np.cbrt(x)\n",
        "    elif n == 2:\n",
        "      if x <= 0.0:\n",
        "        return 0.0\n",
        "      return np.sqrt(x) \n",
        "    return x \n",
        "\n",
        "def DynamicPropertyScaling(InputTimeSeries):\n",
        "    Results = np.full(7, 0.0,dtype=np.float32)\n",
        "    Results[1] = np.nanmax(InputTimeSeries, axis = (0,1))\n",
        "    Results[0] = np.nanmin(InputTimeSeries, axis = (0,1))\n",
        "    Results[3] = np.nanmean(InputTimeSeries, axis = (0,1))\n",
        "    Results[4] = np.nanstd(InputTimeSeries, axis = (0,1))\n",
        "    Results[2] = np.reciprocal(np.subtract(Results[1],Results[0]))\n",
        "    Results[5] = np.multiply(Results[2],np.subtract(Results[3],Results[0]))\n",
        "    Results[6] = np.multiply(Results[2],Results[4])\n",
        "    return Results\n",
        "\n",
        "NpropperTimeMAX = NpropperTime + NumTimeSeriesCalculated  \n",
        "if ScaleProperties:\n",
        "  QuantityTakeroot = np.full(NpropperTimeMAX,1,dtype=np.int)\n",
        "  if CDSpecial:\n",
        "    QuantityTakeroot[NpropperTimeStatic] =2\n",
        "    QuantityTakeroot[NpropperTimeStatic+1] =2\n",
        "\n",
        "# Scale data by roots if requested\n",
        "  for iprop in range(0, NpropperTimeMAX):\n",
        "    if QuantityTakeroot[iprop] >= 2:\n",
        "      if iprop < NpropperTimeStatic:\n",
        "        for iloc in range(0,Nloc):\n",
        "          BasicInputStaticProps[iloc,iprop] = SetTakeroot(BasicInputStaticProps[iloc,iprop],QuantityTakeroot[iprop])\n",
        "      elif iprop < NpropperTime:\n",
        "        for itime in range(0,NumberofTimeunits):\n",
        "          for iloc in range(0,Nloc):\n",
        "            DynamicPropertyTimeSeries[itime,iloc,iprop-NpropperTimeStatic] = SetTakeroot(\n",
        "                DynamicPropertyTimeSeries[itime,iloc,iprop-NpropperTimeStatic],QuantityTakeroot[iprop])\n",
        "      else:\n",
        "        for itime in range(0,NumberofTimeunits):\n",
        "          for iloc in range(0,Nloc):\n",
        "            CalculatedTimeSeries[itime,iloc,iprop-NpropperTime] =SetTakeroot(\n",
        "                CalculatedTimeSeries[itime,iloc,iprop-NpropperTime],QuantityTakeroot[iprop])\n",
        "\n",
        "  QuantityStatisticsNames = ['Min','Max','Norm','Mean','Std','Normed Mean','Normed Std']\n",
        "  QuantityStatistics = np.zeros([NpropperTimeMAX,7], dtype=np.float32)\n",
        "  if NpropperTimeStatic > 0:  \n",
        "    print(BasicInputStaticProps.shape)\n",
        "    max_value = np.amax(BasicInputStaticProps, axis = 0)\n",
        "    min_value = np.amin(BasicInputStaticProps, axis = 0)\n",
        "    mean_value = np.mean(BasicInputStaticProps, axis = 0)\n",
        "    std_value = np.std(BasicInputStaticProps, axis = 0)\n",
        "    normval = np.reciprocal(np.subtract(max_value,min_value))\n",
        "    normed_mean = np.multiply(normval,np.subtract(mean_value,min_value))\n",
        "    normed_std = np.multiply(normval,std_value)\n",
        "    QuantityStatistics[0:NpropperTimeStatic,0] = min_value\n",
        "    QuantityStatistics[0:NpropperTimeStatic,1] = max_value\n",
        "    QuantityStatistics[0:NpropperTimeStatic,2] = normval\n",
        "    QuantityStatistics[0:NpropperTimeStatic,3] = mean_value\n",
        "    QuantityStatistics[0:NpropperTimeStatic,4] = std_value\n",
        "    QuantityStatistics[0:NpropperTimeStatic,5] = normed_mean\n",
        "    QuantityStatistics[0:NpropperTimeStatic,6] = normed_std\n",
        "\n",
        "    NormedInputStaticProps =np.empty_like(BasicInputStaticProps)\n",
        "    for iloc in range(0,Nloc):\n",
        "      NormedInputStaticProps[iloc,:] = np.multiply((BasicInputStaticProps[iloc,:] - min_value[:]),normval[:])\n",
        "\n",
        "  if (NpropperTimeDynamic > 0) or (NumTimeSeriesCalculated>0):\n",
        "    for iprop in range(NpropperTimeStatic,NpropperTimeStatic+NpropperTimeDynamic):\n",
        "      QuantityStatistics[iprop,:] = DynamicPropertyScaling(DynamicPropertyTimeSeries[:,:,iprop-NpropperTimeStatic])\n",
        "    for iprop in range(0,NumTimeSeriesCalculated):\n",
        "      QuantityStatistics[iprop+NpropperTime,:] = DynamicPropertyScaling(CalculatedTimeSeries[:,:,iprop]) \n",
        "\n",
        "    NormedDynamicPropertyTimeSeries = np.empty_like(DynamicPropertyTimeSeries)\n",
        "    for iprop in range(NpropperTimeStatic,NpropperTimeStatic+NpropperTimeDynamic):\n",
        "      NormedDynamicPropertyTimeSeries[:,:,iprop - NpropperTimeStatic] = np.multiply((DynamicPropertyTimeSeries[:,:,iprop - NpropperTimeStatic]\n",
        "                                                - QuantityStatistics[iprop,0]),QuantityStatistics[iprop,2])\n",
        "    \n",
        "    if NumTimeSeriesCalculated > 0:\n",
        "      NormedCalculatedTimeSeries = np.empty_like(CalculatedTimeSeries)\n",
        "      for iprop in range(NpropperTime,NpropperTimeMAX):\n",
        "        NormedCalculatedTimeSeries[:,:,iprop - NpropperTime] = np.multiply((CalculatedTimeSeries[:,:,iprop - NpropperTime]\n",
        "                                                - QuantityStatistics[iprop,0]),QuantityStatistics[iprop,2])\n",
        "      CalculatedTimeSeries = None\n",
        "  \n",
        "    BasicInputStaticProps = None\n",
        "    DynamicPropertyTimeSeries = None\n",
        "    print(startbold + \"Properties scaled\" +resetfonts)\n",
        "\n",
        "  line = 'Name   '\n",
        "  for propval in range (0,7):\n",
        "    line += QuantityStatisticsNames[propval] + '    '\n",
        "  print('\\n' + startbold +startpurple + line + resetfonts)\n",
        "  for iprop in range(0,NpropperTimeMAX):\n",
        "    if iprop == NpropperTimeStatic:\n",
        "      print('\\n')\n",
        "    line = startbold + startpurple + str(iprop) + ' ' + InputPropertyNames[iprop] + resetfonts  + ' Root ' + str(QuantityTakeroot[iprop])\n",
        "    for propval in range (0,7):\n",
        "      line += ' ' + str(round(QuantityStatistics[iprop,propval],3))\n",
        "    print(line)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mProperties scaled\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[35mName   Min    Max    Norm    Mean    Std    Normed Mean    Normed Std    \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[35m0 Cases\u001b[0m Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m1 Deaths\u001b[0m Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m2 Age Distribution\u001b[0m Root 1 0.248 1.0 1.33 0.507 0.091 0.345 0.121\n",
            "\u001b[1m\u001b[35m3 Air Pollution\u001b[0m Root 1 0.0 1.0 1.0 0.413 0.115 0.413 0.115\n",
            "\u001b[1m\u001b[35m4 Co-morbidities\u001b[0m Root 1 0.029 0.581 1.813 0.285 0.099 0.463 0.179\n",
            "\u001b[1m\u001b[35m5 Demographics\u001b[0m Root 1 0.189 1.0 1.233 0.503 0.112 0.388 0.139\n",
            "\u001b[1m\u001b[35m6 Disease Spread\u001b[0m Root 1 0.0 1.0 1.0 0.234 0.243 0.234 0.243\n",
            "\u001b[1m\u001b[35m7 Health Disparities\u001b[0m Root 1 0.004 0.848 1.184 0.264 0.158 0.308 0.187\n",
            "\u001b[1m\u001b[35m8 Hospital Beds\u001b[0m Root 1 0.0 1.0 1.0 0.456 0.086 0.456 0.086\n",
            "\u001b[1m\u001b[35m9 Mobility\u001b[0m Root 1 0.499 1.0 1.996 0.683 0.07 0.367 0.14\n",
            "\u001b[1m\u001b[35m10 Residential Density\u001b[0m Root 1 0.016 0.994 1.023 0.577 0.264 0.574 0.27\n",
            "\u001b[1m\u001b[35m11 Social Distancing\u001b[0m Root 1 0.0 1.0 1.0 0.904 0.166 0.904 0.166\n",
            "\u001b[1m\u001b[35m12 Testing\u001b[0m Root 1 0.0 1.0 1.0 0.569 0.228 0.569 0.228\n",
            "\u001b[1m\u001b[35m13 Transmissible Cases\u001b[0m Root 1 0.0 1.0 1.0 0.527 0.166 0.527 0.166\n",
            "\u001b[1m\u001b[35m14 voting\u001b[0m Root 1 0.144 0.945 1.249 0.514 0.148 0.462 0.185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNVJX74wYLA6"
      },
      "source": [
        "#Set up Futures\n",
        "#-- currently at daily level \n",
        "class Future:\n",
        "    def __init__(self, name, daystart = 0, days =[], wgt=1.0, classweight = 1.0):\n",
        "        self.name = name\n",
        "        self.days = np.array(days)\n",
        "        self.daystart = daystart\n",
        "        self.wgts = np.full_like(self.days,wgt,dtype=float)\n",
        "        self.size = len(self.days)\n",
        "        self.classweight = classweight\n",
        "\n",
        "LengthFutures = 0\n",
        "if GenerateFutures: # daystart overwritten\n",
        "  Secondday = Future('day2',daystart = 23, days=[2],classweight=1./14.)\n",
        "  Thirdday = Future('day3', daystart = 24, days=[3],classweight=1./14.)\n",
        "  Fourthday = Future('day4', daystart = 25, days=[4],classweight=1./14.)\n",
        "  Fifthday = Future('day5', daystart = 26, days=[5],classweight=1./14.)\n",
        "  Sixthday = Future('day6', daystart = 27, days=[6],classweight=1./14.)\n",
        "  Seventhday = Future('day7', daystart = 27, days=[7],classweight=1./14.)\n",
        "  day8 = Future('day8', daystart = 28, days=[8],classweight=1./14.)\n",
        "  day9 =  Future('day9', daystart = 29, days=[9],classweight=1./14.)\n",
        "  day10 =  Future('day10', daystart = 30, days=[10],classweight=1./14.)\n",
        "  day11 =  Future('day11', daystart = 31, days=[11],classweight=1./14.)\n",
        "  day12 =  Future('day12', daystart = 32, days=[12],classweight=1./14.)\n",
        "  day13 =  Future('day13', daystart = 33, days=[13],classweight=1./14.)\n",
        "  day14 =  Future('day14', daystart = 34, days=[14],classweight=1./14.)\n",
        "  day15 =  Future('day15', daystart = 35, days=[15],classweight=1./14.)\n",
        "  # Secondweek = Future('week2', daystart= 19, days=[9,10,11,12,13,14,15],wgt=1./7.,classweight=0.25)\n",
        "  # Thirdweek = Future('week3', daystart= 26, days=[16,17,18,19,20,21,22],wgt=1./7.,classweight=0.25)\n",
        "  # Fourthweek = Future('week4', daystart = 33, days=[23,24,25,26,27,28,29],wgt=1./7.,classweight=0.25)\n",
        "  # Fifthweek = Future('week5', daystart = 40, days=[30,31,32,33,34,35,36],wgt=1./7.,classweight=0.25)\n",
        "  \n",
        "  Futures = [ Secondday,Thirdday,Fourthday,Fifthday,Sixthday,Seventhday,day8,day9,day10,day11,day12,day13,day14,day15]\n",
        "  LengthFutures = len(Futures)\n",
        "  Futuresmaxday = 0\n",
        "  Futuresmaxweek = 0\n",
        "  for i in range(0,LengthFutures):\n",
        "      j = len(Futures[i].days)\n",
        "      if j == 1:\n",
        "          Futuresmaxday = max(Futuresmaxday, Futures[i].days[0])\n",
        "      else:\n",
        "          Futuresmaxweek = max(Futuresmaxweek, Futures[i].days[j-1])\n",
        "      Futures[i].daystart -= Dropearlydata\n",
        "      if Futures[i].daystart < 0: Futures[i].daystart = 0\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf-vwFNFYZV7"
      },
      "source": [
        "# Set Requested Properties Predictions Encodings\n",
        "\n",
        "if ReadApril2021Covid:\n",
        "  InputSource = ['Dynamic']*15\n",
        "  InputSourceNumber = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
        "\n",
        "  PredSource = ['Dynamic','Dynamic']\n",
        "  PredSourceNumber = [0,1]\n",
        "  FuturedPred = [1,1]\n",
        "\n",
        "  # Encodings\n",
        "  PropTypes = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
        "  PropValues = [0, 0, 1, 2, 3,4, 0]\n",
        "\n",
        "  PredTypes = Types = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
        "  PredValues = [0, 0, 1, 2, 3,4, 0]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOF4pau_Yjcn"
      },
      "source": [
        "# Choose Input and Predicted Quantities\n",
        "\n",
        "if len(InputSource) != len(InputSourceNumber):\n",
        "  printexit(' Inconsistent Source Lengths ' + str(len(InputSource)) + str(len(InputSourceNumber)) )\n",
        "if len(PredSource) != len(PredSourceNumber):\n",
        "  printexit(' Inconsistent Rediction Lengths ' + str(len(PredSource)) + str(len(PredSourceNumber)) )\n",
        "\n",
        "# Executed by all even if GenerateFutures false except for direct Romeo data\n",
        "if (not ReadJuly2020Covid) and not (ReadJan2021Covid) and not (ReadApril2021Covid):\n",
        "  if not UseFutures:\n",
        "      LengthFutures = 0\n",
        "  print(startbold + \"Number of Futures -- separate for each regular prediction \" +str(LengthFutures) + resetfonts)\n",
        "  Usedaystart = False\n",
        "\n",
        "if len(PredSource) > 0: # set up Predictions\n",
        "  NumpredbasicperTime = len(PredSource)\n",
        "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n",
        "  NumpredFuturedperTime = 0\n",
        "  NumpredfromInputsperTime = 0\n",
        "  for ipred in range(0,len(PredSource)):\n",
        "    if PredSource[ipred] == 'Dynamic':\n",
        "      NumpredfromInputsperTime += 1\n",
        "  countinputs = 0\n",
        "  countcalcs = 0\n",
        "  for ipred in range(0,len(PredSource)):\n",
        "    if not(PredSource[ipred] == 'Dynamic' or PredSource[ipred] == 'Calc'):\n",
        "      printexit('Illegal Prediction ' + str(ipred) + ' ' + PredSource[ipred])\n",
        "    if PredSource[ipred] == 'Dynamic':\n",
        "      countinputs += 1 \n",
        "    else:\n",
        "      countcalcs += 1\n",
        "    if FuturedPred[ipred] >= 0:\n",
        "      if LengthFutures > 0:\n",
        "        FuturedPred[ipred] = NumpredFuturedperTime\n",
        "        FuturedPointer[ipred] = NumpredFuturedperTime\n",
        "        NumpredFuturedperTime += 1\n",
        "      else:\n",
        "        FuturedPred[ipred] = -1\n",
        "\n",
        "else: # Set defaults\n",
        "  NumpredfromInputsperTime = NumpredFuturedperTime\n",
        "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n",
        "  PredSource =[]\n",
        "  PredSourceNumber = []\n",
        "  FuturedPred =[]\n",
        "  futurepos = 0\n",
        "  for ipred in range(0,NumpredFuturedperTime): \n",
        "    PredSource.append('Dynamic')\n",
        "    PredSourceNumber.append(ipred)\n",
        "    futured = -1\n",
        "    if LengthFutures > 0:\n",
        "      futured = futurepos\n",
        "      FuturedPointer[ipred] = futurepos\n",
        "      futurepos += 1\n",
        "    FuturedPred.append(futured)\n",
        "  for ipred in range(0,NumTimeSeriesCalculated):\n",
        "    PredSource.append('Calc')\n",
        "    PredSourceNumber.append(ipred)\n",
        "    FuturedPred.append(-1) \n",
        "  print('Number of Predictions ' + str(len(PredSource)))   \n",
        "\n",
        "\n",
        "PropertyNameIndex = np.empty(NpropperTime, dtype = np.int32)\n",
        "PropertyAverageValuesPointer = np.empty(NpropperTime, dtype = np.int32)\n",
        "for iprop in range(0,NpropperTime):\n",
        "  PropertyNameIndex[iprop] = iprop # names\n",
        "  PropertyAverageValuesPointer[iprop] = iprop # normalizations\n",
        "\n",
        "# Reset Source -- if OK as read don't set InputSource InputSourceNumber\n",
        "# Reset NormedDynamicPropertyTimeSeries and NormedInputStaticProps\n",
        "# Reset NpropperTime = NpropperTimeStatic + NpropperTimeDynamic\n",
        "if len(InputSource) > 0: # Reset Input Source\n",
        "  NewNpropperTimeStatic = 0\n",
        "  NewNpropperTimeDynamic = 0\n",
        "  for isource in range(0,len(InputSource)):\n",
        "    if InputSource[isource] == 'Static':\n",
        "      NewNpropperTimeStatic += 1\n",
        "    if InputSource[isource] == 'Dynamic':\n",
        "      NewNpropperTimeDynamic += 1\n",
        "  NewNormedDynamicPropertyTimeSeries = np.empty([Num_Time,Nloc,NewNpropperTimeDynamic],dtype = np.float32)  \n",
        "  NewNormedInputStaticProps = np.empty([Nloc,NewNpropperTimeStatic],dtype = np.float32)\n",
        "  NewNpropperTime = NewNpropperTimeStatic + NewNpropperTimeDynamic\n",
        "  NewPropertyNameIndex = np.empty(NewNpropperTime, dtype = np.int32)\n",
        "  NewPropertyAverageValuesPointer = np.empty(NewNpropperTime, dtype = np.int32)\n",
        "  countstatic = 0\n",
        "  countdynamic = 0\n",
        "  for isource in range(0,len(InputSource)):\n",
        "    if InputSource[isource] == 'Static':\n",
        "      OldstaticNumber = InputSourceNumber[isource]\n",
        "      NewNormedInputStaticProps[:,countstatic] = NormedInputStaticProps[:,OldstaticNumber]\n",
        "      NewPropertyNameIndex[countstatic] = PropertyNameIndex[OldstaticNumber]\n",
        "      NewPropertyAverageValuesPointer[countstatic] = PropertyAverageValuesPointer[OldstaticNumber]\n",
        "      countstatic += 1\n",
        "\n",
        "    elif InputSource[isource] == 'Dynamic':\n",
        "      OlddynamicNumber =InputSourceNumber[isource]\n",
        "      NewNormedDynamicPropertyTimeSeries[:,:,countdynamic] = NormedDynamicPropertyTimeSeries[:,:,OlddynamicNumber]\n",
        "      NewPropertyNameIndex[countdynamic+NewNpropperTimeStatic] = PropertyNameIndex[OlddynamicNumber+NpropperTimeStatic]\n",
        "      NewPropertyAverageValuesPointer[countdynamic+NewNpropperTimeStatic] = PropertyAverageValuesPointer[OlddynamicNumber+NpropperTimeStatic]\n",
        "      countdynamic += 1\n",
        "    \n",
        "    else:\n",
        "     printexit('Illegal Property ' + str(isource) + ' ' + InputSource[isource]) \n",
        "\n",
        "else: # pretend data altered\n",
        "  NewPropertyNameIndex = PropertyNameIndex\n",
        "  NewPropertyAverageValuesPointer = PropertyAverageValuesPointer\n",
        "  NewNpropperTime = NpropperTime\n",
        "  NewNpropperTimeStatic = NpropperTimeStatic\n",
        "  NewNpropperTimeDynamic = NpropperTimeDynamic\n",
        "\n",
        "  NewNormedInputStaticProps = NormedInputStaticProps\n",
        "  NewNormedDynamicPropertyTimeSeries = NormedDynamicPropertyTimeSeries"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6iZpP5jYpNv",
        "outputId": "793d45a6-4e5c-4bfe-8801-46f0a645fb7f"
      },
      "source": [
        "# Order of Predictions *****************************\n",
        "# Basic \"futured\" Predictions from property dynamic arrays\n",
        "# Additional predictions without futures and NOT in property arrays including Calculated time series\n",
        "# LengthFutures predictions for first NumpredFuturedperTime predictions\n",
        "# Special predictions (temporal, positional) added later\n",
        "NpredperTime = NumpredbasicperTime + NumpredFuturedperTime*LengthFutures\n",
        "Npredperseq = NpredperTime\n",
        "Predictionbasicname = [' '] * NumpredbasicperTime\n",
        "for ipred in range(0,NumpredbasicperTime):\n",
        "  if PredSource[ipred] == 'Dynamic':\n",
        "    Predictionbasicname[ipred] = InputPropertyNames[PredSourceNumber[ipred]+NpropperTimeStatic]\n",
        "  else:\n",
        "    Predictionbasicname[ipred]= NamespredCalculated[PredSourceNumber[ipred]]\n",
        "\n",
        "TotalFutures = 0\n",
        "if NumpredFuturedperTime <= 0:\n",
        "  GenerateFutures = False\n",
        "if GenerateFutures:\n",
        "  TotalFutures = NumpredFuturedperTime * LengthFutures\n",
        "print(startbold + 'Predictions Total ' + str(Npredperseq) + ' Basic ' + str(NumpredbasicperTime) + ' Of which futured are '\n",
        "  + str(NumpredFuturedperTime) + ' Giving number explicit futures ' + str(TotalFutures) + resetfonts )\n",
        "Predictionname = [' '] * Npredperseq\n",
        "Predictionnametype = [' '] * Npredperseq\n",
        "Predictionoldvalue = np.empty(Npredperseq, dtype=int)\n",
        "Predictionnewvalue = np.empty(Npredperseq, dtype=int)\n",
        "Predictionday = np.empty(Npredperseq, dtype=int)\n",
        "PredictionAverageValuesPointer = np.empty(Npredperseq, dtype=int)\n",
        "Predictionwgt = [1.0] * Npredperseq\n",
        "for ipred in range(0,NumpredbasicperTime):\n",
        "  Predictionnametype[ipred] = PredSource[ipred]\n",
        "  Predictionoldvalue[ipred] = PredSourceNumber[ipred]\n",
        "  Predictionnewvalue[ipred] = ipred\n",
        "  if PredSource[ipred] == 'Dynamic':\n",
        "    PredictionAverageValuesPointer[ipred] = NpropperTimeStatic + Predictionoldvalue[ipred]\n",
        "  else:\n",
        "    PredictionAverageValuesPointer[ipred] = NpropperTime + PredSourceNumber[ipred]\n",
        "  Predictionwgt[ipred] = 1.0\n",
        "  Predictionday[ipred] = 1\n",
        "  extrastring =''\n",
        "  Predictionname[ipred] = 'Next ' + Predictionbasicname[ipred]\n",
        "  if FuturedPred[ipred] >= 0:\n",
        "    extrastring = ' Explicit Futures Added '   \n",
        "  print(str(ipred)+  ' Internal Property # ' + str(PredictionAverageValuesPointer[ipred]) + ' ' + Predictionname[ipred]\n",
        "      + ' Weight ' + str(round(Predictionwgt[ipred],3)) + ' Day ' + str(Predictionday[ipred]) + extrastring )\n",
        "\n",
        "for ifuture in range(0,LengthFutures):\n",
        "  for ipred in range(0,NumpredbasicperTime):\n",
        "    if FuturedPred[ipred] >= 0:\n",
        "      FuturedPosition = NumpredbasicperTime + NumpredFuturedperTime*ifuture + FuturedPred[ipred]\n",
        "      Predictionname[FuturedPosition] = Predictionbasicname[ipred] + Futures[ifuture].name\n",
        "      Predictionday[FuturedPosition] = Futures[ifuture].days[0]\n",
        "      Predictionwgt[FuturedPosition] = Futures[ifuture].classweight\n",
        "      Predictionnametype[FuturedPosition] = Predictionnametype[ipred]\n",
        "      Predictionoldvalue[FuturedPosition] = Predictionoldvalue[ipred]\n",
        "      Predictionnewvalue[FuturedPosition] = Predictionnewvalue[ipred]\n",
        "      PredictionAverageValuesPointer[FuturedPosition] = NpropperTimeStatic + PredictionAverageValuesPointer[ipred]\n",
        "      print(str(iprop)+  ' Internal Property # ' + str(PredictionAverageValuesPointer[FuturedPosition]) + ' ' + \n",
        "        Predictionname[FuturedPosition] + ' Weight ' + str(round(Predictionwgt[FuturedPosition],3))\n",
        "         + ' Day ' + str(Predictionday[FuturedPosition]) + ' This is Explicit Future' )\n",
        "\n",
        "Predictionnamelookup = {}\n",
        "print(startbold + '\\nBasic Predicted Quantities' + resetfonts)\n",
        "for i in range(0,Npredperseq):\n",
        "  Predictionnamelookup[Predictionname[i]] = i\n",
        "\n",
        "  iprop = Predictionnewvalue[i]\n",
        "  line = Predictionbasicname[iprop]\n",
        "  line += ' Weight ' + str(round(Predictionwgt[i],4))\n",
        "  if (iprop < NumpredFuturedperTime) or (iprop >= NumpredbasicperTime):\n",
        "    line += ' Day= ' + str(Predictionday[i])\n",
        "    line += ' Name ' + Predictionname[i]\n",
        "  print(line)\n",
        "\n",
        "  # Note that only Predictionwgt and Predictionname defined for later addons"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mPredictions Total 30 Basic 2 Of which futured are 2 Giving number explicit futures 28\u001b[0m\n",
            "0 Internal Property # 0 Next Cases Weight 1.0 Day 1 Explicit Futures Added \n",
            "1 Internal Property # 1 Next Deaths Weight 1.0 Day 1 Explicit Futures Added \n",
            "14 Internal Property # 0 Casesday2 Weight 0.071 Day 2 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday2 Weight 0.071 Day 2 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday3 Weight 0.071 Day 3 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday3 Weight 0.071 Day 3 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday4 Weight 0.071 Day 4 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday4 Weight 0.071 Day 4 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday5 Weight 0.071 Day 5 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday5 Weight 0.071 Day 5 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday6 Weight 0.071 Day 6 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday6 Weight 0.071 Day 6 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday7 Weight 0.071 Day 7 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday7 Weight 0.071 Day 7 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday8 Weight 0.071 Day 8 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday8 Weight 0.071 Day 8 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday9 Weight 0.071 Day 9 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday9 Weight 0.071 Day 9 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday10 Weight 0.071 Day 10 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday10 Weight 0.071 Day 10 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday11 Weight 0.071 Day 11 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday11 Weight 0.071 Day 11 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday12 Weight 0.071 Day 12 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday12 Weight 0.071 Day 12 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday13 Weight 0.071 Day 13 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday13 Weight 0.071 Day 13 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday14 Weight 0.071 Day 14 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday14 Weight 0.071 Day 14 This is Explicit Future\n",
            "14 Internal Property # 0 Casesday15 Weight 0.071 Day 15 This is Explicit Future\n",
            "14 Internal Property # 1 Deathsday15 Weight 0.071 Day 15 This is Explicit Future\n",
            "\u001b[1m\n",
            "Basic Predicted Quantities\u001b[0m\n",
            "Cases Weight 1.0 Day= 1 Name Next Cases\n",
            "Deaths Weight 1.0 Day= 1 Name Next Deaths\n",
            "Cases Weight 0.0714 Day= 2 Name Casesday2\n",
            "Deaths Weight 0.0714 Day= 2 Name Deathsday2\n",
            "Cases Weight 0.0714 Day= 3 Name Casesday3\n",
            "Deaths Weight 0.0714 Day= 3 Name Deathsday3\n",
            "Cases Weight 0.0714 Day= 4 Name Casesday4\n",
            "Deaths Weight 0.0714 Day= 4 Name Deathsday4\n",
            "Cases Weight 0.0714 Day= 5 Name Casesday5\n",
            "Deaths Weight 0.0714 Day= 5 Name Deathsday5\n",
            "Cases Weight 0.0714 Day= 6 Name Casesday6\n",
            "Deaths Weight 0.0714 Day= 6 Name Deathsday6\n",
            "Cases Weight 0.0714 Day= 7 Name Casesday7\n",
            "Deaths Weight 0.0714 Day= 7 Name Deathsday7\n",
            "Cases Weight 0.0714 Day= 8 Name Casesday8\n",
            "Deaths Weight 0.0714 Day= 8 Name Deathsday8\n",
            "Cases Weight 0.0714 Day= 9 Name Casesday9\n",
            "Deaths Weight 0.0714 Day= 9 Name Deathsday9\n",
            "Cases Weight 0.0714 Day= 10 Name Casesday10\n",
            "Deaths Weight 0.0714 Day= 10 Name Deathsday10\n",
            "Cases Weight 0.0714 Day= 11 Name Casesday11\n",
            "Deaths Weight 0.0714 Day= 11 Name Deathsday11\n",
            "Cases Weight 0.0714 Day= 12 Name Casesday12\n",
            "Deaths Weight 0.0714 Day= 12 Name Deathsday12\n",
            "Cases Weight 0.0714 Day= 13 Name Casesday13\n",
            "Deaths Weight 0.0714 Day= 13 Name Deathsday13\n",
            "Cases Weight 0.0714 Day= 14 Name Casesday14\n",
            "Deaths Weight 0.0714 Day= 14 Name Deathsday14\n",
            "Cases Weight 0.0714 Day= 15 Name Casesday15\n",
            "Deaths Weight 0.0714 Day= 15 Name Deathsday15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZVi9RqMYt0k",
        "outputId": "a39b95d8-5bb9-4ef7-b383-dce7c2cd6886"
      },
      "source": [
        "if PredictionsfromInputs:\n",
        "  InputPredictionsbyTime = np.zeros([Num_Time, Nloc, Npredperseq], dtype = np.float32)\n",
        "  for ipred in range (0,NumpredbasicperTime):\n",
        "    if Predictionnametype[ipred] == 'Dynamic':\n",
        "      InputPredictionsbyTime[:,:,ipred] = NormedDynamicPropertyTimeSeries[:,:,Predictionoldvalue[ipred]]\n",
        "    else:\n",
        "      InputPredictionsbyTime[:,:,ipred] = NormedCalculatedTimeSeries[:,:,Predictionoldvalue[ipred]]\n",
        "\n",
        "  # Add Futures based on Futured properties\n",
        "  if LengthFutures > 0:\n",
        "    NaNall = np.full([Nloc],NaN,dtype = np.float32)\n",
        "    daystartveto = 0\n",
        "    atendveto = 0\n",
        "    allok = NumpredbasicperTime \n",
        "    for ifuture in range(0,LengthFutures):\n",
        "      for itime in range(0,Num_Time):\n",
        "        ActualTime = itime+Futures[ifuture].days[0]-1\n",
        "        if ActualTime >= Num_Time:\n",
        "          for ipred in range (0,NumpredbasicperTime):\n",
        "            Putithere = FuturedPred[ipred]\n",
        "            if Putithere >=0:\n",
        "              InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] = NaNall\n",
        "          atendveto +=1\n",
        "        elif Usedaystart and (itime < Futures[ifuture].daystart):\n",
        "          for ipred in range (0,NumpredbasicperTime):\n",
        "            Putithere = FuturedPred[ipred]\n",
        "            if Putithere >=0:\n",
        "              InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] = NaNall \n",
        "          daystartveto +=1     \n",
        "        else:\n",
        "          for ipred in range (0,NumpredbasicperTime):\n",
        "            Putithere = FuturedPred[ipred]\n",
        "            if Putithere >=0:\n",
        "              if Predictionnametype[ipred] == 'Dynamic':\n",
        "                InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] \\\n",
        "                  = NormedDynamicPropertyTimeSeries[ActualTime,:,Predictionoldvalue[ipred]]\n",
        "              else:\n",
        "                InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] \\\n",
        "                  = NormedCalculatedTimeSeries[ActualTime,:,Predictionoldvalue[ipred]]\n",
        "          allok += NumpredFuturedperTime\n",
        "    print(startbold + 'Futures Added: Predictions set from inputs OK ' +str(allok) + \n",
        "          ' Veto at end ' + str(atendveto) +  ' Veto at start ' + str(daystartveto) + ' Times number of locations' + resetfonts)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mFutures Added: Predictions set from inputs OK 11244 Veto at end 105 Veto at start 0 Times number of locations\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nsgz7aDYYwhF",
        "outputId": "e4df5554-19ae-4c6c-ec62-918a659be838"
      },
      "source": [
        "#Clean-up Input quantities\n",
        "def checkNaN(y):\n",
        "  countNaN = 0\n",
        "  countnotNaN = 0\n",
        "  ctprt = 0\n",
        "  if y is None:\n",
        "    return\n",
        "  if len(y.shape) == 2:\n",
        "    for i in range(0,y.shape[0]):\n",
        "        for j in range(0,y.shape[1]):\n",
        "            if(np.math.isnan(y[i,j])):\n",
        "                countNaN += 1\n",
        "            else:\n",
        "                countnotNaN += 1\n",
        "  else:\n",
        "    for i in range(0,y.shape[0]):\n",
        "      for j in range(0,y.shape[1]):\n",
        "        for k in range(0,y.shape[2]):\n",
        "          if(np.math.isnan(y[i,j,k])):\n",
        "              countNaN += 1\n",
        "              ctprt += 1\n",
        "              print(str(i) + ' ' + str(j) + ' ' + str(k))\n",
        "              if ctprt > 10:\n",
        "                sys.exit(0)\n",
        "          else:\n",
        "              countnotNaN += 1\n",
        "\n",
        "  percent = (100.0*countNaN)/(countNaN + countnotNaN)\n",
        "  print(' is NaN ',str(countNaN),' percent ',str(round(percent,2)),' not NaN ', str(countnotNaN))\n",
        "\n",
        "# Clean-up Input Source\n",
        "if len(InputSource) > 0: \n",
        "  PropertyNameIndex = NewPropertyNameIndex\n",
        "  NewPropertyNameIndex = None\n",
        "  PropertyAverageValuesPointer = NewPropertyAverageValuesPointer\n",
        "  NewPropertyAverageValuesPointer = None\n",
        "\n",
        "  NormedInputStaticProps = NewNormedInputStaticProps\n",
        "  NewNormedInputStaticProps = None\n",
        "  NormedDynamicPropertyTimeSeries = NewNormedDynamicPropertyTimeSeries\n",
        "  NewNormedDynamicPropertyTimeSeries = None\n",
        "\n",
        "  NpropperTime = NewNpropperTime\n",
        "  NpropperTimeStatic = NewNpropperTimeStatic\n",
        "  NpropperTimeDynamic = NewNpropperTimeDynamic\n",
        "\n",
        "print('Static Properties')\n",
        "if NpropperTimeStatic > 0 :\n",
        "  checkNaN(NormedInputStaticProps)\n",
        "else:\n",
        "  print(' None Defined')\n",
        "print('Dynamic Properties')\n",
        "checkNaN(NormedDynamicPropertyTimeSeries)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Static Properties\n",
            " None Defined\n",
            "Dynamic Properties\n",
            " is NaN  0  percent  0.0  not NaN  3067500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NnNBiW_Y2DB"
      },
      "source": [
        "# Covid Data: Agree on Tseq Sequence Length\n",
        "if ReadAugust2020Covid or RereadMay2020:\n",
        "  Tseq = 9\n",
        "if ReadJan2021Covid or ReadAugust2020Covid or ReadApril2021Covid:\n",
        "  Tseq = 13"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RdcDPKY5X8",
        "outputId": "9ba3d4c3-2c89-4d42-8a1e-a4c66a4c1a20"
      },
      "source": [
        "# Generate Sequences from Time labelled data\n",
        "# given Tseq set above\n",
        "\n",
        "# If SymbolicWindows, sequences are not made but we use same array with that dimension set to 1\n",
        "# reshape can get rid of it\n",
        "# Predictions are associated with sequence number which is first time value used in sequence\n",
        "# if SymbolicWindows false then sequences are labelled by sequence # and contain time values from sequence # to sequence# + Tseq-1\n",
        "# if SymbolicWindows True then sequences are labelled by time # and contain one value\n",
        "RawInputSeqDimension = Tseq\n",
        "Num_SeqExtra = 0\n",
        "if SymbolicWindows:\n",
        "  RawInputSeqDimension = 1\n",
        "  Num_SeqExtra =  Tseq-1\n",
        "\n",
        "if GenerateSequences:\n",
        "  UseProperties = np.full(NpropperTime, True, dtype=np.bool)\n",
        "  Npropperseq = 0\n",
        "  IndexintoPropertyArrays = np.empty(NpropperTime, dtype = np.int)\n",
        "  for iprop in range(0,NpropperTime):\n",
        "    if UseProperties[iprop]:\n",
        "      IndexintoPropertyArrays[Npropperseq] = iprop\n",
        "      Npropperseq +=1\n",
        "  Num_Seq = Num_Time - Tseq\n",
        "  RawInputSequences = np.zeros([Num_Seq + Num_SeqExtra, Nloc, RawInputSeqDimension, Npropperseq], dtype =np.float32)\n",
        "  RawInputPredictions = np.zeros([Num_Seq, Nloc, Npredperseq], dtype =np.float32)\n",
        "\n",
        "  locationarray = np.empty(Nloc, dtype=np.float32)\n",
        "  for iseq in range(0,Num_Seq  + Num_SeqExtra):\n",
        "    for windowposition in range(0,RawInputSeqDimension):\n",
        "      itime = iseq + windowposition\n",
        "      for usedproperty  in range (0,Npropperseq):\n",
        "        iprop = IndexintoPropertyArrays[usedproperty]\n",
        "        if iprop>=NpropperTimeStatic:\n",
        "          jprop =iprop-NpropperTimeStatic\n",
        "          locationarray = NormedDynamicPropertyTimeSeries[itime,:,jprop]\n",
        "        else:\n",
        "          locationarray = NormedInputStaticProps[:,iprop]\n",
        "        RawInputSequences[iseq,:,windowposition,usedproperty] = locationarray\n",
        "    if iseq < Num_Seq:\n",
        "      RawInputPredictions[iseq,:,:] = InputPredictionsbyTime[iseq+Tseq,:,:]\n",
        "  print(startbold + 'Sequences set from Time values Num Seq ' + str(Num_Seq) + ' Time ' +str(Num_Time) + resetfonts)  \n",
        "\n",
        "NormedInputTimeSeries = None\n",
        "NormedDynamicPropertyTimeSeries = None\n",
        "if GarbageCollect:\n",
        "  gc.collect()\n",
        "\n",
        "GlobalTimeMask = np.empty([1,1,1,Tseq,Tseq],dtype =np.float32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mSequences set from Time values Num Seq 396 Time 409\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qmvDUkQZL0A",
        "outputId": "edbfc4b8-328b-42ab-f72d-90c11ae0c27b"
      },
      "source": [
        "# Define Possible Temporal and Spatial Positional Encodings\n",
        "def LinearLocationEncoding(TotalLoc):\n",
        "  linear = np.empty(TotalLoc, dtype=float)\n",
        "  for i in range(0,TotalLoc):\n",
        "    linear[i] = float(i)/float(TotalLoc)\n",
        "  return linear\n",
        "\n",
        "def LinearTimeEncoding(Dateslisted):\n",
        "  Firstdate = Dateslisted[0]\n",
        "  numtofind = len(Dateslisted)\n",
        "  dayrange = (Dateslisted[numtofind-1]-Firstdate).days + 1\n",
        "  linear = np.empty(numtofind, dtype=float)\n",
        "  for i in range(0,numtofind):\n",
        "    linear[i] = float((Dateslisted[i]-Firstdate).days)/float(dayrange)\n",
        "  return linear\n",
        "\n",
        "def P2TimeEncoding(numtofind):\n",
        "  P2 = np.empty(numtofind, dtype=float)\n",
        "  for i in range(0,numtofind):\n",
        "    x =  -1 + 2.0*i/(numtofind-1)\n",
        "    P2[i] = 0.5*(3*x*x-1)\n",
        "  return P2\n",
        "\n",
        "def P3TimeEncoding(numtofind):\n",
        "  P3 = np.empty(numtofind, dtype=float)\n",
        "  for i in range(0,numtofind):\n",
        "    x =  -1 + 2.0*i/(numtofind-1)\n",
        "    P3[i] = 0.5*(5*x*x-3)*x\n",
        "  return P3\n",
        "\n",
        "def P4TimeEncoding(numtofind):\n",
        "  P4 = np.empty(numtofind, dtype=float)\n",
        "  for i in range(0,numtofind):\n",
        "    x =  -1 + 2.0*i/(numtofind-1)\n",
        "    P4[i] = 0.125*(35*x*x*x*x - 30*x*x + 3)\n",
        "  return P4\n",
        "\n",
        "def WeeklyTimeEncoding(Dateslisted):\n",
        "  numtofind = len(Dateslisted)\n",
        "  costheta = np.empty(numtofind, dtype=float)\n",
        "  sintheta = np.empty(numtofind, dtype=float)\n",
        "  for i in range(0,numtofind):\n",
        "    j = Dateslisted[i].date().weekday()\n",
        "    theta = float(j)*2.0*math.pi/7.0\n",
        "    costheta[i] = math.cos(theta)\n",
        "    sintheta[i] = math.sin(theta)\n",
        "  return costheta, sintheta\n",
        "\n",
        "def AnnualTimeEncoding(Dateslisted): \n",
        "  numtofind = len(Dateslisted)\n",
        "  costheta = np.empty(numtofind, dtype=float)\n",
        "  sintheta = np.empty(numtofind, dtype=float)\n",
        "  for i in range(0,numtofind):\n",
        "    runningdate = Dateslisted[i]\n",
        "    year = runningdate.year\n",
        "    datebeginyear = datetime(year, 1, 1)\n",
        "    displacement = (runningdate-datebeginyear).days\n",
        "    daysinyear = (datetime(year,12,31)-datebeginyear).days+1\n",
        "    if displacement >= daysinyear:\n",
        "      printexit(\"EXIT Bad Date \", runningdate)\n",
        "    theta = float(displacement)*2.0*math.pi/float(daysinyear)\n",
        "    costheta[i] = math.cos(theta)\n",
        "    sintheta[i] = math.sin(theta)\n",
        "  return costheta, sintheta\n",
        "\n",
        "def ReturnEncoding(numtofind,Typeindex, Typevalue):\n",
        "  Dummy = costheta = np.empty(0, dtype=float)\n",
        "  if Typeindex == 1:\n",
        "    return LinearoverLocationEncoding, Dummy, ('LinearSpace',0.,1.0,0.5,0.2887), ('Dummy',0.,0.,0.,0.)\n",
        "  if Typeindex == 2:\n",
        "    if Dailyunit == 1:\n",
        "      return CosWeeklytimeEncoding, SinWeeklytimeEncoding, ('CosWeekly',-1.0, 1.0, 0.,0.7071), ('SinWeekly',-1.0, 1.0, 0.,0.7071)\n",
        "    else:\n",
        "      return Dummy, Dummy, ('Dummy',0.,0.,0.,0.), ('Dummy',0.,0.,0.,0.)\n",
        "  if Typeindex == 3:\n",
        "    return CosAnnualtimeEncoding, SinAnnualtimeEncoding, ('CosAnnual',-1.0, 1.0, 0.,0.7071), ('SinAnnual',-1.0, 1.0, 0.,0.7071)\n",
        "  if Typeindex == 4:\n",
        "    if Typevalue == 0:\n",
        "      ConstArray = np.full(numtofind,0.5, dtype = float)\n",
        "      return ConstArray, Dummy, ('Constant',0.5,0.5,0.5,0.0), ('Dummy',0.,0.,0.,0.)\n",
        "    if Typevalue == 1:\n",
        "      return LinearovertimeEncoding, Dummy, ('LinearTime',0., 1.0, 0.5,0.2887), ('Dummy',0.,0.,0.,0.)\n",
        "    if Typevalue == 2:\n",
        "      return P2TimeEncoding(numtofind), Dummy, ('P2-Time',-1.0, 1.0, 0.,0.4472), ('Dummy',0.,0.,0.,0.)\n",
        "    if Typevalue == 3:\n",
        "      return P3TimeEncoding(numtofind), Dummy, ('P3-Time',-1.0, 1.0, 0.,0.3780), ('Dummy',0.,0.,0.,0.)\n",
        "    if Typevalue == 4:\n",
        "      return P4TimeEncoding(numtofind), Dummy, ('P4-Time',-1.0, 1.0, 0.,0.3333), ('Dummy',0.,0.,0.,0.)\n",
        "  if Typeindex == 5:\n",
        "      costheta = np.empty(numtofind, dtype=float)\n",
        "      sintheta = np.empty(numtofind, dtype=float)\n",
        "      j = 0\n",
        "      for i in range(0,numtofind):\n",
        "        theta = float(j)*2.0*math.pi/Typevalue\n",
        "        costheta[i] = math.cos(theta)\n",
        "        sintheta[i] = math.sin(theta)\n",
        "        j += 1\n",
        "        if j >= Typevalue:\n",
        "          j = 0\n",
        "      return costheta, sintheta,('Cos '+str(Typevalue)+ ' Len',-1.0, 1.0,0.,0.7071), ('Sin '+str(Typevalue)+ ' Len',-1.0, 1.0,0.,0.7071)\n",
        "\n",
        "# Dates set up in Python datetime format as Python LISTS\n",
        "# All encodings are Numpy arrays\n",
        "print(\"Total number of Time Units \" + str(NumberofTimeunits) + ' ' + TimeIntervalUnitName)\n",
        "if NumberofTimeunits != (Num_Seq + Tseq):\n",
        "  printexit(\"EXIT Wrong Number of Time Units \" + str(Num_Seq + Tseq))\n",
        "\n",
        "Dateslist = []\n",
        "for i in range(0,NumberofTimeunits):\n",
        "  Dateslist.append(InitialDate+timedelta(days=i*Dailyunit))\n",
        "\n",
        "LinearoverLocationEncoding = LinearLocationEncoding(Nloc)\n",
        "LinearovertimeEncoding = LinearTimeEncoding(Dateslist)\n",
        "\n",
        "if Dailyunit == 1:\n",
        "  CosWeeklytimeEncoding, SinWeeklytimeEncoding = WeeklyTimeEncoding(Dateslist)\n",
        "CosAnnualtimeEncoding, SinAnnualtimeEncoding = AnnualTimeEncoding(Dateslist)\n",
        "\n",
        "\n",
        "# Encodings\n",
        "\n",
        "# linearlocationposition\n",
        "# Supported Time Dependent Probes that can be in properties and/or predictions\n",
        "# Special\n",
        "# Annual\n",
        "# Weekly\n",
        "# \n",
        "# Top Down\n",
        "# TD0 Constant at 0.5\n",
        "# TD1 Linear from 0 to 1\n",
        "# TD2 P2(x) where x goes from -1 to 1 as time goes from start to end\n",
        "# \n",
        "# Bottom Up\n",
        "# n-way Cos and sin theta where n = 4 7 8 16 24 32\n",
        "\n",
        "EncodingTypes = {'Spatial':1, 'Weekly':2,'Annual':3,'TopDown':4,'BottomUp':5}\n",
        "\n",
        "PropIndex =[]\n",
        "PropNameMeanStd = []\n",
        "PropMeanStd = []\n",
        "PropArray = []\n",
        "PropPosition = []\n",
        "\n",
        "PredIndex =[]\n",
        "PredNameMeanStd = []\n",
        "PredArray = []\n",
        "PredPosition = []\n",
        "\n",
        "Numberpropaddons = 0\n",
        "propposition = Npropperseq\n",
        "Numberpredaddons = 0\n",
        "predposition = Npredperseq\n",
        "\n",
        "numprop = len(PropTypes)\n",
        "if numprop != len(PropValues):\n",
        "  printexit('Error in property addons ' + str(numprop) + ' ' + str(len(PropValues)))\n",
        "for newpropinlist in range(0,numprop):\n",
        "  Typeindex = EncodingTypes[PropTypes[newpropinlist]]\n",
        "  a,b,c,d = ReturnEncoding(Num_Time,Typeindex, PropValues[newpropinlist])\n",
        "  if c[0] != 'Dummy':\n",
        "    PropIndex.append(Typeindex)\n",
        "    PropNameMeanStd.append(c)\n",
        "    InputPropertyNames.append(c[0])\n",
        "    PropArray.append(a)\n",
        "    PropPosition.append(propposition)\n",
        "    propposition += 1\n",
        "    Numberpropaddons += 1\n",
        "    line = ' '\n",
        "    for ipr in range(0,20):\n",
        "      line += str(round(a[ipr],4)) + ' '\n",
        "    print('c'+line)\n",
        "  if d[0] != 'Dummy':\n",
        "    PropIndex.append(Typeindex)\n",
        "    PropNameMeanStd.append(d)\n",
        "    InputPropertyNames.append(d[0])\n",
        "    PropArray.append(b)\n",
        "    PropPosition.append(propposition)\n",
        "    propposition += 1\n",
        "    Numberpropaddons += 1\n",
        "    line = ' '\n",
        "    for ipr in range(0,20):\n",
        "      line += str(round(b[ipr],4)) + ' '\n",
        "    print('d'+line)\n",
        "\n",
        "numpred = len(PredTypes)\n",
        "if numpred != len(PredValues):\n",
        "  printexit('Error in prediction addons ' + str(numpred) + ' ' + str(len(PredValues)))\n",
        "for newpredinlist in range(0,numpred):\n",
        "  Typeindex = EncodingTypes[PredTypes[newpredinlist]]\n",
        "  a,b,c,d = ReturnEncoding(Num_Time,Typeindex, PredValues[newpredinlist])\n",
        "  if c[0] != 'Dummy':\n",
        "    PredIndex.append(Typeindex)\n",
        "    PredNameMeanStd.append(c)\n",
        "    PredArray.append(a)\n",
        "    Predictionname.append(c[0])\n",
        "    Predictionnamelookup[c] = predposition\n",
        "    PredPosition.append(predposition)\n",
        "    predposition += 1\n",
        "    Numberpredaddons += 1\n",
        "    Predictionwgt.append(0.25)\n",
        "  if d[0] != 'Dummy':\n",
        "    PredIndex.append(Typeindex)\n",
        "    PredNameMeanStd.append(d)\n",
        "    PredArray.append(b)\n",
        "    Predictionname.append(d[0])\n",
        "    Predictionnamelookup[d[0]] = predposition\n",
        "    PredPosition.append(predposition)\n",
        "    predposition += 1\n",
        "    Numberpredaddons += 1\n",
        "    Predictionwgt.append(0.25)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Time Units 409 Day\n",
            "c 0.0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016 0.018 0.02 0.022 0.024 0.026 0.028 0.03 0.032 0.034 0.036 0.038 \n",
            "c 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 \n",
            "c 0.0 0.0024 0.0049 0.0073 0.0098 0.0122 0.0147 0.0171 0.0196 0.022 0.0244 0.0269 0.0293 0.0318 0.0342 0.0367 0.0391 0.0416 0.044 0.0465 \n",
            "c 1.0 0.9853 0.9707 0.9562 0.9418 0.9274 0.9131 0.8988 0.8847 0.8706 0.8565 0.8426 0.8287 0.8149 0.8012 0.7875 0.7739 0.7604 0.747 0.7336 \n",
            "c -1.0 -0.9708 -0.9419 -0.9134 -0.8852 -0.8574 -0.83 -0.8028 -0.7761 -0.7497 -0.7236 -0.6979 -0.6725 -0.6475 -0.6228 -0.5984 -0.5743 -0.5506 -0.5273 -0.5042 \n",
            "c 1.0 0.9515 0.9041 0.8578 0.8124 0.7682 0.7249 0.6827 0.6414 0.6011 0.5618 0.5235 0.4861 0.4497 0.4141 0.3795 0.3458 0.313 0.2811 0.25 \n",
            "c 1.0 0.6235 -0.2225 -0.901 -0.901 -0.2225 0.6235 1.0 0.6235 -0.2225 -0.901 -0.901 -0.2225 0.6235 1.0 0.6235 -0.2225 -0.901 -0.901 -0.2225 \n",
            "d 0.0 0.7818 0.9749 0.4339 -0.4339 -0.9749 -0.7818 0.0 0.7818 0.9749 0.4339 -0.4339 -0.9749 -0.7818 0.0 0.7818 0.9749 0.4339 -0.4339 -0.9749 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOy50qP_ZQxS",
        "outputId": "3b80eb9d-d558-45e7-cbdd-f15f0c62fd01"
      },
      "source": [
        "# Add in Temporal and Spatial Encoding\n",
        "\n",
        "def SetNewAverages(InputList): # name min max mean std\n",
        "  results = np.empty(7, dtype = np.float32)\n",
        "  results[0] = InputList[1]\n",
        "  results[1] = InputList[2]\n",
        "  results[2] = 1.0\n",
        "  results[3] = InputList[3]\n",
        "  results[4] = InputList[4]\n",
        "  results[5] = InputList[3]\n",
        "  results[6] = InputList[4]\n",
        "  return results\n",
        "\n",
        "\n",
        "NpropperseqTOT = Npropperseq + Numberpropaddons\n",
        "\n",
        "# These include both Property and Prediction Variables\n",
        "NpropperTimeMAX =len(QuantityTakeroot)\n",
        "NewNpropperTimeMAX = NpropperTimeMAX + Numberpropaddons + Numberpredaddons  \n",
        "NewQuantityStatistics = np.zeros([NewNpropperTimeMAX,7], dtype=np.float32)\n",
        "NewQuantityTakeroot = np.full(NewNpropperTimeMAX,1,dtype=np.int) # All new ones aare 1 and are set here\n",
        "NewQuantityStatistics[0:NpropperTimeMAX,:] = QuantityStatistics[0:NpropperTimeMAX,:]\n",
        "NewQuantityTakeroot[0:NpropperTimeMAX] = QuantityTakeroot[0:NpropperTimeMAX]\n",
        "\n",
        "# Lookup for property names\n",
        "NewPropertyNameIndex = np.empty(NpropperseqTOT, dtype = np.int32)\n",
        "NumberofNames = len(InputPropertyNames)-Numberpropaddons\n",
        "NewPropertyNameIndex[0:Npropperseq] = PropertyNameIndex[0:Npropperseq]\n",
        "\n",
        "NewPropertyAverageValuesPointer = np.empty(NpropperseqTOT, dtype = np.int32)\n",
        "NewPropertyAverageValuesPointer[0:Npropperseq] = PropertyAverageValuesPointer[0:Npropperseq]\n",
        "\n",
        "for propaddons in range(0,Numberpropaddons):\n",
        "  NewPropertyNameIndex[Npropperseq+propaddons] = NumberofNames + propaddons\n",
        "  NewPropertyAverageValuesPointer[Npropperseq+propaddons] = NpropperTimeMAX + propaddons\n",
        "  NewQuantityStatistics[NpropperTimeMAX + propaddons,:] = SetNewAverages(PropNameMeanStd[propaddons])\n",
        "\n",
        "# Set extra Predictions metadata for Sequences\n",
        "NpredperseqTOT = Npredperseq + Numberpredaddons\n",
        "\n",
        "NewPredictionAverageValuesPointer = np.empty(NpredperseqTOT, dtype = np.int32)\n",
        "NewPredictionAverageValuesPointer[0:Npredperseq] = PredictionAverageValuesPointer[0:Npredperseq]\n",
        "\n",
        "for predaddons in range(0,Numberpredaddons):\n",
        "  NewPredictionAverageValuesPointer[Npredperseq +predaddons] = NpropperTimeMAX + +Numberpropaddons + predaddons\n",
        "  NewQuantityStatistics[NpropperTimeMAX + Numberpropaddons + predaddons,:] = SetNewAverages(PredNameMeanStd[predaddons])\n",
        "\n",
        "RawInputSequencesTOT = np.empty([Num_Seq  + Num_SeqExtra, Nloc, RawInputSeqDimension, NpropperseqTOT], dtype =np.float32)\n",
        "flsize = np.float(Num_Seq  + Num_SeqExtra)*np.float(Nloc)*np.float(RawInputSeqDimension)* np.float(NpropperseqTOT)* 4.0\n",
        "print('Total storage ' +str(round(flsize,0)) + ' Bytes')\n",
        "\n",
        "for iprop in range(0,Npropperseq):\n",
        "  RawInputSequencesTOT[:,:,:,iprop] = RawInputSequences[:,:,:,iprop]\n",
        "\n",
        "kincrement = Tseq- RawInputSeqDimension\n",
        "for i in range(0,Num_Seq  + Num_SeqExtra):\n",
        "    for k in range(0,RawInputSeqDimension):\n",
        "      for iprop in range(0, Numberpropaddons):\n",
        "        if PropIndex[iprop] == 1:\n",
        "          continue\n",
        "        RawInputSequencesTOT[i,:,k,PropPosition[iprop]] = PropArray[iprop][i+k]\n",
        "\n",
        "for iprop in range(0, Numberpropaddons):\n",
        "  if PropIndex[iprop] == 1:\n",
        "    for j in range(0,Nloc):       \n",
        "        RawInputSequencesTOT[:,j,:,PropPosition[iprop]] = PropArray[iprop][j]\n",
        "\n",
        "# Set extra Predictions for Sequences\n",
        "RawInputPredictionsTOT = np.empty([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
        "\n",
        "for ipred in range(0,Npredperseq):\n",
        "  RawInputPredictionsTOT[:,:,ipred] = RawInputPredictions[:,:,ipred]\n",
        "\n",
        "for i in range(0,Num_Seq):\n",
        "  for ipred in range(0, Numberpredaddons):\n",
        "    if PredIndex[ipred] == 1:\n",
        "      continue\n",
        "    actualarray = PredArray[ipred]\n",
        "    RawInputPredictionsTOT[i,:,PredPosition[ipred]] = actualarray[i+Tseq]\n",
        "\n",
        "for ipred in range(0, Numberpredaddons):\n",
        "  if PredIndex[ipred] == 1:\n",
        "    for j in range(0,Nloc):\n",
        "      RawInputPredictionsTOT[:,j,PredPosition[ipred]] = PredArray[ipred][j]\n",
        "\n",
        "PropertyNameIndex  = None\n",
        "PropertyNameIndex = NewPropertyNameIndex\n",
        "QuantityStatistics = None\n",
        "QuantityStatistics = NewQuantityStatistics\n",
        "QuantityTakeroot = None\n",
        "QuantityTakeroot = NewQuantityTakeroot\n",
        "PropertyAverageValuesPointer = None\n",
        "PropertyAverageValuesPointer = NewPropertyAverageValuesPointer\n",
        "PredictionAverageValuesPointer = None\n",
        "PredictionAverageValuesPointer = NewPredictionAverageValuesPointer\n",
        "\n",
        "print('Time and Space encoding added to input and predictions')\n",
        "\n",
        "if SymbolicWindows:\n",
        "  SymbolicInputSequencesTOT = np.empty([Num_Seq, Nloc], dtype =np.int32)\n",
        "  for iseq in range(0,Num_Seq):\n",
        "    for iloc in range(0,Nloc):\n",
        "      SymbolicInputSequencesTOT[iseq,iloc] = np.left_shift(iseq,16) + iloc\n",
        "  ReshapedSequencesTOT = np.transpose(RawInputSequencesTOT,(1,0,3,2))\n",
        "  ReshapedSequencesTOT = np.reshape(ReshapedSequencesTOT,(Nloc,Num_Time-1,NpropperseqTOT))\n",
        "\n",
        "# To calculate masks (identical to Symbolic windows)\n",
        "SpacetimeforMask = np.empty([Num_Seq, Nloc], dtype =np.int32)\n",
        "for iseq in range(0,Num_Seq):\n",
        "  for iloc in range(0,Nloc):\n",
        "    SpacetimeforMask[iseq,iloc] = np.left_shift(iseq,16) + iloc\n",
        "    \n",
        "print(PropertyNameIndex)\n",
        "print(InputPropertyNames)\n",
        "for iprop in range(0,NpropperseqTOT):\n",
        "  print('Property ' + str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]])\n",
        "for ipred in range(0,NpredperseqTOT):\n",
        "  print('Prediction ' + str(ipred) + ' ' + Predictionname[ipred] + ' ' + str(round(Predictionwgt[ipred],3)))\n",
        "\n",
        "\n",
        "\n",
        "RawInputPredictions = None\n",
        "RawInputSequences = None\n",
        "if SymbolicWindows:\n",
        "  RawInputSequencesTOT = None\n",
        "if GarbageCollect:\n",
        "  gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total storage 236808000.0 Bytes\n",
            "Time and Space encoding added to input and predictions\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
            "['Cases', 'Deaths', 'Age Distribution', 'Air Pollution', 'Co-morbidities', 'Demographics', 'Disease Spread', 'Health Disparities', 'Hospital Beds', 'Mobility', 'Residential Density', 'Social Distancing', 'Testing', 'Transmissible Cases', 'voting', 'LinearSpace', 'Constant', 'LinearTime', 'P2-Time', 'P3-Time', 'P4-Time', 'CosWeekly', 'SinWeekly']\n",
            "Property 0 Cases\n",
            "Property 1 Deaths\n",
            "Property 2 Age Distribution\n",
            "Property 3 Air Pollution\n",
            "Property 4 Co-morbidities\n",
            "Property 5 Demographics\n",
            "Property 6 Disease Spread\n",
            "Property 7 Health Disparities\n",
            "Property 8 Hospital Beds\n",
            "Property 9 Mobility\n",
            "Property 10 Residential Density\n",
            "Property 11 Social Distancing\n",
            "Property 12 Testing\n",
            "Property 13 Transmissible Cases\n",
            "Property 14 voting\n",
            "Property 15 LinearSpace\n",
            "Property 16 Constant\n",
            "Property 17 LinearTime\n",
            "Property 18 P2-Time\n",
            "Property 19 P3-Time\n",
            "Property 20 P4-Time\n",
            "Property 21 CosWeekly\n",
            "Property 22 SinWeekly\n",
            "Prediction 0 Next Cases 1.0\n",
            "Prediction 1 Next Deaths 1.0\n",
            "Prediction 2 Casesday2 0.071\n",
            "Prediction 3 Deathsday2 0.071\n",
            "Prediction 4 Casesday3 0.071\n",
            "Prediction 5 Deathsday3 0.071\n",
            "Prediction 6 Casesday4 0.071\n",
            "Prediction 7 Deathsday4 0.071\n",
            "Prediction 8 Casesday5 0.071\n",
            "Prediction 9 Deathsday5 0.071\n",
            "Prediction 10 Casesday6 0.071\n",
            "Prediction 11 Deathsday6 0.071\n",
            "Prediction 12 Casesday7 0.071\n",
            "Prediction 13 Deathsday7 0.071\n",
            "Prediction 14 Casesday8 0.071\n",
            "Prediction 15 Deathsday8 0.071\n",
            "Prediction 16 Casesday9 0.071\n",
            "Prediction 17 Deathsday9 0.071\n",
            "Prediction 18 Casesday10 0.071\n",
            "Prediction 19 Deathsday10 0.071\n",
            "Prediction 20 Casesday11 0.071\n",
            "Prediction 21 Deathsday11 0.071\n",
            "Prediction 22 Casesday12 0.071\n",
            "Prediction 23 Deathsday12 0.071\n",
            "Prediction 24 Casesday13 0.071\n",
            "Prediction 25 Deathsday13 0.071\n",
            "Prediction 26 Casesday14 0.071\n",
            "Prediction 27 Deathsday14 0.071\n",
            "Prediction 28 Casesday15 0.071\n",
            "Prediction 29 Deathsday15 0.071\n",
            "Prediction 30 LinearSpace 0.25\n",
            "Prediction 31 Constant 0.25\n",
            "Prediction 32 LinearTime 0.25\n",
            "Prediction 33 P2-Time 0.25\n",
            "Prediction 34 P3-Time 0.25\n",
            "Prediction 35 P4-Time 0.25\n",
            "Prediction 36 CosWeekly 0.25\n",
            "Prediction 37 SinWeekly 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezN8SOChZVTV"
      },
      "source": [
        "#Set up NNSE Normalized Nash Sutcliffe Efficiency\n",
        "CalculateNNSE = np.full(NpredperseqTOT, False, dtype = np.bool)\n",
        "for ipred in range(0,NumpredbasicperTime):\n",
        "  CalculateNNSE[ipred] = True"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmiHu6K3ZYnP",
        "outputId": "c01cdfb1-c296-40f4-dbd8-20daaf24d9c0"
      },
      "source": [
        "#Location Based Validation\n",
        "\n",
        "LocationBasedValidation = True\n",
        "LocationValidationFraction = 0.0\n",
        "RestartLocationBasedValidation = False\n",
        "RestartRunName = RunName\n",
        "RestartRunName = 'EARTHQN-Transformer3'\n",
        "FullSetValidation = False\n",
        "\n",
        "global SeparateValandTrainingPlots\n",
        "SeparateValandTrainingPlots = True\n",
        "if not LocationBasedValidation:\n",
        "  SeparateValandTrainingPlots = False\n",
        "  LocationValidationFraction = 0.0\n",
        "NlocValplusTraining = Nloc\n",
        "ListofTrainingLocs = np.arange(Nloc, dtype = np.int32)\n",
        "ListofValidationLocs = np.full(Nloc, -1, dtype = np.int32)\n",
        "MappingtoTraining = np.arange(Nloc, dtype = np.int32)\n",
        "MappingtoValidation = np.full(Nloc, -1, dtype = np.int32)\n",
        "TrainingNloc = Nloc\n",
        "ValidationNloc = 0\n",
        "if LocationBasedValidation:\n",
        "  if RestartLocationBasedValidation:\n",
        "      InputFileName = APPLDIR + '/Validation' + RestartRunName\n",
        "      with open(InputFileName, 'r', newline='') as inputfile:\n",
        "        Myreader = reader(inputfile, delimiter=',')\n",
        "        header = next(Myreader)\n",
        "        LocationValidationFraction = np.float32(header[0])\n",
        "        TrainingNloc = np.int32(header[1])\n",
        "        ValidationNloc = np.int32(header[2])     \n",
        "\n",
        "        ListofTrainingLocs = np.empty(TrainingNloc, dtype = np.int32)\n",
        "        ListofValidationLocs = np.empty(ValidationNloc,  dtype = np.int32)\n",
        "        nextrow = next(Myreader)\n",
        "        for iloc in range(0, TrainingNloc):\n",
        "          ListofTrainingLocs[iloc] = np.int32(nextrow[iloc])\n",
        "        nextrow = next(Myreader)\n",
        "        for iloc in range(0, ValidationNloc):\n",
        "          ListofValidationLocs[iloc] = np.int32(nextrow[iloc])\n",
        "\n",
        "      LocationTrainingfraction = 1.0 - LocationValidationFraction\n",
        "      if TrainingNloc + ValidationNloc != Nloc:\n",
        "        printexit('EXIT: Inconsistent location counts for Location Validation ' +str(Nloc)\n",
        "          + ' ' + str(TrainingNloc) + ' ' + str(ValidationNloc))\n",
        "      print(' Validation restarted Fraction ' +str(round(LocationValidationFraction,4)) + ' ' + RestartRunName)\n",
        "\n",
        "  else:\n",
        "    LocationTrainingfraction = 1.0 - LocationValidationFraction\n",
        "    TrainingNloc = math.ceil(LocationTrainingfraction*Nloc)\n",
        "    ValidationNloc = Nloc - TrainingNloc\n",
        "    np.random.shuffle(ListofTrainingLocs)\n",
        "    ListofValidationLocs = ListofTrainingLocs[TrainingNloc:Nloc]\n",
        "    ListofTrainingLocs = ListofTrainingLocs[0:TrainingNloc]\n",
        "\n",
        "  for iloc in range(0,TrainingNloc):\n",
        "    jloc = ListofTrainingLocs[iloc]\n",
        "    MappingtoTraining[jloc] = iloc\n",
        "    MappingtoValidation[jloc] = -1\n",
        "  for iloc in range(0,ValidationNloc):\n",
        "    jloc = ListofValidationLocs[iloc]\n",
        "    MappingtoValidation[jloc] = iloc\n",
        "    MappingtoTraining[jloc] = -1\n",
        "  if ValidationNloc <= 0:\n",
        "    SeparateValandTrainingPlots = False\n",
        "\n",
        "  if not RestartLocationBasedValidation:\n",
        "    OutputFileName = APPLDIR + '/Validation' + RunName\n",
        "    with open(OutputFileName, 'w', newline='') as outputfile:\n",
        "      Mywriter = writer(outputfile, delimiter=',')\n",
        "      Mywriter.writerow([LocationValidationFraction, TrainingNloc, ValidationNloc] )\n",
        "      Mywriter.writerow(ListofTrainingLocs)\n",
        "      Mywriter.writerow(ListofValidationLocs)\n",
        "\n",
        "  print('Training Locations ' + str(TrainingNloc) + ' Validation Locations ' + str(ValidationNloc))\n",
        "  if ValidationNloc <=0:\n",
        "    LocationBasedValidation = False\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Locations 500 Validation Locations 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4B9eh8LZe2O",
        "outputId": "cd870f39-3767-444c-dfdd-79da012f67ad"
      },
      "source": [
        "# General Control Parameters\n",
        "OuterBatchDimension = Num_Seq * TrainingNloc\n",
        "IndividualPlots = False\n",
        "Plotrealnumbers = False\n",
        "PlotsOnlyinTestFIPS = True\n",
        "ListofTestFIPS = ['36061','53033']\n",
        "\n",
        "StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + int(Dailyunit/2),'D')\n",
        "\n",
        "NumericalCutoff = int(Num_Seq/2)\n",
        "CutoffDate = StartDate + np.timedelta64(NumericalCutoff*Dailyunit,'D')\n",
        "print('Start ' + str(StartDate) + ' Cutoff ' + str(CutoffDate) + \" sequence index \" + str(NumericalCutoff))\n",
        "\n",
        "TimeCutLabel = [' All Time ',' Start ',' End ']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start 2020-03-15 Cutoff 2020-09-29 sequence index 198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO02LoadZirs",
        "outputId": "c8d16a2d-d24e-412a-9fcf-64ef31f96307"
      },
      "source": [
        "print(\"Size of sequence window Tseq \", str(Tseq))\n",
        "print(\"Number of Sequences in time Num_Seq \", str(Num_Seq))\n",
        "print(\"Number of locations Nloc \", str(Nloc))\n",
        "print(\"Number of Training Sequences in Location and Time \", str(OuterBatchDimension))\n",
        "print(\"Number of internal properties per sequence including static or dynamic Npropperseq \", str(Npropperseq))\n",
        "print(\"Number of internal properties per sequence adding in explicit space-time encoding \", str(NpropperseqTOT))\n",
        "print(\"Total number of predictions per sequence NpredperseqTOT \", str(NpredperseqTOT))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of sequence window Tseq  13\n",
            "Number of Sequences in time Num_Seq  396\n",
            "Number of locations Nloc  500\n",
            "Number of Training Sequences in Location and Time  198000\n",
            "Number of internal properties per sequence including static or dynamic Npropperseq  15\n",
            "Number of internal properties per sequence adding in explicit space-time encoding  23\n",
            "Total number of predictions per sequence NpredperseqTOT  38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ix-aRHwj3ob"
      },
      "source": [
        "\n",
        "def DLprediction(Xin, yin, DLmodel, modelflag, LabelFit =''):\n",
        "  # modelflag = 0 LSTM = 1 Transformer\n",
        "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
        "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
        "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
        "    current_time = timenow()\n",
        "    print(startbold + startred + current_time + ' ' + RunName + \" DLPrediction \" +RunComment + resetfonts)\n",
        "\n",
        "    FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
        "    # Compare to RawInputPredictionsTOT\n",
        "\n",
        "    RMSEbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
        "    RMSETRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
        "    RMSEVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
        "    RMSVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
        "    AbsEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
        "    AbsVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
        "    ObsVbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT], dtype=np.float64)\n",
        "    Predbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT], dtype=np.float64)\n",
        "    countbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
        "    countVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
        "    countTRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
        "    totalcount = 0\n",
        "    overcount = 0\n",
        "    weightedcount = 0.0\n",
        "    weightedovercount = 0.0\n",
        "    weightedrmse1 = 0.0\n",
        "    weightedrmse1TRAIN = 0.0\n",
        "    weightedrmse1VAL = 0.0\n",
        "\n",
        "    closs = 0.0\n",
        "    dloss = 0.0\n",
        "    eloss = 0.0\n",
        "    floss = 0.0\n",
        "    sw = np.empty([Nloc,NpredperseqTOT],dtype = np.float32)\n",
        "    for iloc in range(0,Nloc):\n",
        "      for k in range(0,NpredperseqTOT):\n",
        "        sw[iloc,k] = Predictionwgt[k]\n",
        "\n",
        "    global tensorsw\n",
        "    tensorsw = tf.convert_to_tensor(sw, np.float32)\n",
        "    Ctime1 = 0.0\n",
        "    Ctime2 = 0.0\n",
        "    Ctime3 = 0.0\n",
        "    samplebar = notebook.trange(Num_Seq,  desc='Predict loop', unit  = 'sequences')\n",
        "    countingcalls = 0\n",
        "\n",
        "    for iseq in range(0, Num_Seq):\n",
        "      StopWatch.start('label1')\n",
        "      if SymbolicWindows:\n",
        "          InputVector = Xin[:,iseq:iseq+Tseq,:]\n",
        "      else:\n",
        "        InputVector = Xin[iseq]\n",
        "      Time = None\n",
        "      if modelflag == 0:\n",
        "        InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
        "      else:\n",
        "        InputVector = np.reshape(InputVector,(1,Tseq*Nloc,NpropperseqTOT))\n",
        "        BasicTimes = np.full(Nloc,iseq, dtype=np.int32)\n",
        "        Time = SetSpacetime(np.reshape(BasicTimes,(1,-1)))\n",
        "      StopWatch.stop('label1')\n",
        "      Ctime1 += StopWatch.get('label1', digits=4)\n",
        "      \n",
        "      StopWatch.start('label2')\n",
        "      PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time)\n",
        "      StopWatch.stop('label2')\n",
        "      Ctime2 += StopWatch.get('label2', digits=4)\n",
        "      StopWatch.start('label3')\n",
        "      PredictedVector = np.reshape(PredictedVector,(Nloc,NpredperseqTOT))\n",
        "      TrueVector = yin[iseq]\n",
        "      functionval = numpycustom_lossGCF1(TrueVector,PredictedVector,sw)\n",
        "      closs += functionval\n",
        "      PredictedVector_t = tf.convert_to_tensor(PredictedVector)\n",
        "      yin_t = tf.convert_to_tensor(TrueVector)\n",
        "      dloss += weightedcustom_lossGCF1(yin_t,PredictedVector_t,tensorsw)\n",
        "      eloss += custom_lossGCF1spec(yin_t,PredictedVector_t) \n",
        "      OutputLoss = 0.0\n",
        "      FitPredictions[iseq] = PredictedVector\n",
        "      for iloc in range(0,Nloc):\n",
        "        yy = yin[iseq,iloc]\n",
        "        yyhat = PredictedVector[iloc]\n",
        "\n",
        "        sum1 = 0.0\n",
        "        for i in range(0,NpredperseqTOT):\n",
        "          overcount += 1\n",
        "          weightedovercount += Predictionwgt[i]\n",
        "\n",
        "          if(math.isnan(yy[i])):\n",
        "            continue\n",
        "          weightedcount += Predictionwgt[i]\n",
        "          totalcount += 1\n",
        "          mse1 = ((yy[i]-yyhat[i])**2)\n",
        "          mse = mse1*sw[iloc,i]\n",
        "          if i < Npredperseq:\n",
        "            floss += mse\n",
        "          sum1 += mse\n",
        "          AbsEbyclass[i] += abs(yy[i] - yyhat[i])\n",
        "          RMSVbyclass[i] += yy[i]**2\n",
        "          AbsVbyclass[i] += abs(yy[i])\n",
        "          RMSEbyclass[i,0] += mse\n",
        "          countbyclass[i,0] += 1.0\n",
        "          if iseq < NumericalCutoff:\n",
        "            countbyclass[i,1] += 1.0\n",
        "            RMSEbyclass[i,1] += mse\n",
        "          else:\n",
        "            countbyclass[i,2] += 1.0\n",
        "            RMSEbyclass[i,2] += mse\n",
        "          if LocationBasedValidation:\n",
        "            if MappingtoTraining[iloc] >= 0:\n",
        "              RMSETRAINbyclass[i,0] += mse\n",
        "              countTRAINbyclass[i,0] += 1.0\n",
        "              if iseq < NumericalCutoff:\n",
        "                RMSETRAINbyclass[i,1] += mse\n",
        "                countTRAINbyclass[i,1] += 1.0\n",
        "              else:\n",
        "                RMSETRAINbyclass[i,2] += mse\n",
        "                countTRAINbyclass[i,2] += 1.0\n",
        "            if MappingtoValidation[iloc] >= 0:\n",
        "              RMSEVALbyclass[i,0] += mse\n",
        "              countVALbyclass[i,0] += 1.0\n",
        "              if iseq < NumericalCutoff:\n",
        "                RMSEVALbyclass[i,1] += mse\n",
        "                countVALbyclass[i,1] += 1.0\n",
        "              else:\n",
        "                RMSEVALbyclass[i,2] += mse\n",
        "                countVALbyclass[i,2] += 1.0\n",
        "          ObsVbytimeandclass [iseq,i] += abs(yy[i])\n",
        "          Predbytimeandclass [iseq,i] += abs(yyhat[i])\n",
        "        weightedrmse1 += sum1\n",
        "        if LocationBasedValidation:\n",
        "          if MappingtoTraining[iloc] >= 0:\n",
        "            weightedrmse1TRAIN += sum1\n",
        "          if MappingtoValidation[iloc] >= 0:\n",
        "            weightedrmse1VAL += sum1\n",
        "        OutputLoss += sum1\n",
        "      StopWatch.stop('label3')\n",
        "      Ctime3 += StopWatch.get('label3', digits=4)\n",
        "      OutputLoss /= Nloc\n",
        "      countingcalls += 1\n",
        "      samplebar.update(1)\n",
        "      samplebar.set_postfix( Call = countingcalls, TotalLoss = OutputLoss)\n",
        "\n",
        "    print('Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)))\n",
        "    weightedrmse1 /= (Num_Seq * Nloc)\n",
        "    floss /= (Num_Seq * Nloc)\n",
        "    if LocationBasedValidation:\n",
        "      weightedrmse1TRAIN /= (Num_Seq * TrainingNloc)\n",
        "      if ValidationNloc>0:\n",
        "        weightedrmse1VAL /= (Num_Seq * ValidationNloc)\n",
        "    dloss = dloss.numpy()\n",
        "    eloss = eloss.numpy()\n",
        "    closs /= Num_Seq\n",
        "    dloss /= Num_Seq\n",
        "    eloss /= Num_Seq\n",
        "\n",
        "    current_time = timenow()\n",
        "    line1 = ''\n",
        "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
        "    GlobalLoss = weightedrmse1\n",
        "    if LocationBasedValidation:\n",
        "      line1 = ' Training ' + str(round(weightedrmse1TRAIN,6)) + ' Validation ' + str(round(weightedrmse1VAL,6))\n",
        "      GlobalTrainingLoss = weightedrmse1TRAIN\n",
        "      GlobalValidationLoss = weightedrmse1VAL\n",
        "    print( startbold + startred + current_time + ' DLPrediction Averages' + ' ' + RunName + ' ' + RunComment +  resetfonts)\n",
        "    line = LabelFit + ' ' + RunName + ' Weighted sum over predicted values ' + str(round(weightedrmse1,6))\n",
        "    line += ' No Encoding Preds ' + str(round(floss,6)) + line1\n",
        "    line += ' from loss function ' + str(round(closs,6)) + ' TF version ' + str(round(dloss,6)) + ' TFspec version ' + str(round(eloss,6))  \n",
        "    print(wraptotext(line))\n",
        "    print('Count ignoring NaN ' +str(round(weightedcount,4))+ ' Counting NaN ' + str(round(weightedovercount,4)), 70 )\n",
        "    print(' Unwgt Count no NaN ',totalcount, ' Unwgt Count with NaN ',overcount, ' Number Sequences ', Nloc*Num_Seq)\n",
        "\n",
        "    ObsvPred = np.sum( np.abs(ObsVbytimeandclass-Predbytimeandclass) , axis=0)\n",
        "    TotalObs = np.sum( ObsVbytimeandclass , axis=0)\n",
        "    SummedEbyclass = np.divide(ObsvPred,TotalObs)\n",
        "    RMSEbyclass1 = np.divide(RMSEbyclass,countbyclass) # NO SQRT\n",
        "    RMSEbyclass2 = np.sqrt(np.divide(RMSEbyclass[:,0],RMSVbyclass))\n",
        "    RelEbyclass = np.divide(AbsEbyclass, AbsVbyclass)\n",
        "    extracomments = []\n",
        "\n",
        "    line1 = '\\nErrors by Prediction Components -- class weights not included except in final Loss components\\n Name Count without NaN, '\n",
        "    line2 = 'Loss wgt * sum errors**2/count, sqrt(sum errors**2/sum target**2), sum(abs(error)/sum(abs(value), abs(sum(abs(value)-abs(pred)))/sum(abs(pred)'\n",
        "    print(wraptotext(startbold + startred + line1 + line2 + resetfonts))\n",
        "    \n",
        "    for i in range(0,NpredperseqTOT):\n",
        "      line = startbold + startred + ' Loss Coeffs '\n",
        "      for timecut in range(0,3):\n",
        "        line += TimeCutLabel[timecut] + 'Full ' + str(round(RMSEbyclass1[i,timecut],6)) + resetfonts\n",
        "      if LocationBasedValidation:\n",
        "        RTRAIN = np.divide(RMSETRAINbyclass[i],countTRAINbyclass[i])\n",
        "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
        "        if countVALbyclass[i,0] > 0:\n",
        "          RVAL = np.divide(RMSEVALbyclass[i],countVALbyclass[i])\n",
        "        for timecut in range(0,3):\n",
        "          line += startbold + startpurple + TimeCutLabel[timecut] + 'TRAIN ' + resetfonts + str(round(RTRAIN[timecut],6))\n",
        "          line += startbold + ' VAL ' + resetfonts + str(round(RVAL[timecut],6))\n",
        "      else:\n",
        "        RTRAIN = RMSEbyclass1[i]\n",
        "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
        "        for timecut in range(0,3):\n",
        "          line += TimeCutLabel[timecut] + 'FULL ' + str(round(RTRAIN[timecut],6))   \n",
        "      print(wraptotext(str(i) + ' ' + startbold + Predictionname[i] + resetfonts + ' All Counts ' + str(round(countbyclass[i,0],0)) + ' '\n",
        "       + str(round(100.0*RMSEbyclass2[i],4)) + ' ' + str(round(100.0*RelEbyclass[i],4)) + ' ' + str(round(100.0*SummedEbyclass[i],4)) +line ))\n",
        "      extracomments.append(['Loss Coeffs F=' + str(round(RTRAIN[0],5)) + ' S=' + str(round(RTRAIN[1],5))+ ' E=' + str(round(RTRAIN[2],5)),\n",
        "                            'Loss Coeffs F=' + str(round(RVAL[0],5)) + ' S=' + str(round(RVAL[1],5))+ ' E=' + str(round(RVAL[2],5))])\n",
        "\n",
        "# Don't use DLPrediction for Transformer Plots. Wait for DL2B,D,E\n",
        "    if modelflag == 1:\n",
        "      return FitPredictions\n",
        "    \n",
        "\n",
        "    print('\\n Next plots come from DLPrediction')\n",
        "    Location_summed_plot(yin, FitPredictions, extracomments = extracomments, Dumpplot = True)  \n",
        "    FindNNSE(yin, FitPredictions)\n",
        "\n",
        "    if IndividualPlots:\n",
        "      ProduceIndividualPlots(yin, FitPredictions)\n",
        "\n",
        "\n",
        "\n",
        "# Call DLprediction2F here if modelflag=0\n",
        "    DLprediction2F(Xin, yin, DLmodel, modelflag)\n",
        "\n",
        "    return FitPredictions   "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck8atC3fj6gt"
      },
      "source": [
        "def ProduceIndividualPlots(Observations, FitPredictions):\n",
        "  current_time = timenow()\n",
        "  print(startbold + startred + current_time + ' Produce Individual Plots ' + RunName + ' ' + RunComment + resetfonts)\n",
        "# Find Best and Worst Locations\n",
        "  fips_b, fips_w = bestandworst(Observations, FitPredictions)\n",
        "\n",
        "\n",
        "  plot_by_fips(6037, Observations, FitPredictions)\n",
        "  plot_by_fips(36061, Observations, FitPredictions)\n",
        "  plot_by_fips(17031, Observations, FitPredictions)\n",
        "  plot_by_fips(53033, Observations, FitPredictions)\n",
        "  if (fips_b!=6037) and (fips_b!=36061) and (fips_b!=17031) and (fips_b!=53033):\n",
        "    plot_by_fips(fips_b, Observations, FitPredictions)\n",
        "  if (fips_w!=6037) and (fips_w!=36061) and (fips_w!=17031) and (fips_w!=53033):\n",
        "    plot_by_fips(fips_w, Observations, FitPredictions)\n",
        "\n",
        "  # Plot top 10 largest cities\n",
        "  sortedcities = np.flip(np.argsort(Locationpopulation))\n",
        "  for pickout in range (0,10):\n",
        "    Locationindex = sortedcities[pickout]\n",
        "    fips = Locationfips[Locationindex]\n",
        "\n",
        "    if (fips == 6037 or fips == 36061 or fips == 17031 or fips == 53033):\n",
        "      continue\n",
        "    if (fips == fips_b or fips == fips_w):\n",
        "      continue\n",
        "    plot_by_fips(fips, Observations, FitPredictions)\n",
        "      \n",
        "  if LengthFutures > 1:\n",
        "      plot_by_futureindex(2, Observations, FitPredictions)\n",
        "  if LengthFutures > 6:\n",
        "      plot_by_futureindex(7, Observations, FitPredictions)\n",
        "  if LengthFutures > 11:\n",
        "      plot_by_futureindex(12, Observations, FitPredictions)\n",
        "  return\n",
        "\n",
        "def bestandworst(Observations, FitPredictions):\n",
        "    current_time = timenow()\n",
        "    print(startbold +  startred + current_time + ' ' + RunName + \" Best and Worst \" +RunComment + resetfonts)\n",
        "\n",
        "    keepabserrorvalues = np.zeros([Nloc,NumpredbasicperTime], dtype=np.float64)\n",
        "    keepRMSEvalues = np.zeros([Nloc,NumpredbasicperTime], dtype=np.float64)\n",
        "    testabserrorvalues = np.zeros(Nloc, dtype=np.float64)\n",
        "    testRMSEvalues = np.zeros(Nloc, dtype=np.float64)\n",
        "\n",
        "    real = np.zeros([NumpredbasicperTime,Num_Seq], dtype=np.float64)\n",
        "    predictsmall = np.zeros([NumpredbasicperTime,Num_Seq], dtype=np.float64) \n",
        "    c_error_props = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
        "    c_error_props = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
        " \n",
        "  \n",
        "    for icity in range(0,Nloc):\n",
        "      validcounts = np.zeros([NumpredbasicperTime], dtype=np.float64) \n",
        "      RMSE = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "        for itime in range (0,Num_Seq):\n",
        "          if not math.isnan(Observations[itime, icity, PredictedQuantity]):\n",
        "            real[PredictedQuantity,itime] = Observations[itime, icity, PredictedQuantity]\n",
        "            predictsmall[PredictedQuantity,itime] = FitPredictions[itime, icity, PredictedQuantity]\n",
        "            validcounts[PredictedQuantity] += 1.0\n",
        "            RMSE[PredictedQuantity] += (Observations[itime, icity, PredictedQuantity]-FitPredictions[itime, icity, PredictedQuantity])**2\n",
        "        c_error_props[PredictedQuantity] = cumulative_error(predictsmall[PredictedQuantity], real[PredictedQuantity]) # abs(error) as percentage\n",
        "        keepabserrorvalues[icity,PredictedQuantity] = c_error_props[PredictedQuantity]\n",
        "        keepRMSEvalues[icity,PredictedQuantity] = RMSE[PredictedQuantity] *100. / validcounts[PredictedQuantity]\n",
        "\n",
        "      testabserror = 0.0\n",
        "      testRMSE = 0.0\n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "         testabserror += c_error_props[PredictedQuantity]\n",
        "         testRMSE += keepRMSEvalues[icity,PredictedQuantity]\n",
        "      testabserrorvalues[icity] = testabserror\n",
        "      testRMSEvalues[icity] = testRMSE\n",
        "    \n",
        "    sortingindex = np.argsort(testabserrorvalues)\n",
        "    bestindex = sortingindex[0]\n",
        "    worstindex = sortingindex[Nloc-1]\n",
        "    fips_b = Locationfips[bestindex]\n",
        "    fips_w = Locationfips[worstindex]\n",
        "\n",
        "    current_time = timenow()\n",
        "    print( startbold + \"\\n\" + current_time + \" Best \" + str(fips_b) + \" \" + Locationname[bestindex] + \" \" + Locationstate[bestindex] + ' ABS(error) ' + \n",
        "          str(round(testabserrorvalues[bestindex],2)) + ' RMSE ' + str(round(testRMSEvalues[bestindex],2)) + resetfonts)\n",
        "     \n",
        "    for topcities in range(0,10):\n",
        "      localindex = sortingindex[topcities]\n",
        "      printstring = str(topcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] + \" ABS(error) Total \" + str(round(testabserrorvalues[localindex],4)) + \" Components \" \n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "        printstring += ' ' + str(round(keepabserrorvalues[localindex,PredictedQuantity],2))\n",
        "      print(printstring)\n",
        "    print(\"\\nlist RMSE\")\n",
        "    for topcities in range(0,9):\n",
        "      localindex = sortingindex[topcities]\n",
        "      printstring = str(topcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" RMSE Total \" + str(round(testRMSEvalues[localindex],4)) + \" Components \" \n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "        printstring += ' ' + str(round(keepRMSEvalues[localindex,PredictedQuantity],2))\n",
        "      print(printstring)\n",
        "\n",
        "    print( startbold + \"\\n\" + current_time + \" Worst \" + str(fips_w) + \" \" + Locationname[worstindex] + \" \" + Locationstate[worstindex] + ' ABS(error) ' + \n",
        "          str(round(testabserrorvalues[worstindex],2)) + ' RMSE ' + str(round(testRMSEvalues[worstindex],2)) + resetfonts)\n",
        " \n",
        "    for badcities in range(Nloc-1,Nloc-11,-1):\n",
        "      localindex = sortingindex[badcities]\n",
        "      printstring = str(badcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" ABS(error) Total \" + str(round(testabserrorvalues[localindex],4)) + \" Components \" \n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "        printstring += ' ' + str(round(keepabserrorvalues[localindex,PredictedQuantity],2))\n",
        "      print(printstring)\n",
        "    print(\"\\nlist RMSE\")\n",
        "    for badcities in range(0,9):\n",
        "      localindex = sortingindex[badcities]\n",
        "      printstring = str(badcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" RMSE Total \" + str(round(testRMSEvalues[localindex],4)) + \" Components \" \n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "        printstring += ' ' + str(round(keepRMSEvalues[localindex,PredictedQuantity],2))\n",
        "      print(printstring)\n",
        " \n",
        "    return fips_b,fips_w  "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvmhijgwkEFo"
      },
      "source": [
        "def setValTrainlabel(iValTrain):\n",
        "\n",
        "  if SeparateValandTrainingPlots:\n",
        "    if iValTrain == 0:\n",
        "      Overalllabel = 'Training ' \n",
        "      if GlobalTrainingLoss > 0.0001:\n",
        "        Overalllabel += str(round(GlobalTrainingLoss,5)) + ' '\n",
        "    if iValTrain == 1:\n",
        "      Overalllabel = 'Validation '\n",
        "      if GlobalValidationLoss > 0.0001:\n",
        "        Overalllabel += str(round(GlobalValidationLoss,5)) + ' '\n",
        "  else:\n",
        "    Overalllabel = 'Full ' + str(round(GlobalLoss,5)) + ' '\n",
        "  Overalllabel += RunName + ' '\n",
        "  return Overalllabel\n",
        "\n",
        "def Location_summed_plot(Observations, FitPredictions,  fill=True, otherlabs= [], otherfits=[], extracomments = None, Dumpplot = False):\n",
        "    # Only deal with futures as days; plot sum over locations\n",
        "    current_time = timenow()\n",
        "    print(wraptotext(startbold + startred + current_time + ' Location_summed_plot ' + RunName + ' ' + RunComment + resetfonts))\n",
        "    otherlen = len(otherlabs)\n",
        "    basiclength = Num_Seq\n",
        "    predictlength = LengthFutures\n",
        "    if not UseFutures:\n",
        "        predictlength = 0\n",
        "    totallength = basiclength + predictlength\n",
        "    if extracomments is None:\n",
        "      extracomments = []\n",
        "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "        extracomments.append([' ',''])\n",
        "\n",
        "    NumberValTrainLoops = 1\n",
        "    if SeparateValandTrainingPlots:\n",
        "      NumberValTrainLoops = 2\n",
        "        \n",
        "    real = np.zeros([NumpredbasicperTime,NumberValTrainLoops,basiclength])\n",
        "    predictsmall = np.zeros([NumpredbasicperTime,NumberValTrainLoops,basiclength])\n",
        "    predict = np.zeros([NumpredbasicperTime,NumberValTrainLoops,totallength])\n",
        "    if otherlen!=0:\n",
        "      otherpredict = np.zeros([otherlen,NumpredbasicperTime,NumberValTrainLoops, totallength])  \n",
        "\n",
        "      \n",
        "    for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "      for iValTrain in range(0,NumberValTrainLoops):\n",
        "\n",
        "        for iloc in range(0,Nloc):\n",
        "          if SeparateValandTrainingPlots:\n",
        "            if iValTrain == 0:\n",
        "              if MappingtoTraining[iloc] < 0:\n",
        "                continue\n",
        "            if iValTrain == 1:\n",
        "              if MappingtoTraining[iloc] >= 0:\n",
        "                continue\n",
        "          for itime in range (0,Num_Seq):\n",
        "            if np.math.isnan(Observations[itime, iloc, PredictedQuantity]):\n",
        "              real[PredictedQuantity,iValTrain,itime] += FitPredictions[itime, iloc, PredictedQuantity]\n",
        "            else:\n",
        "              real[PredictedQuantity,iValTrain,itime] += Observations[itime, iloc, PredictedQuantity]\n",
        "            predict[PredictedQuantity,iValTrain,itime] += FitPredictions[itime, iloc, PredictedQuantity]\n",
        "            for others in range (0,otherlen):\n",
        "              otherpredict[others,PredictedQuantity,iValTrain,itime] += FitPredictions[itime, iloc, PredictedQuantity] + otherfits[others,itime, iloc, PredictedQuantity]\n",
        "          if FuturedPointer[PredictedQuantity] >= 0:\n",
        "            for ifuture in range(0,LengthFutures):\n",
        "              jfuture = NumpredbasicperTime + NumpredFuturedperTime*ifuture\n",
        "              predict[PredictedQuantity,iValTrain,Num_Seq+ifuture] += FitPredictions[itime, iloc, \n",
        "                                      FuturedPointer[PredictedQuantity] + jfuture] \n",
        "              for others in range (0,otherlen):\n",
        "                otherpredict[others,PredictedQuantity,iValTrain,Num_Seq+ifuture] += FitPredictions[itime, iloc, PredictedQuantity + jfuture] + otherfits[others, itime, iloc, PredictedQuantity + jfuture]\n",
        "        for itime in range(0,basiclength):\n",
        "            predictsmall[PredictedQuantity,iValTrain,itime] = predict[PredictedQuantity,iValTrain,itime]\n",
        "          \n",
        "    error = np.absolute(real - predictsmall)\n",
        "    xsmall = np.arange(0,Num_Seq)\n",
        "\n",
        "    neededrows = math.floor((NumpredbasicperTime*NumberValTrainLoops +1.1)/2)\n",
        "    iValTrain = -1\n",
        "    PredictedQuantity = -1\n",
        "    for rowloop in range(0,neededrows):\n",
        "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
        "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
        "      for kplot in range (0,2):\n",
        "        if NumberValTrainLoops == 2:\n",
        "          iValTrain = kplot\n",
        "        else:\n",
        "          iValTrain = 0\n",
        "        if iValTrain == 0:\n",
        "          PredictedQuantity +=1\n",
        "          if PredictedQuantity > (NumpredbasicperTime-1):\n",
        "            PredictedQuantity = NumpredbasicperTime-1          \n",
        "        Overalllabel = setValTrainlabel(iValTrain)\n",
        "        \n",
        "        eachplt = ax1\n",
        "        if kplot == 1:\n",
        "          eachplt = ax2\n",
        "        \n",
        "        Overalllabel = 'Full '\n",
        "        if SeparateValandTrainingPlots:\n",
        "          if iValTrain == 0:\n",
        "            Overalllabel = 'Training ' \n",
        "            if GlobalTrainingLoss > 0.0001:\n",
        "              Overalllabel += str(round(GlobalTrainingLoss,5)) + ' '\n",
        "          if iValTrain == 1:\n",
        "            Overalllabel = 'Validation '\n",
        "            if GlobalValidationLoss > 0.0001:\n",
        "              Overalllabel += str(round(GlobalValidationLoss,5)) + ' '\n",
        "        else:\n",
        "          Overalllabel += RunName + ' ' + str(round(GlobalLoss,5)) + ' '\n",
        "\n",
        "        maxplot = np.float32(totallength)\n",
        "        if UseRealDatesonplots:\n",
        "          StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + math.floor(Dailyunit/2),'D')\n",
        "          EndDate = StartDate + np.timedelta64(totallength*Dailyunit)\n",
        "          datemin, datemax = makeadateplot(figure,eachplt, datemin=StartDate, datemax=EndDate)\n",
        "          Dateplot = True\n",
        "          Dateaxis = np.empty(totallength, dtype = 'datetime64[D]')\n",
        "          Dateaxis[0] = StartDate\n",
        "          for idate in range(1,totallength):\n",
        "            Dateaxis[idate] = Dateaxis[idate-1] + np.timedelta64(Dailyunit,'D')\n",
        "        else:\n",
        "          Dateplot = False\n",
        "          datemin = 0.0\n",
        "          datemax = maxplot\n",
        "\n",
        "        sumreal = 0.0\n",
        "        sumerror = 0.0\n",
        "        for itime in range(0,Num_Seq):\n",
        "          sumreal += abs(real[PredictedQuantity,iValTrain,itime])\n",
        "          sumerror += error[PredictedQuantity,iValTrain,itime]\n",
        "        c_error = round(100.0*sumerror/sumreal,2)\n",
        "\n",
        "        if UseRealDatesonplots:\n",
        "          eachplt.plot(Dateaxis[0:real.shape[-1]],real[PredictedQuantity,iValTrain,:], label=f'real')\n",
        "          eachplt.plot(Dateaxis,predict[PredictedQuantity,iValTrain,:], label='prediction')            \n",
        "          eachplt.plot(Dateaxis[0:error.shape[-1]],error[PredictedQuantity,iValTrain,:], label=f'error', color=\"red\")\n",
        "          for others in range (0,otherlen):\n",
        "            eachplt.plot(Dateaxis[0:otherpredict.shape[-1]],otherpredict[others,PredictedQuantity,iValTrain,:], label=otherlabs[others])\n",
        "\n",
        "          if fill:\n",
        "            eachplt.fill_between(Dateaxis[0:predictsmall.shape[-1]], predictsmall[PredictedQuantity,iValTrain,:], \n",
        "                                 real[PredictedQuantity,iValTrain,:], alpha=0.1, color=\"grey\")\n",
        "            eachplt.fill_between(Dateaxis[0:error.shape[-1]], error[PredictedQuantity,iValTrain,:], alpha=0.05, color=\"red\")\n",
        "\n",
        "        else:\n",
        "          eachplt.plot(real[PredictedQuantity,iValTrain,:], label=f'real')\n",
        "          eachplt.plot(predict[PredictedQuantity,iValTrain,:], label='prediction')\n",
        "          eachplt.plot(error[PredictedQuantity,iValTrain,:], label=f'error', color=\"red\")\n",
        "          for others in range (0,otherlen):\n",
        "            eachplt.plot(otherpredict[others,PredictedQuantity,iValTrain,:], label=otherlabs[others])\n",
        "\n",
        "          if fill:\n",
        "            eachplt.fill_between(xsmall, predictsmall[PredictedQuantity,iValTrain,:], real[PredictedQuantity,iValTrain,:], \n",
        "                                 alpha=0.1, color=\"grey\")\n",
        "            eachplt.fill_between(xsmall, error[PredictedQuantity,iValTrain,:], alpha=0.05, color=\"red\")\n",
        "\n",
        "        extrastring = Overalllabel + current_time + ' ' + RunName + \" \" \n",
        "        extrastring += f\"Length={Num_Seq}, Location Summed Results {Predictionbasicname[PredictedQuantity]}, \"\n",
        "        extrastring += extracomments[PredictedQuantity][iValTrain]\n",
        "        eachplt.set_title('\\n'.join(wrap(extrastring,70)))\n",
        "        if Dateplot:\n",
        "          eachplt.set_xlabel('Years')\n",
        "        else:\n",
        "          eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
        "        eachplt.set_ylabel(Predictionbasicname[PredictedQuantity])\n",
        "        eachplt.grid(False)\n",
        "        eachplt.legend()\n",
        "      figure.tight_layout()\n",
        "      if Dumpplot and Dumpoutkeyplotsaspics:\n",
        "        VT = 'Both'\n",
        "        if NumberValTrainLoops == 1:\n",
        "          VT='Full'\n",
        "        plt.savefig(APPLDIR +'/Outputs/DLResults' + VT + str(PredictedQuantity) +RunName + '.png ',format='png')\n",
        "      plt.show()\n",
        "\n",
        "# Produce more detailed plots in time\n",
        "    splitsize = Plotsplitsize\n",
        "    if splitsize <= 1:\n",
        "      return\n",
        "    Numpoints = math.floor((Num_Seq+0.001)/splitsize)\n",
        "    extraone = Num_Seq%Numpoints\n",
        "\n",
        "    neededrows = math.floor((splitsize*NumberValTrainLoops +1.1)/2)\n",
        "    iValTrain = -1\n",
        "    PredictedQuantity = 0\n",
        "    iseqnew = 0\n",
        "    counttimes = 0\n",
        "    for rowloop in range(0,neededrows):\n",
        "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
        "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
        "      for kplot in range (0,2):\n",
        "        if NumberValTrainLoops == 2:\n",
        "          iValTrain = kplot\n",
        "        else:\n",
        "          iValTrain = 0\n",
        "        Overalllabel = setValTrainlabel(iValTrain)\n",
        "        eachplt = ax1\n",
        "        if kplot == 1:\n",
        "          eachplt = ax2\n",
        "        sumreal = 0.0\n",
        "        sumerror = 0.0\n",
        "\n",
        "        if iValTrain == 0:\n",
        "          iseqold = iseqnew\n",
        "          iseqnew = iseqold + Numpoints\n",
        "          if counttimes < extraone:\n",
        "            iseqnew +=1\n",
        "          counttimes += 1\n",
        "        for itime in range(iseqold,iseqnew):\n",
        "          sumreal += abs(real[PredictedQuantity,iValTrain,itime])\n",
        "          sumerror += error[PredictedQuantity,iValTrain,itime]\n",
        "        c_error = round(100.0*sumerror/sumreal,2)\n",
        "\n",
        "        eachplt.plot(xsmall[iseqold:iseqnew],predict[PredictedQuantity,iValTrain,iseqold:iseqnew], label='prediction')\n",
        "        eachplt.plot(xsmall[iseqold:iseqnew],real[PredictedQuantity,iValTrain,iseqold:iseqnew], label=f'real')\n",
        "        eachplt.plot(xsmall[iseqold:iseqnew],error[PredictedQuantity,iValTrain,iseqold:iseqnew], label=f'error', color=\"red\")\n",
        "\n",
        "        if fill:\n",
        "            eachplt.fill_between(xsmall[iseqold:iseqnew], predictsmall[PredictedQuantity,iValTrain,iseqold:iseqnew], real[PredictedQuantity,iseqold:iseqnew], alpha=0.1, color=\"grey\")\n",
        "            eachplt.fill_between(xsmall[iseqold:iseqnew], error[PredictedQuantity,iValTrain,iseqold:iseqnew], alpha=0.05, color=\"red\")\n",
        "\n",
        "        extrastring = Overalllabel + current_time + ' ' + RunName + \" \" + f\"Range={iseqold}, {iseqnew} Rel Error {c_error} Location Summed Results {Predictionbasicname[PredictedQuantity]}, \"\n",
        "        eachplt.set_title('\\n'.join(wrap(extrastring,70)))\n",
        "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
        "        eachplt.set_ylabel(Predictionbasicname[PredictedQuantity])\n",
        "        eachplt.grid(True)\n",
        "        eachplt.legend()\n",
        "      figure.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "def normalizeforplot(casesdeath,Locationindex,value):\n",
        "\n",
        "    if np.math.isnan(value):\n",
        "      return value\n",
        "    if Plotrealnumbers:\n",
        "      predaveragevaluespointer = PredictionAverageValuesPointer[casesdeath]\n",
        "      newvalue = value/QuantityStatistics[predaveragevaluespointer,2] + QuantityStatistics[predaveragevaluespointer,0]\n",
        "      rootflag = QuantityTakeroot[predaveragevaluespointer]\n",
        "      if rootflag == 2:\n",
        "        newvalue = newvalue**2\n",
        "      if rootflag == 3:\n",
        "        newvalue = newvalue**3\n",
        "    else:\n",
        "      newvalue = value\n",
        "    if PopulationNorm:\n",
        "      newvalue *= Locationpopulation[Locationindex]\n",
        "    return newvalue\n",
        "\n",
        "# PLOT individual city data\n",
        "def plot_by_fips(fips, Observations, FitPredictions, dots=True, fill=True):\n",
        "    Locationindex = FIPSintegerlookup[fips]\n",
        "    current_time = timenow()\n",
        "    print(startbold + startred + current_time + ' plot by location ' + str(Locationindex) + ' ' + str(fips) + ' ' + Locationname[Locationindex] + ' ' +RunName + ' ' + RunComment + resetfonts)\n",
        "\n",
        "    basiclength = Num_Seq\n",
        "    predictlength = LengthFutures\n",
        "    if not UseFutures:\n",
        "        predictlength = 0\n",
        "    totallength = basiclength + predictlength\n",
        "    real = np.zeros([NumpredbasicperTime,basiclength])\n",
        "    predictsmall = np.zeros([NumpredbasicperTime,basiclength])\n",
        "    predict = np.zeros([NumpredbasicperTime,totallength]) \n",
        "\n",
        "    for PredictedQuantity in range(0,NumpredbasicperTime):\n",
        "      for itime in range (0,Num_Seq):\n",
        "        if np.math.isnan(Observations[itime, Locationindex, PredictedQuantity]):\n",
        "          Observations[itime, Locationindex, PredictedQuantity] = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
        "        else:\n",
        "          real[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, Observations[itime, Locationindex, PredictedQuantity])\n",
        "          predict[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, Locationindex, PredictedQuantity])\n",
        "      if FuturedPointer[PredictedQuantity] >= 0:\n",
        "        for ifuture in range(0,LengthFutures):\n",
        "          jfuture = NumpredbasicperTime + NumpredFuturedperTime*ifuture\n",
        "          predict[PredictedQuantity,Num_Seq+ifuture] += normalizeforplot(PredictedQuantity,Locationindex, \n",
        "                                          FitPredictions[itime, Locationindex, FuturedPointer[PredictedQuantity] + jfuture])\n",
        "      for itime in range(0,basiclength):\n",
        "          predictsmall[PredictedQuantity,itime] = predict[PredictedQuantity,itime]\n",
        "        \n",
        "    error = np.absolute(real - predictsmall)\n",
        "    xsmall = np.arange(0,Num_Seq)\n",
        "\n",
        "    neededrows = math.floor((NumpredbasicperTime +1.1)/2)\n",
        "    iplot = -1\n",
        "    for rowloop in range(0,neededrows):\n",
        "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
        "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
        "      for kplot in range (0,2):\n",
        "        iplot +=1\n",
        "        if iplot > (NumpredbasicperTime-1):\n",
        "          iplot = NumpredbasicperTime-1\n",
        "        eachplt = ax1\n",
        "        if kplot == 1:\n",
        "          eachplt = ax2\n",
        "\n",
        "        sumreal = 0.0\n",
        "        sumerror = 0.0\n",
        "        for itime in range(0,Num_Seq):\n",
        "          sumreal += abs(real[iplot,itime])\n",
        "          sumerror += error[iplot,itime]\n",
        "        c_error = round(100.0*sumerror/sumreal,2)\n",
        "        RMSEstring = ''\n",
        "        if not Plotrealnumbers:\n",
        "          sumRMSE = 0.0\n",
        "          count = 0.0\n",
        "          for itime in range(0,Num_Seq):\n",
        "            sumRMSE += (real[iplot,itime] - predict[iplot,itime])**2\n",
        "            count += 1.0\n",
        "          RMSE_error = round(100.0*sumRMSE/count,4)\n",
        "          RMSEstring = ' RMSE ' + str(RMSE_error)\n",
        "\n",
        "        x = list(range(0, totallength))\n",
        "        if dots:\n",
        "            eachplt.scatter(x, predict[iplot])\n",
        "            eachplt.scatter(xsmall, real[iplot])\n",
        "\n",
        "        eachplt.plot(predict[iplot], label=f'{fips} prediction')\n",
        "        eachplt.plot(real[iplot], label=f'{fips} real')\n",
        "        eachplt.plot(error[iplot], label=f'{fips} error', color=\"red\")\n",
        "        if fill:\n",
        "            eachplt.fill_between(xsmall, predictsmall[iplot], real[iplot], alpha=0.1, color=\"grey\")\n",
        "            eachplt.fill_between(xsmall, error[iplot], alpha=0.05, color=\"red\")\n",
        "\n",
        "        name = Locationname[Locationindex]\n",
        "        if Plotrealnumbers:\n",
        "            name = \"Actual Numbers \" + name\n",
        "        stringpopulation = \" \"\n",
        "\n",
        "        stringpopulation = \" Population \" +str(Locationpopulation[Locationindex])\n",
        "\n",
        "        titlestring = current_time + ' ' + RunName + f\" {name}, Label={fips}\" + stringpopulation + f\" Length={Num_Seq}, Abs Rel Error={c_error}%\" + RMSEstring + ' ' + RunName\n",
        "        eachplt.set_title('\\n'.join(wrap(titlestring,70)))\n",
        "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
        "        eachplt.set_ylabel(Predictionbasicname[iplot])\n",
        "        eachplt.grid(True)\n",
        "        eachplt.legend()\n",
        "\n",
        "      figure.tight_layout()\n",
        "      plt.show();\n",
        "\n",
        "\n",
        "def cumulative_error(real,predicted):\n",
        "  error = np.absolute(real-predicted).sum()\n",
        "  basevalue = np.absolute(real).sum()\n",
        "  return 100.0*error/basevalue\n",
        "\n",
        "# Plot summed results by Prediction Type\n",
        "# selectedfuture one more than usual future index\n",
        "def plot_by_futureindex(selectedfuture, Observations, FitPredictions, fill=True, extrastring=''):\n",
        "    current_time = timenow()\n",
        "    print(startbold + startred + current_time + ' plot by Future Index ' + str(selectedfuture) + ' ' + RunName + ' ' + RunComment + resetfonts)\n",
        "\n",
        "    selectedfield = NumpredbasicperTime + NumpredFuturedperTime*(selectedfuture-1)\n",
        "    if selectedfuture == 0:\n",
        "      selectedfield = 0\n",
        "    real = np.zeros([NumpredFuturedperTime,Num_Seq])\n",
        "    predictsmall = np.zeros([NumpredFuturedperTime,Num_Seq])\n",
        "    validdata = 0\n",
        "\n",
        "    for PredictedQuantity in range(0,NumpredFuturedperTime):\n",
        "      for iloc in range(0,Nloc):\n",
        "        for itime in range (0,Num_Seq):\n",
        "          real[PredictedQuantity,itime] += Observations[itime, iloc, selectedfield+PredictedQuantity]\n",
        "          predictsmall[PredictedQuantity,itime] += FitPredictions[itime, iloc, selectedfield+PredictedQuantity]\n",
        "      for itime in range (0,Num_Seq):\n",
        "        if np.math.isnan(real[PredictedQuantity,itime]):\n",
        "            real[PredictedQuantity,itime] = predictsmall[PredictedQuantity,itime]\n",
        "        else:\n",
        "            if PredictedQuantity == 0:\n",
        "              validdata += 1    \n",
        "\n",
        "    error = np.absolute(real - predictsmall)\n",
        "    xsmall = np.arange(0,Num_Seq)\n",
        "\n",
        "    neededrows = math.floor((NumpredFuturedperTime +1.1)/2)\n",
        "    iplot = -1\n",
        "    for rowloop in range(0,neededrows):\n",
        "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
        "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
        "      for kplot in range (0,2):\n",
        "        iplot +=1\n",
        "        if iplot > (NumpredbasicperTime-1):\n",
        "          iplot = NumpredbasicperTime-1\n",
        "        eachplt = ax1\n",
        "        if kplot == 1:\n",
        "          eachplt = ax2\n",
        "        sumreal = 0.0\n",
        "        sumerror = 0.0\n",
        "        for itime in range(0,Num_Seq):\n",
        "          sumreal += abs(real[iplot,itime])\n",
        "          sumerror += error[iplot,itime]\n",
        "        c_error = round(100.0*sumerror/sumreal,2)\n",
        "\n",
        "        eachplt.plot(predictsmall[iplot,:], label='prediction')\n",
        "        eachplt.plot(real[iplot,:], label=f'real')\n",
        "        eachplt.plot(error[iplot,:], label=f'error', color=\"red\")\n",
        "\n",
        "        if fill:\n",
        "            eachplt.fill_between(xsmall, predictsmall[iplot,:], real[iplot,:], alpha=0.1, color=\"grey\")\n",
        "            eachplt.fill_between(xsmall, error[iplot,:], alpha=0.05, color=\"red\")\n",
        "        errorstring= \" Error % \" + str(c_error)\n",
        "        printstring = current_time + \" Future Index \" + str(selectedfuture) + \" \" + RunName \n",
        "        printstring += \" \" + f\"Length={Num_Seq}, Location Summed Results {Predictionbasicname[iplot]}, \" + errorstring + \" \" + extrastring\n",
        "        eachplt.set_title('\\n'.join(wrap(printstring,70)))\n",
        "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
        "        eachplt.set_ylabel(Predictionbasicname[iplot])\n",
        "        eachplt.grid(True)\n",
        "        eachplt.legend()\n",
        "      figure.tight_layout()\n",
        "      plt.show()\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3_zcJpzkFsq"
      },
      "source": [
        "# Calculate NNSE\n",
        "# Sum (Obsevations - Mean)^2 / [Sum (Obsevations - Mean)^2 + Sum(Observations-Predictions)^2]\n",
        "def FindNNSE(Observations, FitPredictions, Label=''):\n",
        "\n",
        "  NNSEList = np.empty(NpredperseqTOT, dtype = np.int)\n",
        "  NumberNNSEcalc = 0\n",
        "  for ipred in range(0,NpredperseqTOT):\n",
        "    if CalculateNNSE[ipred]:\n",
        "      NNSEList[NumberNNSEcalc] = ipred\n",
        "      NumberNNSEcalc +=1\n",
        "  if NumberNNSEcalc == 0:\n",
        "    return\n",
        "  StoreNNSE = np.zeros([Nloc,NumberNNSEcalc], dtype = np.float64)\n",
        "  basiclength = Num_Seq\n",
        "\n",
        "  current_time = timenow()\n",
        "  print(wraptotext(startbold + startred + current_time + ' Calculate NNSE ' + Label + ' ' +RunName + ' ' + RunComment + resetfonts))\n",
        "  for NNSEpredindex in range(0,NumberNNSEcalc):\n",
        "    PredictedQuantity = NNSEList[NNSEpredindex]\n",
        "    averageNNSE = 0.0\n",
        "    averageNNSETraining = 0.0\n",
        "    averageNNSEValidation = 0.0\n",
        "    line = ''\n",
        "    for Locationindex in range(0, Nloc):\n",
        "      QTObssq = 0.0\n",
        "      QTDiffsq = 0.0\n",
        "      QTObssum = 0.0\n",
        "      for itime in range (0,Num_Seq):\n",
        "        Observed = Observations[itime, Locationindex, PredictedQuantity]\n",
        "        if np.math.isnan(Observed):\n",
        "          Observed = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
        "        real = normalizeforplot(PredictedQuantity, Locationindex, Observed)\n",
        "        predict = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, \n",
        "                                    Locationindex, PredictedQuantity])\n",
        "        QTObssq += real**2\n",
        "        QTDiffsq += (real-predict)**2\n",
        "        QTObssum += real\n",
        "      Obsmeasure = QTObssq - (QTObssum**2 / Num_Seq )\n",
        "      StoreNNSE[Locationindex,NNSEpredindex] =  Obsmeasure / (Obsmeasure +QTDiffsq )\n",
        "      if MappingtoTraining[Locationindex] >= 0:\n",
        "        averageNNSETraining += StoreNNSE[Locationindex,NNSEpredindex]\n",
        "      if MappingtoValidation[Locationindex] >= 0:\n",
        "        averageNNSEValidation += StoreNNSE[Locationindex,NNSEpredindex]\n",
        "      averageNNSE += StoreNNSE[Locationindex,NNSEpredindex]\n",
        "      line += str(round(StoreNNSE[Locationindex,NNSEpredindex],3)) + ' '\n",
        "    \n",
        "    if ValidationNloc > 0:\n",
        "      averageNNSEValidation = averageNNSEValidation / ValidationNloc\n",
        "    averageNNSETraining = averageNNSETraining / TrainingNloc\n",
        "    averageNNSE = averageNNSE / Nloc\n",
        "\n",
        "# Location Summed    \n",
        "    QTObssq = 0.0\n",
        "    QTDiffsq = 0.0\n",
        "    QTObssum = 0.0\n",
        "    QTObssqT = 0.0\n",
        "    QTDiffsqT = 0.0\n",
        "    QTObssumT = 0.0\n",
        "    QTObssqV = 0.0\n",
        "    QTDiffsqV = 0.0\n",
        "    QTObssumV = 0.0\n",
        "    for itime in range (0,Num_Seq):\n",
        "      real = 0.0\n",
        "      predict = 0.0\n",
        "      realT = 0.0\n",
        "      predictT = 0.0\n",
        "      realV = 0.0\n",
        "      predictV = 0.0\n",
        "      for Locationindex in range(0, Nloc):\n",
        "        Observed = Observations[itime, Locationindex, PredictedQuantity]\n",
        "        if np.math.isnan(Observed):\n",
        "          Observed = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
        "        localreal = normalizeforplot(PredictedQuantity, Locationindex, Observed)\n",
        "        localpredict = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, \n",
        "                                    Locationindex, PredictedQuantity])\n",
        "        real += localreal\n",
        "        predict += localpredict\n",
        "        if MappingtoTraining[Locationindex] >= 0:\n",
        "          realT += localreal\n",
        "          predictT += localpredict\n",
        "        if MappingtoValidation[Locationindex] >= 0:\n",
        "          realV  += localreal\n",
        "          predictV += localpredict\n",
        "\n",
        "      QTObssq += real**2\n",
        "      QTDiffsq += (real-predict)**2\n",
        "      QTObssum += real\n",
        "      QTObssqT += realT**2\n",
        "      QTDiffsqT += (realT-predictT)**2\n",
        "      QTObssumT += realT\n",
        "      QTObssqV += realV**2\n",
        "      QTDiffsqV += (realV-predictV)**2\n",
        "      QTObssumV += realV\n",
        "    Obsmeasure = QTObssq - (QTObssum**2 / Num_Seq )\n",
        "    SummedNNSE =  Obsmeasure / (Obsmeasure +QTDiffsq )\n",
        "    ObsmeasureT = QTObssqT - (QTObssumT**2 / Num_Seq )\n",
        "    SummedNNSET =  ObsmeasureT / (ObsmeasureT +QTDiffsqT )\n",
        "    ObsmeasureV = QTObssqV - (QTObssumV**2 / Num_Seq )\n",
        "    if ValidationNloc > 0:\n",
        "      SummedNNSEV =  ObsmeasureV / (ObsmeasureV +QTDiffsqV )\n",
        "    else:\n",
        "      SummedNNSEV =  0.0\n",
        "\n",
        "    print(wraptotext('NNSE ' + startbold + Label + ' ' + str(PredictedQuantity) + ' ' + Predictionname[PredictedQuantity] + startred + ' Averaged ' +\n",
        "          str(round(averageNNSE,3)) + resetfonts + ' Training ' + str(round(averageNNSETraining,3)) +\n",
        "          ' Validation ' + str(round(averageNNSEValidation,3)) + startred + startbold + ' Summed ' +  \n",
        "          str(round(SummedNNSE,3)) + resetfonts + ' Training ' + str(round(SummedNNSET,3)) +\n",
        "          ' Validation ' + str(round(SummedNNSEV,3))))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oMmDYvokHSU"
      },
      "source": [
        "def custom_lossGCF1(y_actual,y_pred):\n",
        "    tupl = np.shape(y_actual)\n",
        "    flagGCF = tf.math.is_nan(y_actual)\n",
        "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
        "    tensordiff = tf.math.reduce_sum(tf.math.square(y_actual-y_pred))\n",
        "\n",
        "    if len(tupl) >= 2:\n",
        "      tensordiff /= tupl[0]\n",
        "    if len(tupl) >= 3:\n",
        "      tensordiff /= tupl[1]\n",
        "    if len(tupl) >= 4:\n",
        "      tensordiff /= tupl[2]\n",
        "    return tensordiff\n",
        "\n",
        "@tf.autograph.experimental.do_not_convert\n",
        "def custom_lossGCF1spec(y_actual,y_pred):\n",
        "    global tensorsw\n",
        "    tupl = np.shape(y_actual)\n",
        "    flagGCF = tf.math.is_nan(y_actual)\n",
        "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
        "    sw = tensorsw[tf.math.logical_not(flagGCF)]\n",
        "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
        "\n",
        "    if len(tupl) >= 2:\n",
        "      tensordiff /= tupl[0]\n",
        "    if len(tupl) >= 3:\n",
        "      tensordiff /= tupl[1]\n",
        "    if len(tupl) >= 4:\n",
        "      tensordiff /= tupl[2]\n",
        "    return tensordiff\n",
        "\n",
        "def custom_lossGCF1A(y_actual,y_pred):\n",
        "    print(np.shape(y_actual), np.shape(y_pred))\n",
        "    flagGCF = tf.math.is_nan(y_actual)\n",
        "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
        "    tensordiff = tf.math.square(y_actual-y_pred)\n",
        "    return tf.math.reduce_mean(tensordiff)\n",
        "\n",
        "# Basic TF does NOT supply sample_weight\n",
        "def custom_lossGCF1B(y_actual,y_pred,sample_weight=None):\n",
        "    tupl = np.shape(y_actual)\n",
        "\n",
        "    flagGCF = tf.math.is_nan(y_actual)\n",
        "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
        "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
        "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
        "    if len(tupl) >= 2:\n",
        "      tensordiff /= tupl[0]\n",
        "    if len(tupl) >= 3:\n",
        "      tensordiff /= tupl[1]\n",
        "    if len(tupl) >= 4:\n",
        "      tensordiff /= tupl[2]\n",
        "    return tensordiff\n",
        "    \n",
        "def custom_lossGCF4(y_actual,y_pred):\n",
        "    tensordiff = y_actual-y_pred\n",
        "    newtensordiff = tf.where(tf.math.is_nan(tensordiff), tf.zeros_like(tensordiff), tensordiff)\n",
        "    return tf.math.reduce_mean(tf.math.square(newtensordiff))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSJcOq-akI17"
      },
      "source": [
        "def SetSpacetime(BasicTimes):\n",
        "  global GlobalTimeMask\n",
        "  Time = None\n",
        "  if (MaskingOption == 0) or (not GlobalSpacetime):\n",
        "    return Time\n",
        "  NumTOTAL = BasicTimes.shape[1]\n",
        "  BasicTimes = BasicTimes.astype(np.int16)\n",
        "  BasicTimes = np.reshape(BasicTimes,(BasicTimes.shape[0],NumTOTAL,1))\n",
        "  addons = np.arange(0,Tseq,dtype =np.int16)\n",
        "  addons = np.reshape(addons,(1,1,Tseq))\n",
        "  Time = BasicTimes+addons\n",
        "  Time = np.reshape(Time,(BasicTimes.shape[0], NumTOTAL*Tseq))\n",
        "  BasicPureTime = np.arange(0,Tseq,dtype =np.int16) \n",
        "  BasicPureTime = np.reshape(BasicPureTime,(Tseq,1))\n",
        "  GlobalTimeMask = tf.where( (BasicPureTime-np.transpose(BasicPureTime))>0, 0.0,1.0)\n",
        "  GlobalTimeMask = np.reshape(GlobalTimeMask,(1,1,1,Tseq,Tseq))\n",
        "  return Time\n",
        "\n",
        "def shuffleDLinput(Xin,yin,AuxiliaryArray=None, Spacetime=None):\n",
        " # Auxiliary array could be weight or location/time tracker\n",
        " # These are per batch so sorted axis is first\n",
        "  \n",
        "  np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))\n",
        "  trainingorder = list(range(0, len(Xin)))\n",
        "  random.shuffle(trainingorder)\n",
        "\n",
        "  Xinternal = list()\n",
        "  yinternal = list()\n",
        "  if AuxiliaryArray is not None:\n",
        "    AuxiliaryArrayinternal = list()\n",
        "  if Spacetime is not None:\n",
        "    Spacetimeinternal = list()\n",
        "  for i in trainingorder:\n",
        "    Xinternal.append(Xin[i])\n",
        "    yinternal.append(yin[i])\n",
        "    if AuxiliaryArray is not None:\n",
        "      AuxiliaryArrayinternal.append(AuxiliaryArray[i])\n",
        "    if Spacetime is not None:\n",
        "      Spacetimeinternal.append(Spacetime[i])\n",
        "  X = np.array(Xinternal)\n",
        "  y = np.array(yinternal)\n",
        "  if (AuxiliaryArray is None) and (Spacetime is None):\n",
        "    return X, y\n",
        "  if (AuxiliaryArray is not None) and (Spacetime is None):\n",
        "    AA = np.array(AuxiliaryArrayinternal)\n",
        "    return X,y,AA\n",
        "  if (AuxiliaryArray is None) and (Spacetime is not None):\n",
        "    St = np.array(Spacetimeinternal)\n",
        "    return X,y,St\n",
        "  AA = np.array(AuxiliaryArrayinternal)\n",
        "  St = np.array(Spacetimeinternal)\n",
        "  return X,y,AA,St\n",
        "\n",
        "# Simple Plot of Loss from history\n",
        "def finalizeDL(ActualModel, recordtrainloss, recordvalloss, validationfrac, X_in, y_in, modelflag, LabelFit =''):\n",
        "  \n",
        "  if len(recordtrainloss) == 0:\n",
        "    recordtrainloss = [0]\n",
        "\n",
        "  histlen = len(recordtrainloss)\n",
        "\n",
        "  trainloss = recordtrainloss[histlen-1]\n",
        "  plt.rcParams[\"figure.figsize\"] = [8,6]\n",
        "  plt.plot(recordtrainloss)\n",
        "  if (validationfrac > 0.001) and len(recordvalloss) > 0:\n",
        "    valloss = recordvalloss[histlen-1]\n",
        "    plt.plot(recordvalloss)\n",
        "  else:\n",
        "    valloss = 0.0\n",
        "  \n",
        "  current_time = timenow()\n",
        "  print(startbold + startred + current_time + ' ' + RunName + ' finalizeDL ' + RunComment +resetfonts)\n",
        "  plt.title(LabelFit + ' ' + RunName+' model loss ' + str(round(trainloss,7)) + ' Val ' + str(round(valloss,7)))\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.yscale(\"log\")\n",
        "  plt.grid(True)\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  FitPredictions = DLprediction(X_in, y_in,ActualModel,modelflag, LabelFit = LabelFit)\n",
        "  for debugfips in ListofTestFIPS:\n",
        "    if debugfips != '': \n",
        "      debugfipsoutput(debugfips, FitPredictions, X_in, y_in)\n",
        "  return\n",
        "\n",
        "def debugfipsoutput(debugfips, FitPredictions, Xin, Observations):\n",
        "\n",
        "  print(startbold + startred + 'debugfipsoutput for ' + str(debugfips) + RunName + ' ' + RunComment +resetfonts)\n",
        "# Set Location Number in Arrays\n",
        "  LocationNumber = FIPSstringlookup[debugfips]\n",
        "\n",
        "  # Sequences to look at\n",
        "  Seqcount = 5\n",
        "  Seqnumber =  np.empty(Seqcount, dtype = np.int)\n",
        "  Seqnumber[0] = 0\n",
        "  Seqnumber[1] = int(Num_Seq/4)-1\n",
        "  Seqnumber[2] = int(Num_Seq/2)-1\n",
        "  Seqnumber[3] = int((3*Num_Seq)/4) -1\n",
        "  Seqnumber[4] = Num_Seq-1\n",
        "\n",
        "  # Window Positions to look at\n",
        "  Wincount = 5\n",
        "  Winnumber = np.empty(Wincount, dtype = np.int)\n",
        "  Winnumber[0] = 0\n",
        "  Winnumber[1] = int(Tseq/4)-1\n",
        "  Winnumber[2] = int(Tseq/2)-1\n",
        "  Winnumber[3] = int((3*Tseq)/4) -1\n",
        "  Winnumber[4] = Tseq-1\n",
        "\n",
        "  if SymbolicWindows:\n",
        "    InputSequences = np.empty([Seqcount,Wincount, NpropperseqTOT], dtype=np.float32)\n",
        "    for jseq in range(0,Seqcount):\n",
        "      iseq = Seqnumber[jseq]\n",
        "      for jwindow in range(0,Wincount):\n",
        "        window = Winnumber[jwindow]\n",
        "        InputSequences[jseq,jwindow] = Xin[LocationNumber,iseq+jseq]\n",
        "  else:\n",
        "    InputSequences = Xin \n",
        "\n",
        "  # Location Info\n",
        " \n",
        "  print('\\n' + startbold + startred + debugfips + ' # ' + str(LocationNumber) + ' ' +\n",
        "        Locationname[LocationNumber] + ' ' + Locationstate[LocationNumber] + ' Pop '\n",
        "        + str(Locationpopulation[LocationNumber]) + resetfonts)\n",
        "  plot_by_fips(int(debugfips), Observations, FitPredictions)\n",
        " \n",
        "  if PlotsOnlyinTestFIPS:\n",
        "    return\n",
        "    \n",
        "  # Print Input Data to Test\n",
        "  # Static Properties\n",
        "  print(startbold + startred + 'Static Properties ' + debugfips + ' ' +\n",
        "         Locationname[LocationNumber] + resetfonts)\n",
        "  line = ''\n",
        "  for iprop in range(0,NpropperTimeStatic):\n",
        "    if SymbolicWindows:\n",
        "      val = InputSequences[0,0,iprop]\n",
        "    else:\n",
        "      val = InputSequences[0,LocationNumber,0,iprop]\n",
        "    line += startbold + InputPropertyNames[PropertyNameIndex[iprop]] + resetfonts + ' ' + str(round(val,3)) + ' '\n",
        "  print('\\n'.join(wrap(line,200)))\n",
        "\n",
        " # Dynamic Properties\n",
        "  for iprop in range(NpropperTimeStatic, NpropperTime):\n",
        "    print('\\n')\n",
        "    for jwindow in range(0,Wincount):\n",
        "      window = Winnumber[jwindow]\n",
        "      line = startbold + InputPropertyNames[PropertyNameIndex[iprop]] + ' W= '+str(window) +resetfonts\n",
        "      for jseq in range(0,Seqcount):\n",
        "        iseq = Seqnumber[jseq]\n",
        "        line += startbold + startred + ' ' + str(iseq) + ')' +resetfonts\n",
        "        if SymbolicWindows:\n",
        "          val = InputSequences[jseq,jwindow,iprop]\n",
        "        else:\n",
        "          val = InputSequences[iseq,LocationNumber,window,iprop]\n",
        "        line +=   ' ' + str(round(val,3))\n",
        "      print('\\n'.join(wrap(line,200)))\n",
        "  \n",
        "\n",
        "  # Total Input\n",
        "  print('\\n')\n",
        "  line = startbold + 'Props: ' + resetfonts \n",
        "  for iprop in range(0,NpropperseqTOT):\n",
        "    if iprop%5 == 0:\n",
        "      line += startbold + startred + ' ' + str(iprop) + ')' + resetfonts     \n",
        "    line += ' ' + InputPropertyNames[PropertyNameIndex[iprop]]\n",
        "  print('\\n'.join(wrap(line,200)))\n",
        "  for jseq in range(0,Seqcount):\n",
        "    iseq = Seqnumber[jseq]\n",
        "    for jwindow in range(0,Wincount):\n",
        "      window = Winnumber[jwindow]\n",
        "      line = startbold + 'Input: All in Seq ' + str(iseq) + ' W= ' + str(window) + resetfonts\n",
        "      for iprop in range(0,NpropperseqTOT):\n",
        "        if iprop%5 == 0:\n",
        "          line += startbold + startred + ' ' + str(iprop) + ')' +resetfonts\n",
        "        if SymbolicWindows:\n",
        "          val = InputSequences[jseq,jwindow,iprop]\n",
        "        else:\n",
        "          val = InputSequences[iseq,LocationNumber,window,iprop]\n",
        "        result = str(round(val,3))\n",
        "        line += ' ' + result\n",
        "      print('\\n'.join(wrap(line,200)))\n",
        "\n",
        "  # Total Prediction\n",
        "  print('\\n')\n",
        "  line = startbold + 'Preds: ' + resetfonts \n",
        "  for ipred in range(0,NpredperseqTOT):\n",
        "    if ipred%5 == 0:\n",
        "      line += startbold + startred + ' ' + str(ipred) + ')' + resetfonts     \n",
        "    line += ' ' + Predictionname[ipred]\n",
        "  for jseq in range(0,Seqcount):\n",
        "    iseq = Seqnumber[jseq]\n",
        "    line = startbold + 'Preds: All in Seq ' + str(iseq) + resetfonts\n",
        "    for ipred in range(0,NpredperseqTOT):\n",
        "      fred = Observations[iseq,LocationNumber,ipred]\n",
        "      if np.math.isnan(fred):\n",
        "        result = 'NaN'\n",
        "      else:\n",
        "        result = str(round(fred,3))\n",
        "      if ipred%5 == 0:\n",
        "          line += startbold + startred + ' ' + str(ipred) + ')' + resetfonts     \n",
        "      line += ' ' + result\n",
        "    print('\\n'.join(wrap(line,200)))   \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CrCfnHykJgu"
      },
      "source": [
        "\n",
        "def printloss(name,mean,var,SampleSize, lineend =''):\n",
        "  mean /= SampleSize\n",
        "  var /= SampleSize\n",
        "  std = math.sqrt(var - mean**2)\n",
        "  print(name + ' Mean ' + str(round(mean,5)) + ' Std Deviation ' + str(round(std,7)) + ' ' + lineend)\n",
        "  \n",
        "def DLprediction2E(Xin, yin, DLmodel, modelflag):\n",
        "  # Form restricted Attention separately over Training and Validation\n",
        "\n",
        "  if not LocationBasedValidation:\n",
        "    return\n",
        "  if UsedTransformervalidationfrac < 0.001 or ValidationNloc <= 0:\n",
        "    return\n",
        "  if SkipDL2E:\n",
        "    return\n",
        "  if GarbageCollect:\n",
        "    gc.collect()\n",
        "\n",
        "  SampleSize = 1\n",
        "  FitRanges_PartialAtt = np.zeros([Num_Seq, Nloc, NpredperseqTOT,5], dtype =np.float32)\n",
        "  FRanges = np.full((NpredperseqTOT), 1.0, dtype = np.float32)\n",
        "  # 0 count 1 mean 2 Standard Deviation 3 Min 4 Max\n",
        "\n",
        "  print(wraptotext(startbold+startred+ 'DLPrediction2E Partial Attention ' +current_time + ' ' + RunName + RunComment +  resetfonts))\n",
        "\n",
        "  global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
        "\n",
        "  global FullSetValidation\n",
        "  saveFullSetValidation = FullSetValidation\n",
        "  FullSetValidation = False\n",
        "  X_predict, y_predict, Spacetime_predict, X_val, y_val, Spacetime_val = setSeparateDLinput(1, Spacetime = True)\n",
        "  FullSetValidation = saveFullSetValidation\n",
        "\n",
        "  Nloc_sample = TrainingNloc\n",
        "  OuterBatchDimension = Num_Seq\n",
        "  d_sample = Tseq * TrainingNloc\n",
        "  max_d_sample = d_sample\n",
        "  UsedValidationNloc = ValidationNloc\n",
        "\n",
        "  if SymbolicWindows:\n",
        "    X_Transformertraining = np.reshape(X_predict, (OuterBatchDimension, Nloc_sample))\n",
        "  else:\n",
        "    X_Transformertraining = np.reshape(X_predict, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
        "  y_Transformertraining = np.reshape(y_predict, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
        "  Spacetime_Transformertraining = np.reshape(Spacetime_predict, (OuterBatchDimension, Nloc_sample))\n",
        "\n",
        "  if SymbolicWindows:\n",
        "    X_Transformerval = np.reshape(X_val, (OuterBatchDimension, UsedValidationNloc))\n",
        "  else:\n",
        "    X_Transformerval = np.reshape(X_val, (OuterBatchDimension, UsedValidationNloc*Tseq, NpropperseqTOT))\n",
        "  y_Transformerval = np.reshape(y_val, (OuterBatchDimension, UsedValidationNloc, NpredperseqTOT))\n",
        "  Spacetime_Transformerval = np.reshape(Spacetime_val, (OuterBatchDimension, UsedValidationNloc))\n",
        "\n",
        "  if UseClassweights:     \n",
        "    sw_Transformertraining = np.empty_like(y_predict, dtype=np.float32)\n",
        "    for i in range(0,sw_Transformertraining.shape[0]):\n",
        "      for j in range(0,sw_Transformertraining.shape[1]):\n",
        "        for k in range(0,NpredperseqTOT):\n",
        "          sw_Transformertraining[i,j,k] = Predictionwgt[k]\n",
        "    sw_Transformerval = np.empty_like(y_val, dtype=np.float32)\n",
        "    for i in range(0,sw_Transformerval.shape[0]):\n",
        "      for jloc in range(0,sw_Transformerval.shape[1]):\n",
        "        for k in range(0,NpredperseqTOT):\n",
        "          sw_Transformerval[i,jloc,k] = Predictionwgt[k]  \n",
        "  else:\n",
        "    sw_Transformertraining = []\n",
        "    sw_Transformerval = []\n",
        "\n",
        "  if SymbolicWindows:\n",
        "    X_Transformertrainingflat2 = np.reshape(X_Transformertraining, (-1, TrainingNloc))\n",
        "    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1))\n",
        "  else:\n",
        "    X_Transformertrainingflat2 = np.reshape(X_Transformertraining, (-1, TrainingNloc,Tseq, NpropperseqTOT))\n",
        "    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1, Tseq, NpropperseqTOT))\n",
        "  y_Transformertrainingflat1 = np.reshape(y_Transformertraining, (-1,NpredperseqTOT) )\n",
        "  Spacetime_Transformertrainingflat1 = np.reshape(Spacetime_Transformertraining,(-1))   \n",
        "  if UseClassweights: \n",
        "    sw_Transformertrainingflat1 = np.reshape(sw_Transformertraining, (-1,NpredperseqTOT) )\n",
        "  if SymbolicWindows:\n",
        "    X_Transformervalflat2 = np.reshape(X_Transformerval, (-1, UsedValidationNloc))\n",
        "    X_Transformervalflat1 = np.reshape(X_Transformervalflat2, (-1))\n",
        "  else:\n",
        "    X_Transformervalflat2 = np.reshape(X_Transformerval, (-1, UsedValidationNloc,Tseq, NpropperseqTOT))\n",
        "    X_Transformervalflat1 = np.reshape(X_Transformervalflat2, (-1, Tseq, NpropperseqTOT))\n",
        "  y_Transformervalflat1 = np.reshape(y_Transformerval, (-1,NpredperseqTOT) )\n",
        "  Spacetime_Transformervalflat1 = np.reshape(Spacetime_Transformerval,(-1))\n",
        "  if UseClassweights: \n",
        "    sw_Transformervalflat1 = np.reshape(sw_Transformerval, (-1,NpredperseqTOT) )\n",
        "\n",
        "  meanvalue2 = 0.0\n",
        "  meanvalue3 = 0.0\n",
        "  meanvalue4 = 0.0\n",
        "  variance2= 0.0\n",
        "  variance3= 0.0\n",
        "  variance4= 0.0\n",
        "\n",
        "# START LOOP OVER SAMPLES\n",
        "  samplebar = notebook.trange(SampleSize,  desc='Full Samples', unit  = 'sample')\n",
        "  epochsize = 2*OuterBatchDimension\n",
        "  if IncreaseNloc_sample > 1:\n",
        "    epochsize = int(epochsize/IncreaseNloc_sample)\n",
        "  elif DecreaseNloc_sample > 1:\n",
        "    epochsize = int(epochsize*DecreaseNloc_sample)\n",
        "  bbar = notebook.trange(epochsize,  desc='Batch    loop', unit  = 'sample')\n",
        "  for shuffling in range (0,SampleSize):\n",
        "    if GarbageCollect:\n",
        "      gc.collect()\n",
        "\n",
        "# TRAINING SET\n",
        "    if TimeShufflingOnly:\n",
        "      X_train, y_train, sw_train, Spacetime_train = shuffleDLinput(X_Transformertraining, \n",
        "            y_Transformertraining, sw_Transformertraining, Spacetime = Spacetime_Transformertraining)\n",
        "    else:\n",
        "      X_train, y_train, sw_train, Spacetime_train = shuffleDLinput(X_Transformertrainingflat1, \n",
        "            y_Transformertrainingflat1, sw_Transformertrainingflat1, Spacetime = Spacetime_Transformertrainingflat1)  \n",
        "\n",
        "    Nloc_sample = TrainingNloc\n",
        "    OuterBatchDimension = Num_Seq       \n",
        "    Totaltodo = Nloc_sample*OuterBatchDimension\n",
        "    if IncreaseNloc_sample > 1:\n",
        "      Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
        "    elif DecreaseNloc_sample > 1:\n",
        "      Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
        "    OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
        "    if OuterBatchDimension * Nloc_sample != Totaltodo:\n",
        "      printexit('Inconsistent Nloc_sample ' + str(Nloc_sample))\n",
        "    d_sample = Tseq * Nloc_sample\n",
        "    max_d_sample = d_sample\n",
        "\n",
        "    if SymbolicWindows:\n",
        "      X_train = np.reshape(X_train, (OuterBatchDimension, Nloc_sample))\n",
        "    else:\n",
        "      X_train = np.reshape(X_train, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
        "    y_train = np.reshape(y_train, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
        "    sw_train = np.reshape(sw_train, (OuterBatchDimension, Nloc_sample, NpredperseqTOT)) \n",
        "    Spacetime_train = np.reshape(Spacetime_train, (OuterBatchDimension, Nloc_sample))\n",
        "\n",
        "    quan3 = 0.0\n",
        "    quan4 = 0.0\n",
        "    losspercallVl = 0.0\n",
        "    losspercallTr = 0.0\n",
        "    TotalTr = 0.0\n",
        "    TotalVl = 0.0\n",
        "    for Trainingindex in range(0, OuterBatchDimension):\n",
        "      if GarbageCollect:\n",
        "        gc.collect()\n",
        "      X_trainlocal = X_train[Trainingindex] \n",
        "      if SymbolicWindows:\n",
        "        X_trainlocal = np.reshape(X_trainlocal,[1,X_trainlocal.shape[0]])\n",
        "      else:\n",
        "        X_trainlocal = np.reshape(X_trainlocal,[1,X_trainlocal.shape[0],X_trainlocal.shape[1]])\n",
        "      \n",
        "      Numinbatch = X_trainlocal.shape[0]\n",
        "      NuminAttention = X_trainlocal.shape[1]\n",
        "      NumTOTAL = Numinbatch*NuminAttention \n",
        "      # SymbolicWindows X_train is indexed by Batch index, Location List for Attention. Missing 1(replace by Window), 1 (replace by properties)\n",
        "      if SymbolicWindows:         \n",
        "        X_trainlocal = np.reshape(X_trainlocal,NumTOTAL)\n",
        "        iseqarray = np.right_shift(X_trainlocal,16)\n",
        "        ilocarray = np.bitwise_and(X_trainlocal, 0b1111111111111111)\n",
        "        X_train_withSeq = list()\n",
        "        for iloc in range(0,NumTOTAL):\n",
        "          X_train_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
        "        X_train_withSeq = np.array(X_train_withSeq)\n",
        "        X_train_withSeq = np.reshape(X_train_withSeq,(Numinbatch, d_sample, NpropperseqTOT))\n",
        "        Time = None\n",
        "        if modelflag==1: \n",
        "          Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
        "        PredictedVector = DLmodel(X_train_withSeq, training = PredictionTraining, Time=Time )\n",
        "      else:\n",
        "        Spacetime_trainlocal = Spacetime_train[Trainingindex]\n",
        "        iseqarray = np.right_shift(Spacetime_trainlocal,16)\n",
        "        ilocarray = np.bitwise_and(Spacetime_trainlocal, 0b1111111111111111)\n",
        "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
        "        PredictedVector = DLmodel(X_trainlocal, training = PredictionTraining, Time=Time )\n",
        "      PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
        "\n",
        "      TrueVector = y_train[Trainingindex]\n",
        "      TrueVector = np.reshape(TrueVector,(1,Nloc_sample,NpredperseqTOT))\n",
        "      sw_trainlocal = sw_train[Trainingindex] \n",
        "      sw_trainlocal = np.reshape(sw_trainlocal,[1,sw_trainlocal.shape[0],sw_trainlocal.shape[1]])\n",
        "      losspercallTr = numpycustom_lossGCF1(TrueVector,PredictedVector,sw_trainlocal)\n",
        "      quan3 += losspercallTr\n",
        "      \n",
        "      for iloc_sample in range(0,Nloc_sample):\n",
        "        LocLocal = ilocarray[iloc_sample]\n",
        "        SeqLocal = iseqarray[iloc_sample]\n",
        "        yyhat = PredictedVector[0,iloc_sample]\n",
        "        if (FitRanges_PartialAtt [SeqLocal,LocLocal,0,0] < 0.1):\n",
        "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = yyhat\n",
        "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = yyhat\n",
        "        else:\n",
        "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = np.maximum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,3],yyhat)\n",
        "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = np.minimum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,4],yyhat)\n",
        "        FitRanges_PartialAtt [SeqLocal,LocLocal,:,0] += FRanges\n",
        "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,1] += yyhat\n",
        "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,2] += np.square(yyhat)\n",
        "\n",
        "      fudge = 1.0/(1+Trainingindex)\n",
        "      TotalTr = quan3 *fudge\n",
        "      bbar.set_postfix(TotalTr = TotalTr, Tr = losspercallTr)\n",
        "      bbar.update(Transformerbatch_size)\n",
        "# END Training Batch Loop\n",
        "    TotalTr= quan3/OuterBatchDimension\n",
        "\n",
        "# VALIDATION SET\n",
        "    Nloc_sample = UsedValidationNloc\n",
        "    OuterBatchDimension = Num_Seq  \n",
        "    Totaltodo = Nloc_sample*OuterBatchDimension\n",
        "    if IncreaseNloc_sample > 1:\n",
        "      Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
        "    elif DecreaseNloc_sample > 1:\n",
        "      Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
        "    OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
        "    if OuterBatchDimension * Nloc_sample != Totaltodo:\n",
        "      printexit('Inconsistent Nloc_sample ' + str(Nloc_sample))\n",
        "    d_sample = Tseq * Nloc_sample\n",
        "    max_d_sample = d_sample        \n",
        "\n",
        "    if TimeShufflingOnly:\n",
        "      X_val, y_val, sw_val, Spacetime_val = shuffleDLinput(\n",
        "          X_Transformerval, y_Transformerval, sw_Transformerval, Spacetime_Transformerval)\n",
        "    else:\n",
        "      X_val, y_val, sw_val, Spacetime_val = shuffleDLinput(\n",
        "          X_Transformervalflat1, y_Transformervalflat1, sw_Transformervalflat1, Spacetime_Transformervalflat1)\n",
        "      if SymbolicWindows:\n",
        "        X_val = np.reshape(X_val, (OuterBatchDimension, Nloc_sample))\n",
        "      else:\n",
        "        X_val = np.reshape(X_val, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
        "    y_val = np.reshape(y_val, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
        "    sw_val = np.reshape(sw_val, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
        "    Spacetime_val = np.reshape(Spacetime_val, (OuterBatchDimension, Nloc_sample))\n",
        "\n",
        "# START VALIDATION Batch Loop\n",
        "    for Validationindex in range(0,OuterBatchDimension):\n",
        "      X_valbatch = X_val[Validationindex]\n",
        "      y_valbatch = y_val[Validationindex]\n",
        "      sw_valbatch = sw_val[Validationindex]\n",
        "      Spacetime_valbatch = Spacetime_val[Validationindex]\n",
        "      if SymbolicWindows:\n",
        "        X_valbatch = np.reshape(X_valbatch,[1,X_valbatch.shape[0]])\n",
        "      else:\n",
        "        X_valbatch = np.reshape(X_valbatch,[1,X_valbatch.shape[0],X_valbatch.shape[1]])\n",
        "      y_valbatch = np.reshape(y_valbatch,[1,y_valbatch.shape[0],y_valbatch.shape[1]])\n",
        "      sw_valbatch = np.reshape(sw_valbatch,[1,sw_valbatch.shape[0],sw_valbatch.shape[1]])\n",
        "      Numinbatch = X_valbatch.shape[0]\n",
        "      NuminAttention = X_valbatch.shape[1]\n",
        "      NumTOTAL = Numinbatch*NuminAttention\n",
        "\n",
        "      if SymbolicWindows:          \n",
        "        X_valbatch = np.reshape(X_valbatch,NumTOTAL)\n",
        "        iseqarray = np.right_shift(X_valbatch,16)\n",
        "        ilocarray = np.bitwise_and(X_valbatch, 0b1111111111111111)\n",
        "        X_valbatch_withSeq = list()\n",
        "        for iloc in range(0,NumTOTAL):\n",
        "          X_valbatch_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
        "        X_valbatch_withSeq = np.array(X_valbatch_withSeq)\n",
        "        X_valbatch_withSeq = np.reshape(X_valbatch_withSeq,(Numinbatch, d_sample, NpropperseqTOT))\n",
        "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
        "        PredictedVector = DLmodel(X_valbatch_withSeq, training = PredictionTraining, Time=Time )\n",
        "      else:\n",
        "        Spacetime_valbatch = np.reshape(Spacetime_valbatch,-1)\n",
        "        iseqarray = np.right_shift(Spacetime_valbatch,16)\n",
        "        ilocarray = np.bitwise_and(Spacetime_valbatch, 0b1111111111111111)\n",
        "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
        "        PredictedVector = DLmodel(X_valbatch, training = PredictionTraining, Time=Time )        \n",
        "      PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
        "\n",
        "      TrueVector = np.reshape(y_valbatch,(1,Nloc_sample,NpredperseqTOT))\n",
        "      sw_valbatch = np.reshape(sw_valbatch,(1,Nloc_sample,NpredperseqTOT))\n",
        "\n",
        "      losspercallVl = numpycustom_lossGCF1(TrueVector,PredictedVector,sw_valbatch)\n",
        "      quan4 += losspercallVl\n",
        "      \n",
        "      for iloc_sample in range(0,Nloc_sample):\n",
        "        LocLocal = ilocarray[iloc_sample]\n",
        "        SeqLocal = iseqarray[iloc_sample]\n",
        "        yyhat = PredictedVector[0,iloc_sample]\n",
        "        if (FitRanges_PartialAtt [SeqLocal,LocLocal,0,0] < 0.1):\n",
        "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = yyhat\n",
        "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = yyhat\n",
        "        else:\n",
        "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = np.maximum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,3],yyhat)\n",
        "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = np.minimum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,4],yyhat)\n",
        "        FitRanges_PartialAtt [SeqLocal,LocLocal,:,0] += FRanges\n",
        "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,1] += yyhat\n",
        "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,2] += np.square(yyhat)\n",
        "      TotalVl = quan4/(1+Validationindex)\n",
        "      losspercall = (TotalTr*TrainingNloc+TotalVl*ValidationNloc)/Nloc\n",
        "      bbar.update(Transformerbatch_size)\n",
        "      bbar.set_postfix(Loss = losspercall, TotalTr = TotalTr, TotalVl= TotalVl, Vl = losspercallVl)\n",
        "# END VALIDATION BATCH LOOP\n",
        "\n",
        "# Processing at the end of Sampling Loop\n",
        "    fudge = 1.0/OuterBatchDimension\n",
        "    quan2 = (quan3*TrainingNloc + quan4*ValidationNloc)/Nloc\n",
        "    quan2 *= fudge\n",
        "    meanvalue2 += quan2\n",
        "    variance2 += quan2**2\n",
        "    if LocationBasedValidation:\n",
        "      quan3 *= fudge\n",
        "      quan4 *= fudge\n",
        "      meanvalue3 += quan3\n",
        "      meanvalue4 += quan4 \n",
        "      variance3 += quan3**2\n",
        "      variance4 += quan4**2       \n",
        "    samplebar.update(1)\n",
        "    if LocationBasedValidation:\n",
        "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Tr = quan3, Val = quan4)\n",
        "    else:\n",
        "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2)\n",
        "    bbar.reset()\n",
        "# End Shuffling loop\n",
        "\n",
        "  printloss(' Full Loss ',meanvalue2,variance2,SampleSize)\n",
        "  printloss(' Training Loss ',meanvalue3,variance3,SampleSize)\n",
        "  printloss(' Validation Loss ',meanvalue4,variance4,SampleSize)\n",
        "  global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
        "  GlobalLoss = meanvalue2\n",
        "  GlobalTrainingLoss = meanvalue3\n",
        "  GlobalValidationLoss = meanvalue4\n",
        "\n",
        "  FitRanges_PartialAtt[:,:,:,1] = np.divide(FitRanges_PartialAtt[:,:,:,1],FitRanges_PartialAtt[:,:,:,0])\n",
        "  FitRanges_PartialAtt[:,:,:,2] = np.sqrt(np.maximum(np.divide(FitRanges_PartialAtt[:,:,:,2],FitRanges_PartialAtt[:,:,:,0]) -\n",
        "                                np.square(FitRanges_PartialAtt[:,:,:,1]), 0.0)) \n",
        "  FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
        "  for iseq in range(0,Num_Seq):\n",
        "    for iloc in range(0,Nloc):    \n",
        "      FitPredictions[iseq,iloc,:] = FitRanges_PartialAtt[iseq,iloc,:,1]\n",
        "  DLprediction3(yin, FitPredictions, ' Separate Attention mean values')\n",
        "  FindNNSE(yin, FitPredictions, Label='Separate Attention' )\n",
        "    \n",
        "  print(startbold+startred+ 'END DLPrediction2E ' +current_time + ' ' + RunName + RunComment +resetfonts) \n",
        "  return \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZpfE40PkL6q"
      },
      "source": [
        "def DLprediction2F(Xin, yin, DLmodel, modelflag):\n",
        "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
        "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
        "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
        "  # Label Array is always [Num_Seq][Nloc] [0=Window(first sequence)#, 1=Location]\n",
        "\n",
        "  if SkipDL2F:\n",
        "    return\n",
        "  if GarbageCollect:\n",
        "    gc.collect()\n",
        "  global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
        "\n",
        "  SensitivityAnalyze = np.full((NpropperseqTOT), False, dtype = np.bool)\n",
        "  SensitivityChange = np.zeros ((NpropperseqTOT), dtype = np.float32)\n",
        "  SensitvitybyPrediction = False\n",
        "  if ReadApril2021Covid:\n",
        "    for iprop in range(0,NpropperseqTOT):\n",
        "      SensitivityAnalyze[iprop] = True\n",
        "\n",
        "  something = 0\n",
        "  SensitivityList = []\n",
        "  for iprop in range(0,NpropperseqTOT):\n",
        "    if SensitivityAnalyze[iprop]:\n",
        "      something +=1\n",
        "      SensitivityList.append(iprop)\n",
        "  if something == 0:\n",
        "    return\n",
        "  ScaleProperty = 0.99\n",
        "  SampleSize = 1\n",
        "\n",
        "\n",
        "  SensitivityFitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT, 1 + something], dtype =np.float32)\n",
        "  FRanges = np.full((NpredperseqTOT), 1.0, dtype = np.float32)\n",
        "  current_time = timenow()\n",
        "  print(wraptotext(startbold+startred+ 'DLPrediction2F ' +current_time + ' ' + RunName + RunComment +  resetfonts))\n",
        "\n",
        "  sw = np.empty_like(yin, dtype=np.float32)\n",
        "  for i in range(0,sw.shape[0]):\n",
        "    for j in range(0,sw.shape[1]):\n",
        "      for k in range(0,NpredperseqTOT):\n",
        "        sw[i,j,k] = Predictionwgt[k] \n",
        "  labelarray =np.empty([Num_Seq, Nloc, 2], dtype = np.int32)\n",
        "  for iseq in range(0, Num_Seq):\n",
        "    for iloc in range(0,Nloc):\n",
        "      labelarray[iseq,iloc,0] = iseq\n",
        "      labelarray[iseq,iloc,1] = iloc\n",
        "\n",
        "  Totaltodo = Num_Seq*Nloc\n",
        "  Nloc_sample = Nloc # default\n",
        "\n",
        "  if IncreaseNloc_sample > 1:\n",
        "    Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
        "  elif DecreaseNloc_sample > 1:\n",
        "    Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
        "\n",
        "  if Totaltodo%Nloc_sample != 0:\n",
        "    printexit('Invalid Nloc_sample ' + str(Nloc_sample) + \" \" + str(Totaltodo))\n",
        "  d_sample = Tseq * Nloc_sample        \n",
        "  max_d_sample = d_sample\n",
        "  OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
        "  print(' Predict with ' +str(Nloc_sample) + ' sequences per sample and batch size ' + str(OuterBatchDimension))\n",
        "\n",
        "  print(startbold+startred+ 'Sensitivity using Property ScaleFactor ' + str(round(ScaleProperty,3)) + resetfonts)\n",
        "  for Sensitivities in range(0,1+something):\n",
        "    if Sensitivities == 0:\n",
        "      iprop = -1\n",
        "      print(startbold+startred+ 'Basic Predictions' + resetfonts)\n",
        "      if SymbolicWindows:\n",
        "        ReshapedSequencesTOTmodified = ReshapedSequencesTOT\n",
        "      else:\n",
        "        Xinmodified = Xin\n",
        "    else:\n",
        "      iprop = SensitivityList[Sensitivities-1]\n",
        "      maxminplace = PropertyNameIndex[iprop]\n",
        "      lastline = ''\n",
        "      if iprop < Npropperseq:\n",
        "        lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
        "      print(startbold+startred+ 'Property ' + str(iprop) + ' ' + InputPropertyNames[maxminplace] + resetfonts + lastline)\n",
        "      if SymbolicWindows:\n",
        "        ReshapedSequencesTOTmodified = np.copy(ReshapedSequencesTOT)\n",
        "        ReshapedSequencesTOTmodified[:,:,iprop] = ScaleProperty * ReshapedSequencesTOTmodified[:,:,iprop]\n",
        "      else:\n",
        "        Xinmodified = np.copy(Xin)\n",
        "        Xinmodified[:,:,:,iprop] = ScaleProperty*Xinmodified[:,:,:,iprop]\n",
        "    CountFitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
        "    meanvalue2 = 0.0\n",
        "    meanvalue3 = 0.0\n",
        "    meanvalue4 = 0.0\n",
        "    variance2= 0.0\n",
        "    variance3= 0.0\n",
        "    variance4= 0.0\n",
        "\n",
        "    samplebar = notebook.trange(SampleSize,  desc='Full Samples', unit  = 'sample')\n",
        "    bbar = notebook.trange(OuterBatchDimension,  desc='Batch    loop', unit  = 'sample')\n",
        "    for shuffling in range (0,SampleSize):\n",
        "      if GarbageCollect:\n",
        "        gc.collect()\n",
        "      yuse = yin\n",
        "      labeluse = labelarray\n",
        "      y2= np.reshape(yuse, (-1, NpredperseqTOT)).copy()\n",
        "      labelarray2 = np.reshape(labeluse, (-1,2))\n",
        "\n",
        "      if SymbolicWindows:\n",
        "        # Xin X2 X3 not used rather ReshapedSequencesTOT\n",
        "        labelarray2, y2 = shuffleDLinput(labelarray2, y2)\n",
        "        ReshapedSequencesTOTuse = ReshapedSequencesTOTmodified\n",
        "      else:\n",
        "        Xuse = Xinmodified\n",
        "        X2 = np.reshape(Xuse, (-1, Tseq, NpropperseqTOT)).copy()\n",
        "        X2, y2, labelarray2 = shuffleDLinput(X2, y2,labelarray2)\n",
        "        X3 = np.reshape(X2, (-1, d_sample, NpropperseqTOT))\n",
        "        \n",
        "      y3 = np.reshape(y2, (-1, Nloc_sample, NpredperseqTOT))\n",
        "      sw = np.reshape(sw, (-1, Nloc_sample, NpredperseqTOT))\n",
        "      labelarray3 = np.reshape(labelarray2, (-1, Nloc_sample, 2))\n",
        "\n",
        "      quan2 = 0.0\n",
        "      quan3 = 0.0\n",
        "      quan4 = 0.0\n",
        "      for Batchindex in range(0, OuterBatchDimension):\n",
        "        if GarbageCollect:\n",
        "          gc.collect()\n",
        "\n",
        "        if SymbolicWindows:\n",
        "          X3local = list()\n",
        "          for iloc_sample in range(0,Nloc_sample):\n",
        "            LocLocal = labelarray3[Batchindex, iloc_sample,1]\n",
        "            SeqLocal = labelarray3[Batchindex, iloc_sample,0]\n",
        "            X3local.append(ReshapedSequencesTOTuse[LocLocal,SeqLocal:SeqLocal+Tseq])\n",
        "          InputVector = np.array(X3local)\n",
        "        else:\n",
        "          InputVector = X3[Batchindex]\n",
        "\n",
        "        Labelsused = labelarray3[Batchindex]\n",
        "        Time = None\n",
        "        if modelflag == 0:\n",
        "          InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
        "        else:\n",
        "          Time = SetSpacetime(np.reshape(Labelsused[:,0],(1,-1)))\n",
        "          InputVector = np.reshape(InputVector,(1,Tseq*Nloc_sample,NpropperseqTOT))\n",
        "        PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time )\n",
        "        PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
        "\n",
        "        swbatched = sw[Batchindex,:,:]\n",
        "        if LocationBasedValidation:\n",
        "          swT = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
        "          swV = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
        "          for iloc_sample in range(0,Nloc_sample):\n",
        "            fudgeT = Nloc/TrainingNloc\n",
        "            fudgeV = Nloc/ValidationNloc\n",
        "            iloc = Labelsused[iloc_sample,1]\n",
        "            if MappingtoTraining[iloc] >= 0:\n",
        "              swT[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeT\n",
        "            else:\n",
        "              swV[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeV\n",
        "        TrueVector = y3[Batchindex]\n",
        "        TrueVector = np.reshape(TrueVector,(1,Nloc_sample,NpredperseqTOT))\n",
        "        swbatched = np.reshape(swbatched,(1,Nloc_sample,NpredperseqTOT))\n",
        "\n",
        "        losspercall = numpycustom_lossGCF1(TrueVector,PredictedVector,swbatched)\n",
        "        quan2 += losspercall\n",
        "        bbar.update(1)\n",
        "        if LocationBasedValidation:\n",
        "          losspercallTr = numpycustom_lossGCF1(TrueVector,PredictedVector,swT)\n",
        "          quan3 += losspercallTr\n",
        "          losspercallVl = numpycustom_lossGCF1(TrueVector,PredictedVector,swV)\n",
        "          quan4 += losspercallVl\n",
        "        \n",
        "        for iloc_sample in range(0,Nloc_sample):\n",
        "          LocLocal = Labelsused[iloc_sample,1]\n",
        "          SeqLocal = Labelsused[iloc_sample,0]\n",
        "          yyhat = PredictedVector[0,iloc_sample]\n",
        "          CountFitPredictions [SeqLocal,LocLocal,:] += FRanges\n",
        "          SensitivityFitPredictions [SeqLocal,LocLocal,:,Sensitivities] += yyhat\n",
        "\n",
        "        fudge = 1.0/(1.0 + Batchindex)\n",
        "        mean2 = quan2 * fudge \n",
        "        if LocationBasedValidation:\n",
        "          mean3 = quan3 * fudge\n",
        "          mean4 = quan4 * fudge\n",
        "          bbar.set_postfix(AvLoss = mean2, AvTr = mean3, AvVl = mean4, Loss = losspercall, Tr = losspercallTr, Vl = losspercallVl)\n",
        "        else:\n",
        "          bbar.set_postfix(Loss = losspercall, AvLoss = mean2 ) \n",
        "\n",
        "  # Processing at the end of Sampling Loop\n",
        "      fudge = 1.0/OuterBatchDimension\n",
        "      quan2 *= fudge\n",
        "      quan3 *= fudge\n",
        "      quan4 *= fudge\n",
        "      meanvalue2 += quan2\n",
        "      variance2 += quan2**2\n",
        "      variance3 += quan3**2\n",
        "      variance4 += quan4**2\n",
        "      if LocationBasedValidation:\n",
        "        meanvalue3 += quan3\n",
        "        meanvalue4 += quan4        \n",
        "      samplebar.update(1)\n",
        "      if LocationBasedValidation:\n",
        "        samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Tr = quan3, Val = quan4)\n",
        "      else:\n",
        "        samplebar.set_postfix(Shuffle=shuffling, Loss = quan2)\n",
        "      bbar.reset()\n",
        "  # End Shuffling loop\n",
        "\n",
        "    if Sensitivities == 0:\n",
        "      iprop = -1\n",
        "      lineend = startbold+startred+ 'Basic Predictions' + resetfonts\n",
        "    else:\n",
        "      iprop = SensitivityList[Sensitivities-1]\n",
        "      nameplace = PropertyNameIndex[iprop]\n",
        "      maxminplace = PropertyAverageValuesPointer[iprop]\n",
        "      lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
        "      lineend= startbold+startred + 'Property ' + str(iprop) + ' ' + InputPropertyNames[nameplace] + resetfonts + lastline\n",
        "\n",
        "    meanvalue2 /= SampleSize \n",
        "\n",
        "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
        "    printloss(' Full Loss ',meanvalue2,variance2,SampleSize, lineend = lineend)\n",
        "    meanvalue2 /= SampleSize\n",
        "    GlobalLoss = meanvalue2\n",
        "    GlobalTrainingLoss = 0.0\n",
        "    GlobalValidationLoss = 0.0\n",
        "    \n",
        "    if LocationBasedValidation:\n",
        "      printloss(' Training Loss ',meanvalue3,variance3,SampleSize, lineend = lineend)\n",
        "      printloss(' Validation Loss ',meanvalue4,variance4,SampleSize, lineend = lineend)\n",
        "      meanvalue3 /= SampleSize\n",
        "      meanvalue4 /= SampleSize\n",
        "      GlobalTrainingLoss = meanvalue3\n",
        "      GlobalValidationLoss = meanvalue4\n",
        "\n",
        "# Sequence Location Predictions\n",
        "    SensitivityFitPredictions[:,:,:,Sensitivities] = np.divide(SensitivityFitPredictions[:,:,:,Sensitivities],CountFitPredictions[:,:,:])\n",
        "    if Sensitivities == 0:\n",
        "      Goldstandard = np.sum(np.abs(SensitivityFitPredictions[:,:,:,Sensitivities]), axis =(0,1))\n",
        "      TotalGS = np.sum(Goldstandard)\n",
        "      continue\n",
        "    Change = np.sum(np.abs(np.subtract(SensitivityFitPredictions[:,:,:,Sensitivities],SensitivityFitPredictions[:,:,:,0])), axis =(0,1))\n",
        "    TotalChange = np.sum(Change)\n",
        "    SensitivityChange[iprop] = TotalChange\n",
        "    print(str(round(TotalChange,5)) +  ' GS ' + str(round(TotalGS,5)) + ' ' +lineend)\n",
        "    if SensitvitybyPrediction:\n",
        "      for ipred in range(0,NpredperseqTOT):\n",
        "        print(str(round(Change[ipred],5)) +  ' GS ' + str(round(Goldstandard[ipred],5)) \n",
        "        + ' ' + str(ipred) + ' ' + Predictionname[ipred] + ' wgt ' + str(round(Predictionwgt[ipred],3)))\n",
        "    \n",
        "  print(startbold+startred+ '\\nSummarize Changes Total ' + str(round(TotalGS,5))+ ' Property ScaleFactor ' + str(round(ScaleProperty,3)) + resetfonts )\n",
        "  for Sensitivities in range(1,1+something):\n",
        "    iprop = SensitivityList[Sensitivities-1]\n",
        "    nameplace = PropertyNameIndex[iprop]\n",
        "    maxminplace = PropertyAverageValuesPointer[iprop]\n",
        "    \n",
        " \n",
        "    lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
        "    lastline += ' Normed Std ' +str(round(QuantityStatistics[maxminplace,6],4))\n",
        "    TotalChange = SensitivityChange[iprop] \n",
        "    NormedChange = TotalChange/((1-ScaleProperty)*TotalGS)\n",
        "    stdmeanratio = 0.0\n",
        "    stdchangeratio = 0.0   \n",
        "    if np.abs(QuantityStatistics[maxminplace,5]) > 0.0001:\n",
        "      stdmeanratio = QuantityStatistics[maxminplace,6]/QuantityStatistics[maxminplace,5]\n",
        "    if np.abs(QuantityStatistics[maxminplace,6]) > 0.0001:\n",
        "      stdchangeratio = NormedChange/QuantityStatistics[maxminplace,6]\n",
        "\n",
        "    lratios =  ' Normed Change '+ str(round(NormedChange,5)) + ' /std ' + str(round(stdchangeratio,5))\n",
        "    lratios += ' Std/Mean ' + str(round(stdmeanratio,5))\n",
        "    print(str(iprop) + ' Change '+ str(round(TotalChange,2)) + startbold + lratios\n",
        "          + ' ' + InputPropertyNames[nameplace] + resetfonts + lastline)\n",
        "\n",
        "  current_time = timenow()  \n",
        "  print(startbold+startred+ '\\nEND DLPrediction2F ' + current_time + ' ' + RunName + RunComment +resetfonts) \n",
        "  return \n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATXNwS2IkOeN"
      },
      "source": [
        "CustomLoss = 1         # Can be 0 1 4\n",
        "UseClassweights = True\n",
        "\n",
        "PredictionTraining = False\n",
        "\n",
        "if NpredperseqTOT <=2:\n",
        "  useFutures = False\n",
        "  CustomLoss = 0\n",
        "  UseClassweights = False\n",
        "\n",
        "number_of_LSTMworkers = 1\n",
        "LSTMepochs = 100\n",
        "LSTMbatch_size = TrainingNloc\n",
        "LSTMbatch_size = min(LSTMbatch_size, TrainingNloc)\n",
        "\n",
        "LSTMactivationvalue = \"selu\"\n",
        "LSTMrecurrent_activation = \"sigmoid\"\n",
        "LSTMoptimizer = 'adam'\n",
        "LSTMdropout1=0.2\n",
        "LSTMrecurrent_dropout1 = 0.2\n",
        "LSTMdropout2=0.2\n",
        "LSTMrecurrent_dropout2 = 0.2\n",
        "number_LSTMnodes= 16\n",
        "LSTMFinalMLP = 64\n",
        "LSTMInitialMLP = 32\n",
        "LSTMThirdLayer = False\n",
        "\n",
        "LSTMSkipInitial = False\n",
        "LSTMverbose = 0\n",
        "LSTMvalidationfrac = 0.0\n",
        "UsedLSTMvalidationfrac = LSTMvalidationfrac\n",
        "if LocationBasedValidation:\n",
        "  UsedLSTMvalidationfrac = LocationBasedValidation\n",
        "  LSTMvalidationfrac = UsedLSTMvalidationfrac\n",
        "bestmethod = 2\n",
        "if UsedLSTMvalidationfrac < 0.001:\n",
        "    bestmethod = 1\n",
        " "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WlM3MTvkPJ9"
      },
      "source": [
        "def get_model_summary(model):\n",
        "  stream = io.StringIO()\n",
        "  model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
        "  summary_string = stream.getvalue()\n",
        "  stream.close()\n",
        "  return summary_string\n",
        "\n",
        "def setDLinput(Spacetime = True):\n",
        "  # Initial data is Flatten([Num_Seq][Nloc]) [Tseq] with values [Nprop-Sel + Nforcing + Add(ExPosEnc-Selin)] starting with   RawInputSequencesTOT\n",
        "  # Predictions are Flatten([Num_Seq] [Nloc]) [Predvals=Npred+ExPosEnc-Selout] [Predtimes = Forecast-time range] starting with RawInputPredictionsTOT\n",
        "  # No assumptions as to type of variables here\n",
        "  if SymbolicWindows:\n",
        "    X_predict = SymbolicInputSequencesTOT.reshape(OuterBatchDimension,1,1)\n",
        "  else:\n",
        "    X_predict = RawInputSequencesTOT.reshape(OuterBatchDimension,Tseq,NpropperseqTOT)\n",
        "  y_predict = RawInputPredictionsTOT.reshape(OuterBatchDimension,NpredperseqTOT)\n",
        "  if Spacetime:\n",
        "     SpacetimeforMask_predict =  SpacetimeforMask.reshape(OuterBatchDimension,1,1).copy()\n",
        "     return X_predict, y_predict, SpacetimeforMask_predict\n",
        "  return X_predict, y_predict\n",
        "\n",
        "def setSeparateDLinput(model, Spacetime = False):\n",
        "  # Initial data is Flatten([Num_Seq][Nloc]) [Tseq] with values [Nprop-Sel + Nforcing + Add(ExPosEnc-Selin)] starting with   RawInputSequencesTOT\n",
        "  # Predictions are Flatten([Num_Seq] [Nloc]) [Predvals=Npred+ExPosEnc-Selout] [Predtimes = Forecast-time range] starting with RawInputPredictionsTOT\n",
        "  # No assumptions as to type of variables here\n",
        "  # model = 0 LSTM =1 transformer\n",
        "  if model == 0: \n",
        "    Spacetime = False\n",
        "  X_val = None\n",
        "  y_val = None\n",
        "  Spacetime_val = None\n",
        "  Spacetime_train = None\n",
        "  if SymbolicWindows:\n",
        "    InputSequences = np.empty([Num_Seq, TrainingNloc], dtype = np.int32)\n",
        "    for iloc in range(0,TrainingNloc):   \n",
        "      InputSequences[:,iloc] = SymbolicInputSequencesTOT[:,ListofTrainingLocs[iloc]]\n",
        "    if model == 0:\n",
        "      X_train = InputSequences.reshape(Num_Seq*TrainingNloc,1,1)\n",
        "    else:\n",
        "      X_train = InputSequences\n",
        "    if Spacetime:\n",
        "      Spacetime_train = X_train.copy()\n",
        "\n",
        "    if LocationValidationFraction > 0.001:\n",
        "      UsedValidationNloc = ValidationNloc\n",
        "      if FullSetValidation:\n",
        "        UsedValidationNloc = Nloc\n",
        "      ValInputSequences = np.empty([Num_Seq, UsedValidationNloc], dtype = np.int32)\n",
        "      if FullSetValidation:\n",
        "        for iloc in range(0,Nloc):\n",
        "          ValInputSequences[:,iloc] = SymbolicInputSequencesTOT[:,iloc]\n",
        "      else:\n",
        "        for iloc in range(0,ValidationNloc):\n",
        "          ValInputSequences[:,iloc] = SymbolicInputSequencesTOT[:,ListofValidationLocs[iloc]]\n",
        "      if model == 0:\n",
        "        X_val = ValInputSequences.reshape(Num_Seq * UsedValidationNloc,1,1)\n",
        "      else:\n",
        "        X_val = ValInputSequences\n",
        "      if Spacetime:\n",
        "        Spacetime_val = X_val.copy()\n",
        "\n",
        "  else: # Symbolic Windows false Calculate Training\n",
        "    InputSequences = np.empty([Num_Seq, TrainingNloc,Tseq,NpropperseqTOT], dtype = np.float32)\n",
        "    for iloc in range(0,TrainingNloc): \n",
        "      InputSequences[:,iloc,:,:] = RawInputSequencesTOT[:,ListofTrainingLocs[iloc],:,:]\n",
        "    if model == 0:\n",
        "      X_train = InputSequences.reshape(Num_Seq*TrainingNloc,Tseq,NpropperseqTOT)\n",
        "    else:\n",
        "      X_train = InputSequences\n",
        "    if Spacetime:\n",
        "      Spacetime_train = np.empty([Num_Seq, TrainingNloc], dtype = np.int32)\n",
        "      for iloc in range(0,TrainingNloc):   \n",
        "        Spacetime_train[:,iloc] = SpacetimeforMask[:,ListofTrainingLocs[iloc]]\n",
        "\n",
        "    if LocationValidationFraction > 0.001: # Symbolic Windows false Calculate Validation\n",
        "      UsedValidationNloc = ValidationNloc\n",
        "      if FullSetValidation:\n",
        "        UsedValidationNloc = Nloc\n",
        "      ValInputSequences = np.empty([Num_Seq, UsedValidationNloc,Tseq,NpropperseqTOT], dtype = np.float32)\n",
        "      if FullSetValidation:\n",
        "        for iloc in range(0,Nloc):\n",
        "          ValInputSequences[:,iloc,:,:] = RawInputSequencesTOT[:,iloc,:,:]\n",
        "      else:\n",
        "        for iloc in range(0,ValidationNloc):\n",
        "          ValInputSequences[:,iloc,:,:] = RawInputSequencesTOT[:,ListofValidationLocs[iloc],:,:]\n",
        "      if model == 0:\n",
        "        X_val = ValInputSequences.reshape(Num_Seq * UsedValidationNloc,Tseq,NpropperseqTOT)\n",
        "      else:\n",
        "        X_val = ValInputSequences\n",
        "      if Spacetime:\n",
        "        Spacetime_val = np.empty([Num_Seq, UsedValidationNloc], dtype = np.int32)\n",
        "        if FullSetValidation:\n",
        "          for iloc in range(0,Nloc):\n",
        "            Spacetime_val[:,iloc] = SpacetimeforMask[:,iloc]\n",
        "        else:\n",
        "          for iloc in range(0,ValidationNloc):   \n",
        "            Spacetime_val[:,iloc] = SpacetimeforMask[:,ListofValidationLocs[iloc]]\n",
        "  \n",
        "  # Calculate training predictions \n",
        "  InputPredictions = np.empty([Num_Seq, TrainingNloc,NpredperseqTOT], dtype = np.float32)\n",
        "  for iloc in range(0,TrainingNloc):\n",
        "    InputPredictions[:,iloc,:] = RawInputPredictionsTOT[:,ListofTrainingLocs[iloc],:]\n",
        "  if model == 0:\n",
        "    y_train = InputPredictions.reshape(OuterBatchDimension,NpredperseqTOT)\n",
        "  else:\n",
        "    y_train = InputPredictions\n",
        "\n",
        "  # Calculate validation predictions \n",
        "  if LocationValidationFraction > 0.001:\n",
        "    ValInputPredictions = np.empty([Num_Seq, UsedValidationNloc,NpredperseqTOT], dtype = np.float32)\n",
        "    if FullSetValidation:\n",
        "      for iloc in range(0,Nloc):\n",
        "        ValInputPredictions[:,iloc,:] = RawInputPredictionsTOT[:,iloc,:]\n",
        "    else:\n",
        "      for iloc in range(0,ValidationNloc):\n",
        "        ValInputPredictions[:,iloc,:] = RawInputPredictionsTOT[:,ListofValidationLocs[iloc],:]\n",
        "    if model == 0:\n",
        "      y_val = ValInputPredictions.reshape(Num_Seq * ValidationNloc,NpredperseqTOT)\n",
        "    else:\n",
        "      y_val = ValInputPredictions\n",
        "\n",
        "  if Spacetime:\n",
        "    return X_train, y_train, Spacetime_train, X_val, y_val, Spacetime_val\n",
        "  else:    \n",
        "    return X_train, y_train,X_val,y_val\n",
        "\n",
        "def InitializeDLforTimeSeries(message,processindex,y_predict):\n",
        "  if( processindex == 0 ):\n",
        "      current_time = timenow()\n",
        "      line = (startbold + current_time + ' ' + message + resetfonts + \" Window Size \" + str(Tseq) + \n",
        "            \" Number of samples over time that sequence starts at and location:\" +str(OuterBatchDimension) + \n",
        "            \" Number input features per sequence:\" + str(NpropperseqTOT) +  \n",
        "            \" Number of predicted outputs per sequence:\" + str(NpredperseqTOT) + \n",
        "            \" Batch_size:\" + str(LSTMbatch_size) + \n",
        "            \" n_nodes:\" + str(number_LSTMnodes) + \n",
        "            \" epochs:\" + str(LSTMepochs))\n",
        "      print(wraptotext(line))\n",
        "      checkNaN(y_predict)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_awrZA8kQoh"
      },
      "source": [
        "class MyLSTMmodel(tf.keras.Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyLSTMmodel, self).__init__(**kwargs)\n",
        "    self.fullLSTM = MyLSTMlayer()\n",
        "\n",
        "  def call(self, inputs):  \n",
        "    outputs = self.fullLSTM(inputs)\n",
        "    return outputs\n",
        "\n",
        "  def build_graph(self, shapes):\n",
        "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
        "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])\n",
        "    \n",
        "class MyLSTMlayer(tf.keras.Model):\n",
        "# Class for a simple multiple layer LSTM with FCN at start and end\n",
        "# All parameters defined externally\n",
        "# structured so MyLSTMlayer can be used standalone or in part of a transformer\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyLSTMlayer, self).__init__(**kwargs)\n",
        "    if (LSTMInitialMLP > 0) and (not LSTMSkipInitial):\n",
        "      self.dense_1 = tf.keras.layers.Dense(LSTMInitialMLP, activation=LSTMactivationvalue)\n",
        "    self.LSTM_1 =tf.keras.layers.LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
        "                  activation= LSTMactivationvalue , return_sequences=True, recurrent_activation= LSTMrecurrent_activation)\n",
        "    self.LSTM_2 =tf.keras.layers.LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
        "        activation= LSTMactivationvalue , return_sequences=LSTMThirdLayer, recurrent_activation= LSTMrecurrent_activation)\n",
        "    if(LSTMThirdLayer):\n",
        "      self.LSTM_3 =tf.keras.layers.LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
        "                    activation= LSTMactivationvalue , return_sequences=False, recurrent_activation= LSTMrecurrent_activation)\n",
        "    self.dense_2 = tf.keras.layers.Dense(LSTMFinalMLP, activation=LSTMactivationvalue)\n",
        "    self.dense_f = tf.keras.layers.Dense(NpredperseqTOT)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if (LSTMInitialMLP > 0) and (not LSTMSkipInitial):\n",
        "      Runningdata = self.dense_1(inputs)\n",
        "      Runningdata = self.LSTM_1(Runningdata, training=training)\n",
        "    else:\n",
        "      Runningdata = self.LSTM_1(inputs, training=training)\n",
        "    Runningdata = self.LSTM_2(Runningdata, training=training)\n",
        "    if(LSTMThirdLayer):\n",
        "      Runningdata = self.LSTM_3(Runningdata, training=training)\n",
        "    if(LSTMFinalMLP > 0):\n",
        "      Runningdata = self.dense_2(Runningdata)\n",
        "    Outputdata = self.dense_f(Runningdata)\n",
        "    return Outputdata\n",
        "\n",
        "  def build_graph(self, shapes):\n",
        "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
        "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3lzZhBQkTtA"
      },
      "source": [
        "def RunLSTMClassVersion():\n",
        "  # Run the LSTM model defined by Model and Layer class\n",
        "\n",
        "\n",
        "  X_predict, y_predict = setDLinput(Spacetime = False)\n",
        "  InitializeDLforTimeSeries('Class Version ',processindex,y_predict)\n",
        "\n",
        "  X_train, y_train = shuffleDLinput(X_predict, y_predict)\n",
        " \n",
        "  myLSTMmodel = MyLSTMmodel(name ='myLSTMmodel')\n",
        "  if CustomLoss == 0:\n",
        "      myLSTMmodel.compile(loss='mse', optimizer= LSTMoptimizer)\n",
        "  if CustomLoss == 1:\n",
        "      myLSTMmodel.compile(loss= custom_lossGCF1, optimizer= LSTMoptimizer)\n",
        "  if CustomLoss == 4:    \n",
        "      myLSTMmodel.compile(loss= custom_lossGCF4, optimizer= LSTMoptimizer)\n",
        "\n",
        "  the_callbacks = [TqdmCallback(),]\n",
        "\n",
        "  if UseClassweights:   \n",
        "      cw = {}\n",
        "      for i in range(0,NpredperseqTOT):\n",
        "        cw[i] = Predictionwgt[i]     \n",
        "      sw = np.empty_like(y_train, dtype=np.float32)\n",
        "      for j in range(0,sw.shape[0]):\n",
        "        for i in range(0,NpredperseqTOT):\n",
        "          sw[j,i] = Predictionwgt[i]  \n",
        "      modelresult = myLSTMmodel.fit(X_train, y_train,\n",
        "            sample_weight = sw,\n",
        "            epochs=LSTMepochs,\n",
        "            batch_size=LSTMbatch_size,\n",
        "#           class_weight = cw,\n",
        "            verbose=LSTMverbose,\n",
        "            validation_split=UsedLSTMvalidationfrac,\n",
        "            callbacks=the_callbacks\n",
        "            )\n",
        "  else:\n",
        "      modelresult = myLSTMmodel.fit(X_train, y_train,\n",
        "            epochs=LSTMepochs,\n",
        "            batch_size=LSTMbatch_size,\n",
        "            verbose=LSTMverbose,\n",
        "            validation_split=UsedLSTMvalidationfrac,\n",
        "            callbacks=the_callbacks\n",
        "            ) \n",
        "  myLSTMmodel.summary()\n",
        "\n",
        "  recordtrainloss = modelresult.history['loss']\n",
        "  recordvalloss = modelresult.history['val_loss']\n",
        "  finalizeDL(myLSTMmodel,recordtrainloss, recordvalloss,UsedLSTMvalidationfrac,RawInputSequencesTOT, RawInputPredictionsTOT,0)\n",
        "  return\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfBTw2zgkVDg"
      },
      "source": [
        "def weightedcustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
        "    tupl = np.shape(y_actual)\n",
        "\n",
        "    flagGCF = tf.math.is_nan(y_actual)\n",
        "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
        "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
        "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
        "    if len(tupl) >= 2:\n",
        "      tensordiff /= tupl[0]\n",
        "    if len(tupl) >= 3:\n",
        "      tensordiff /= tupl[1]\n",
        "    if len(tupl) >= 4:\n",
        "      tensordiff /= tupl[2]\n",
        "    return tensordiff\n",
        "\n",
        "def numpycustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
        "    tupl = np.shape(y_actual)\n",
        "\n",
        "    flagGCF = np.isnan(y_actual)\n",
        "    y_actual = y_actual[np.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[np.logical_not(flagGCF)]\n",
        "    sw = sample_weight[np.logical_not(flagGCF)]\n",
        "    tensordiff = np.sum(np.multiply(np.square(y_actual-y_pred),sw))\n",
        "    if len(tupl) >= 2:\n",
        "      tensordiff /= tupl[0]\n",
        "    if len(tupl) >= 3:\n",
        "      tensordiff /= tupl[1]\n",
        "    if len(tupl) >= 4:\n",
        "      tensordiff /= tupl[2]\n",
        "    return tensordiff\n",
        "\n",
        "def weightedcustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
        "    tupl = np.shape(y_actual)\n",
        "\n",
        "    flagGCF = tf.math.is_nan(y_actual)\n",
        "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
        "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
        "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
        "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
        "    if len(tupl) >= 2:\n",
        "      tensordiff /= tupl[0]\n",
        "    if len(tupl) >= 3:\n",
        "      tensordiff /= tupl[1]\n",
        "    if len(tupl) >= 4:\n",
        "      tensordiff /= tupl[2]\n",
        "    return tensordiff\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA0O5psMkX2A"
      },
      "source": [
        "class TFTrainingMonitor():\n",
        "  def __init__(self):\n",
        "\n",
        "    # These OPERATIONAL variables control saving of best fits\n",
        "    self.lastsavedepoch = -1 # Epoch number where last saved fit done\n",
        "    self.BestLossValueSaved = NaN # Training Loss value of last saved fit\n",
        "    self.BestValLossValueSaved = NaN # Validation Loss value of last saved fit\n",
        "    self.Numsuccess = 0 # count little successes up to SuccessLimit\n",
        "    self.Numfailed = 0\n",
        "    self.LastLossValue = NaN # Loss on previous epoch\n",
        "    self.MinLossValue = NaN # Saved minimum loss value\n",
        "    self.LastValLossValue = NaN # Validation Loss on previous epoch\n",
        "    self.MinValLossValue = NaN # validation loss value at last save\n",
        "    self.BestLossSaved = False # Boolean to indicate that best Loss value saved\n",
        "    self.saveMinLosspath = '' # Checkpoint path for saved network \n",
        "    self.epochcount = 0\n",
        "    self.NumberTimesSaved = 0 # Number of Checkpointing steps for Best Loss\n",
        "    self.NumberTimesRestored = 0 # Number of Checkpointing Restores\n",
        "    self.LittleJumpdifference = NaN\n",
        "    self.LittleValJumpdifference = NaN\n",
        "    self.AccumulateSuccesses = 0\n",
        "    self.AccumulateFailures = np.zeros(5, dtype=np.int)\n",
        "    self.RestoreReasons = np.zeros(8, dtype = np.int)\n",
        "    self.NameofFailures = ['Success','Train Only Failed','Val Only Failed','Both Failed', 'NaN']\n",
        "    self.NameofRestoreReasons = ['Both Big Jump', 'Both Little Jump','Train Big Jump', 'Train Little Jump','Val Big Jump','Val Little Jump',' Failure Limit', ' NaN']\n",
        "# End OPERATIONAL Control set up for best fit checkpointing\n",
        "\n",
        "# These are parameters use can set\n",
        "    self.UseBestAvailableLoss = True\n",
        "    self.LittleJump = 2.0 # Multiplier for checking jump compared to recent changes\n",
        "    self.ValLittleJump = 2.0 # Multiplier for checking jump compared to recent changes\n",
        "    self.startepochs = -1 # Ignore this number of epochs to let system get started\n",
        "    self.SuccessLimit = 20 # Don't keep saving. Wait for this number of (little) successes\n",
        "    self.FailureLimit = 10 # Number of failures before restore\n",
        "    self.BadJumpfraction = 0.2 # This fractional jump will trigger attempt to go back to saved value\n",
        "    self.ValBadJumpfraction = 0.2 # This fractional jump will trigger attempt to go back to saved value\n",
        "    self.ValidationFraction = 0.0 # Must be used validation fraction\n",
        "    DownplayValidationIncrease = True\n",
        "\n",
        "# End parameters user can set\n",
        "\n",
        "    self.checkpoint = None\n",
        "    self.CHECKPOINTDIR = ''\n",
        "    self.RunName = ''\n",
        "\n",
        "    self.train_epoch = 0.0\n",
        "    self.val_epoch = 0.0\n",
        "    tfepochstep = None\n",
        "    recordtrainloss =[]\n",
        "    recordvalloss = []\n",
        "\n",
        "  def SetControlParms(self, UseBestAvailableLoss = None, LittleJump = None, startepochs = None, ValLittleJump = None,\n",
        "       ValBadJumpfraction = None, SuccessLimit = None, FailureLimit = None, BadJumpfraction = None, DownplayValidationIncrease=True):\n",
        "    if UseBestAvailableLoss is not None:\n",
        "      self.UseBestAvailableLoss = UseBestAvailableLoss\n",
        "    if LittleJump is not None:\n",
        "      self.LittleJump = LittleJump\n",
        "    if ValLittleJump is not None:\n",
        "      self.ValLittleJump = ValLittleJump\n",
        "    if startepochs is not None:\n",
        "      self.startepochs = startepochs\n",
        "    if SuccessLimit is not None:\n",
        "      self.SuccessLimit = SuccessLimit\n",
        "    if FailureLimit is not None:\n",
        "      self.FailureLimit = FailureLimit\n",
        "    if BadJumpfraction is not None:\n",
        "      self.BadJumpfraction = BadJumpfraction\n",
        "    if ValBadJumpfraction is not None:\n",
        "      self.ValBadJumpfraction = ValBadJumpfraction\n",
        "    if DownplayValidationIncrease:\n",
        "      self.ValBadJumpfraction = 200.0\n",
        "      self.ValLittleJump = 2000.0 \n",
        "    elif ValLittleJump is None:\n",
        "      self.ValLittleJump = 2.0\n",
        "    elif ValBadJumpfraction is None:\n",
        "      self.ValBadJumpfraction = 0.2\n",
        "      \n",
        "  def SetCheckpointParms(self,checkpointObject,CHECKPOINTDIR,RunName = '',Restoredcheckpoint= False, Restored_path = '',  \n",
        "                         ValidationFraction = 0.0, SavedTrainLoss = NaN, SavedValLoss = NaN):\n",
        "    self.ValidationFraction = ValidationFraction\n",
        "    self.checkpoint = checkpointObject\n",
        "    self.CHECKPOINTDIR = CHECKPOINTDIR\n",
        "    self.RunName = RunName\n",
        "    if Restoredcheckpoint:\n",
        "      self.BestLossSaved = True\n",
        "      self.saveMinLosspath = Restored_path # Checkpoint path for saved network \n",
        "      self.LastLossValue = SavedTrainLoss\n",
        "      self.LastValLossValue = SavedValLoss\n",
        "      self.BestLossValueSaved = SavedTrainLoss\n",
        "      self.BestValLossValueSaved = SavedValLoss\n",
        "      self.lastsavedepoch =  self.epochcount\n",
        "      self.MinLossValue = SavedTrainLoss\n",
        "      self.MinValLossValue = SavedValLoss\n",
        "\n",
        "  def EpochEvaluate(self, epochcount,train_epoch, val_epoch, tfepochstep, recordtrainloss, recordvalloss):\n",
        "    FalseReturn = 0\n",
        "    TrueReturn = 1\n",
        "    self.epochcount = epochcount\n",
        "    self.train_epoch = train_epoch\n",
        "    self.val_epoch = val_epoch\n",
        "    self.tfepochstep = tfepochstep\n",
        "    self.recordtrainloss = recordtrainloss\n",
        "    self.recordvalloss = recordvalloss\n",
        "\n",
        "    Needtorestore = False \n",
        "    Failreason = 5 # nonsense\n",
        "    LossChange = 0.0\n",
        "    ValLossChange = 0.0\n",
        "    if np.math.isnan(self.train_epoch) or np.math.isnan(self.val_epoch):\n",
        "      Restoreflag = 7\n",
        "      self.RestoreReasons[Restoreflag] += 1\n",
        "      Needtorestore = True\n",
        "      Failreason = 4\n",
        "      self.AccumulateFailures[Failreason] += 1\n",
        "      print(str(self.epochcount) + ' NAN Seen Reason ' + str(Failreason) + ' #succ ' + str(self.Numsuccess) + ' #fail ' + str(self.Numfailed) + ' ' + str(round(self.train_epoch,6)) + ' ' + str(round(self.val_epoch,6)), flush=True)\n",
        "      return TrueReturn, self.train_epoch, self.val_epoch\n",
        "\n",
        "    if self.epochcount  <= self.startepochs:\n",
        "      return FalseReturn, self.train_epoch, self.val_epoch\n",
        "\n",
        "    if not np.math.isnan(self.LastLossValue):\n",
        "      LossChange = self.train_epoch - self.LastLossValue\n",
        "      if self.ValidationFraction > 0.001:\n",
        "        ValLossChange = self.val_epoch - self.LastValLossValue\n",
        "    if LossChange <= 0:\n",
        "      if self.ValidationFraction > 0.001:\n",
        "# Quick Fix\n",
        "        self.Numsuccess +=1\n",
        "        self.AccumulateSuccesses += 1\n",
        "        if ValLossChange <= 0:\n",
        "          Failreason = 0\n",
        "        else:\n",
        "          Failreason = 2\n",
        "      else:\n",
        "        self.Numsuccess +=1\n",
        "        self.AccumulateSuccesses += 1\n",
        "        Failreason = 0\n",
        "    else:\n",
        "      Failreason = 1\n",
        "      if self.ValidationFraction > 0.001:\n",
        "        if ValLossChange > 0:\n",
        "          Failreason = 3          \n",
        "    if Failreason > 0:\n",
        "        self.Numfailed += 1\n",
        "    self.AccumulateFailures[Failreason] += 1\n",
        "\n",
        "    if (not np.math.isnan(self.LastLossValue)) and (Failreason > 0):\n",
        "      print(str(self.epochcount) + ' Reason ' + str(Failreason) + ' #succ ' + str(self.Numsuccess) + ' #fail ' + str(self.Numfailed) + ' ' + str(round(self.train_epoch,6)) \n",
        "        + ' ' + str(round(self.LastLossValue,6)) + ' '+ str(round(self.val_epoch,6))+ ' ' + str(round(self.LastValLossValue,6)), flush=True)\n",
        "    self.LastLossValue = self.train_epoch\n",
        "    self.LastValLossValue = self.val_epoch\n",
        "    \n",
        "    StoreMinLoss = False\n",
        "    if not np.math.isnan(self.MinLossValue):\n",
        "#      if (self.train_epoch < self.MinLossValue) and (self.val_epoch <= self.MinValLossValue):\n",
        "      if (self.train_epoch < self.MinLossValue):\n",
        "        if self.Numsuccess >= self.SuccessLimit:\n",
        "          StoreMinLoss = True\n",
        "    else:\n",
        "      StoreMinLoss = True\n",
        "    if StoreMinLoss:\n",
        "      self.Numsuccess = 0\n",
        "      extrastuff = ''\n",
        "      extrastuff_val = ' '\n",
        "      if not np.math.isnan(self.MinLossValue):\n",
        "        extrastuff = ' Previous ' + str(round(self.MinLossValue,7))\n",
        "        self.LittleJumpdifference  = self.MinLossValue - self.train_epoch\n",
        "        if self.ValidationFraction > 0.001:\n",
        "          if not np.math.isnan(self.MinValLossValue):\n",
        "            extrastuff_val = ' Previous ' + str(round(self.MinValLossValue,7))\n",
        "            LittleValJumpdifference = max(self.MinValLossValue - self.val_epoch, self.LittleJumpdifference)    \n",
        "      self.saveMinLosspath = self.checkpoint.save(file_prefix=self.CHECKPOINTDIR + self.RunName +'MinLoss')\n",
        "      if not self.BestLossSaved:\n",
        "        print('\\nInitial Checkpoint at ' + self.saveMinLosspath + ' from ' + self.CHECKPOINTDIR)\n",
        "      self.MinLossValue = self.train_epoch\n",
        "      self.MinValLossValue = self.val_epoch\n",
        "      if self.ValidationFraction > 0.001:\n",
        "        extrastuff_val = ' Val Loss ' + str(round(self.val_epoch,7)) + extrastuff_val\n",
        "      print(' Epoch ' + str(self.epochcount) + ' Loss ' + str(round(self.train_epoch,7)) + extrastuff + extrastuff_val+ ' Failed ' + str(self.Numfailed), flush = True)\n",
        "      self.Numfailed = 0\n",
        "      self.BestLossSaved = True\n",
        "      self.BestLossValueSaved = self.train_epoch\n",
        "      self.BestValLossValueSaved = self.val_epoch\n",
        "      self.lastsavedepoch = self.epochcount\n",
        "      self.NumberTimesSaved += 1\n",
        "      return FalseReturn, self.train_epoch, self.val_epoch\n",
        "  \n",
        "    RestoreTrainflag = -1\n",
        "    Trainrestore = False\n",
        "    if LossChange > 0.0:\n",
        "      if LossChange > self.BadJumpfraction * self.train_epoch:\n",
        "        Trainrestore = True\n",
        "        RestoreTrainflag = 0\n",
        "      if not np.math.isnan(self.LittleJumpdifference):\n",
        "        if LossChange > self.LittleJumpdifference * self.LittleJump:\n",
        "          Trainrestore = True\n",
        "          if RestoreTrainflag < 0:\n",
        "            RestoreTrainflag = 1\n",
        "      if self.BestLossSaved:  \n",
        "        if self.train_epoch < self.MinLossValue:\n",
        "          Trainrestore = False\n",
        "          RestoreTrainflag = -1\n",
        "    \n",
        "    RestoreValflag = -1\n",
        "    Valrestore = False\n",
        "    if ValLossChange > 0.0:\n",
        "      if ValLossChange > self.ValBadJumpfraction * self.val_epoch:\n",
        "        Valrestore = True\n",
        "        RestoreValflag = 0\n",
        "      if not np.math.isnan(self.LittleValJumpdifference):\n",
        "        if ValLossChange > self.LittleValJumpdifference * self.ValLittleJump:\n",
        "          Valrestore = True\n",
        "          if RestoreValflag < 0:\n",
        "            RestoreValflag = 1\n",
        "      if self.BestLossSaved:  \n",
        "        if self.val_epoch < self.MinValLossValue:\n",
        "          Valrestore = False\n",
        "          RestoreValflag = -1\n",
        "    Restoreflag = -1\n",
        "    if Trainrestore and Valrestore:\n",
        "      Needtorestore = True\n",
        "      if RestoreTrainflag == 0:\n",
        "        Restoreflag = 0\n",
        "      else:\n",
        "        Restoreflag = 1\n",
        "    elif Trainrestore:\n",
        "      Needtorestore = True\n",
        "      Restoreflag = RestoreTrainflag + 2\n",
        "    elif Valrestore:\n",
        "      Needtorestore = True\n",
        "      Restoreflag = RestoreValflag + 4\n",
        "    if (self.Numfailed >= self.FailureLimit) and (Restoreflag == -1):\n",
        "      Restoreflag = 6\n",
        "      Needtorestore = True\n",
        "    if Restoreflag >= 0:\n",
        "      self.RestoreReasons[Restoreflag] += 1\n",
        "    if Needtorestore and (not self.BestLossSaved):\n",
        "      print('bad Jump ' + str(round(LossChange,7)) + ' Epoch ' + str(self.epochcount) + ' But nothing saved')\n",
        "      return FalseReturn, self.train_epoch, self.val_epoch\n",
        "    if Needtorestore:\n",
        "      return TrueReturn, self.train_epoch, self.val_epoch\n",
        "    else:\n",
        "      return FalseReturn, self.train_epoch, self.val_epoch\n",
        "\n",
        "  def RestoreBestFit(self):\n",
        "    if self.BestLossSaved:\n",
        "      self.checkpoint.tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
        "      self.checkpoint.tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
        "      self.checkpoint.restore(save_path=self.saveMinLosspath).expect_partial()\n",
        "      self.tfepochstep  = self.checkpoint.tfepochstep \n",
        "      self.recordvalloss = self.checkpoint.tfrecordvalloss.numpy().tolist()\n",
        "      self.recordtrainloss = self.checkpoint.tfrecordtrainloss.numpy().tolist()\n",
        "      trainlen = len(self.recordtrainloss)\n",
        "      self.Numsuccess = 0\n",
        "      extrastuff = ''\n",
        "      if self.ValidationFraction > 0.001:\n",
        "        vallen =len(self.recordvalloss)\n",
        "        if vallen > 0:\n",
        "          extrastuff = ' Replaced Val Loss ' + str(round(self.recordvalloss[vallen-1],7))+ ' bad val ' + str(round(self.val_epoch,7))\n",
        "        else:\n",
        "          extrastuff = ' No previous Validation Loss'\n",
        "      print(str(self.epochcount) + ' Failed ' + str(self.Numfailed) + ' Restored Epoch ' + str(trainlen-1) + ' Replaced Loss ' + str(round(self.recordtrainloss[trainlen-1],7))\n",
        "        + ' bad ' + str(round(self.train_epoch,7)) + extrastuff + ' Checkpoint at ' + self.saveMinLosspath)\n",
        "      self.train_epoch = self.recordtrainloss[trainlen-1]\n",
        "      self.Numfailed = 0\n",
        "      self.LastLossValue = self.train_epoch\n",
        "      self.NumberTimesRestored += 1\n",
        "      if self.ValidationFraction > 0.001:\n",
        "        vallen = len(self.recordvalloss)\n",
        "        if vallen > 0:\n",
        "          self.val_epoch = self.recordvalloss[vallen-1]\n",
        "        else:\n",
        "          self.val_epoch =  0.0\n",
        "      return self.tfepochstep, self.recordtrainloss, self.recordvalloss, self.train_epoch, self.val_epoch\n",
        "\n",
        "  def PrintEndofFit(self, Numberofepochs):\n",
        "      print(startbold + 'Number of Saves ' +  str(self.NumberTimesSaved) + ' Number of Restores ' + str(self.NumberTimesRestored))\n",
        "      print('Epochs Requested ' + str(Numberofepochs) + ' Actually Stored ' + str(len(self.recordtrainloss)) + ' ' + str(self.tfepochstep.numpy()) \n",
        "      + ' Successes ' +str(self.AccumulateSuccesses) + resetfonts)\n",
        "      trainlen = len(self.recordtrainloss)\n",
        "      train_epoch1 = self.recordtrainloss[trainlen-1]\n",
        "      lineforval = ''\n",
        "      if self.ValidationFraction > 0.001:\n",
        "        lineforval = ' Last val '+ str(round(self.val_epoch,7))\n",
        "      print(startbold + 'Last loss '+ str(round(self.train_epoch,7)) + ' Last loss in History ' + str(round(train_epoch1,7))+ ' Best Saved Loss '\n",
        "      + str(round(self.BestLossValueSaved,7)) + lineforval + resetfonts)\n",
        "      print(startbold + startred +\"\\nFailure Reasons\" + resetfonts)\n",
        "      for ireason in range(0,len(self.AccumulateFailures)):\n",
        "        print('Optimization Failure ' + str(ireason) + ' ' + self.NameofFailures[ireason] + ' ' + str(self.AccumulateFailures[ireason]))\n",
        "      print(startbold + startred +\"\\nRestore Reasons\" + resetfonts)\n",
        "      for ireason in range(0,len(self.RestoreReasons)):\n",
        "        print('Backup to earlier fit ' + str(ireason) + ' ' + self.NameofRestoreReasons[ireason] + ' ' + str(self.RestoreReasons[ireason]))\n",
        "\n",
        "  def BestPossibleFit(self): # Use Best Saved if appropriate\n",
        "    if self.UseBestAvailableLoss:\n",
        "      if self.BestLossSaved:\n",
        "        if self.BestLossValueSaved < self.train_epoch:\n",
        "          self.checkpoint.tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
        "          self.checkpoint.tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
        "          self.checkpoint.restore(save_path=self.saveMinLosspath).expect_partial()\n",
        "          self.tfepochstep  = self.checkpoint.tfepochstep \n",
        "          self.recordvalloss = self.checkpoint.tfrecordvalloss.numpy().tolist()\n",
        "          self.recordtrainloss = self.checkpoint.tfrecordtrainloss.numpy().tolist()\n",
        "          trainlen = len(self.recordtrainloss)\n",
        "          Oldtraining = self.train_epoch\n",
        "          self.train_epoch = self.recordtrainloss[trainlen-1]\n",
        "          extrainfo = ''\n",
        "          if self.ValidationFraction > 0.001:\n",
        "            vallen = len(self.recordvalloss)\n",
        "            if vallen > 0:\n",
        "              extrainfo = '\\nVal Loss ' + str(round(self.recordvalloss[vallen-1],7)) + ' old Val ' + str(round(self.val_epoch,7))\n",
        "              self.val_epoch = self.recordvalloss[vallen-1] \n",
        "            else:\n",
        "              self.val_epoch = 0.0\n",
        "              extrainfo = '\\n no previous validation loss'\n",
        "          print(startpurple+ startbold + 'Switch to Best Saved Value. Restored Epoch ' + str(trainlen-1)\n",
        "          + '\\nNew Loss ' + str(round(self.recordtrainloss[trainlen-1],7)) + ' old ' + str(round(Oldtraining,7))\n",
        "          + extrainfo + '\\nCheckpoint at ' + self.saveMinLosspath + resetfonts)\n",
        "\n",
        "        else:\n",
        "          print(startpurple+ startbold + '\\nFinal fit is best: train ' + str(round(self.train_epoch,7)) + ' Val Loss ' + str(round(self.val_epoch,7)) + resetfonts)\n",
        "    return self.tfepochstep, self.recordtrainloss, self.recordvalloss, self.train_epoch, self.val_epoch   "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMvWazfckZro"
      },
      "source": [
        "class MyLSTMcustommodel(tf.keras.Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyLSTMcustommodel, self).__init__(**kwargs)\n",
        "    self.fullLSTM = MyLSTMlayer()\n",
        "\n",
        "  def compile(self, optimizer,  loss):\n",
        "      super(MyLSTMcustommodel, self).compile()\n",
        "      self.optimizer = tf.keras.optimizers.get(optimizer)\n",
        "      self.loss_object = loss\n",
        "      self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "      self.loss_tracker.reset_states()\n",
        "      self.val_tracker = tf.keras.metrics.Mean(name=\"val\")\n",
        "      self.val_tracker.reset_states()\n",
        "      return\n",
        "\n",
        "  def resetmetrics(self):\n",
        "      self.loss_tracker.reset_states()\n",
        "      self.val_tracker.reset_states()\n",
        "      return\n",
        "\n",
        "  def build_graph(self, shapes):\n",
        "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
        "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, data, Time=None):\n",
        "    if len(data) == 3:\n",
        "      X_train, y_train, sw_train = data\n",
        "    else:\n",
        "      X_train, y_train = data\n",
        "      sw_train = []\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(X_train, training=True)\n",
        "      loss = self.loss_object(y_train, predictions, sw_train)\n",
        "\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "    self.loss_tracker.update_state(loss)\n",
        "    return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data, Time=None):\n",
        "    if len(data) == 3:\n",
        "      X_val, y_val, sw_val = data\n",
        "    else:\n",
        "      X_val, y_val = data\n",
        "      sw_val = []\n",
        "\n",
        "    predictions = self(X_val, training=False)\n",
        "    loss = self.loss_object(y_val, predictions, sw_val)\n",
        "\n",
        "    self.val_tracker.update_state(loss)\n",
        "    return {\"val_loss\": self.val_tracker.result()}\n",
        "\n",
        "  def call(self, inputs, training=None, Time=None):  \n",
        "    outputs = self.fullLSTM(inputs, training=training)\n",
        "    return outputs\n",
        "\n",
        "        \n",
        "def RunLSTMCustomVersion():\n",
        "  # Run the LSTM model defined by Model and Layer class with custom training\n",
        "  # Use Tensorflow datasets\n",
        "\n",
        "  garbagecollectcall = 0\n",
        "  global LSTMvalidationfrac\n",
        "  global UsedLSTMvalidationfrac\n",
        "\n",
        "  if LocationBasedValidation:\n",
        "    UsedLSTMvalidationfrac = LocationValidationFraction\n",
        "    X_predict, y_predict, X_val, y_val = setSeparateDLinput(0)\n",
        "    InitializeDLforTimeSeries('Class custom  Version with location-based validation ',processindex,y_predict)\n",
        "    epochsize = X_predict.shape[0]\n",
        "    if UsedLSTMvalidationfrac > 0.001:\n",
        "      epochsize = X_predict.shape[0] + X_val.shape[0]\n",
        "    if UseClassweights:     \n",
        "      sw = np.empty_like(y_predict, dtype=np.float32)\n",
        "      for j in range(0,sw.shape[0]):\n",
        "        for i in range(0,NpredperseqTOT):\n",
        "          sw[j,i] = Predictionwgt[i] \n",
        "      X_train, y_train, sw_train = shuffleDLinput(X_predict, y_predict, sw)\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, sw_train))\n",
        "    else:\n",
        "      X_train, y_train = shuffleDLinput(X_predict, y_predict)\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "      sw_train =[]\n",
        "    \n",
        "    if UsedLSTMvalidationfrac > 0.001:\n",
        "      if UseClassweights:     \n",
        "        sw_val = np.empty_like(y_val, dtype=np.float32)\n",
        "        for j in range(0,sw_val.shape[0]):\n",
        "          for i in range(0,NpredperseqTOT):\n",
        "            sw_val[j,i] = Predictionwgt[i] \n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val, sw_val))\n",
        "      else:\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "        sw_val =[]\n",
        "\n",
        "# Dimensions are X_predict: OuterBatchDimension,Tseq,NpropperseqTOT\n",
        "# OR if SymbolicWindows OuterBatchDimension,1,1\n",
        "# y_predict OuterBatchDimension,NpredperseqTOT\n",
        "  else:\n",
        "    X_predict, y_predict = setDLinput(Spacetime = False)\n",
        "    InitializeDLforTimeSeries('Class custom  Version ',processindex,y_predict)\n",
        "    epochsize = X_predict.shape[0]\n",
        "\n",
        "    if UseClassweights:     \n",
        "      sw = np.empty_like(y_predict, dtype=np.float32)\n",
        "      for j in range(0,sw.shape[0]):\n",
        "        for i in range(0,NpredperseqTOT):\n",
        "          sw[j,i] = Predictionwgt[i] \n",
        "      X_train, y_train, sw_train = shuffleDLinput(X_predict, y_predict, sw)\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, sw_train))\n",
        "    else:\n",
        "      X_train, y_train = shuffleDLinput(X_predict, y_predict)\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "      sw_train =[]\n",
        "\n",
        "    val_dataset =[]\n",
        "    if UsedLSTMvalidationfrac > 0.001:\n",
        "      total = X_train.shape[0]\n",
        "      totval = int(UsedLSTMvalidationfrac*total)\n",
        "      print(\" Validation samples \", totval, \" Training samples \", total-totval)\n",
        "      if totval > 0:\n",
        "        val_dataset = train_dataset.take(totval)\n",
        "        train_dataset = train_dataset.skip(totval)\n",
        "      else:\n",
        "        UsedLSTMvalidationfrac = 0.0\n",
        "\n",
        "  train_dataset = train_dataset.shuffle(buffer_size = OuterBatchDimension, reshuffle_each_iteration=True)\n",
        "  train_dataset = train_dataset.batch(LSTMbatch_size)\n",
        "  if UsedLSTMvalidationfrac > 0.001:\n",
        "    val_dataset = val_dataset.batch(LSTMbatch_size)\n",
        "\n",
        "\n",
        "  myLSTMcustommodel = MyLSTMcustommodel(name ='myLSTMcustommodel')\n",
        "\n",
        "  myLSTMcustommodel.compile(loss= weightedcustom_lossGCF1, optimizer= LSTMoptimizer)\n",
        "\n",
        "  recordtrainloss = []\n",
        "  recordvalloss = []\n",
        "  tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
        "  tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
        "  tfepochstep = tf.Variable(0, trainable = False)\n",
        "\n",
        "  usecustomfit = True\n",
        "  if usecustomfit and UseClassweights:\n",
        "\n",
        "# Set up checkpoints to read or write\n",
        "    mycheckpoint = tf.train.Checkpoint(optimizer=myLSTMcustommodel.optimizer, \n",
        "                                     model=myLSTMcustommodel, tfepochstep=tf.Variable(0),\n",
        "                                     tfrecordtrainloss=tfrecordtrainloss,tfrecordvalloss=tfrecordvalloss)\n",
        "    \n",
        "# This restores back up\n",
        "    if Restorefromcheckpoint:\n",
        "      save_path = inputCHECKPOINTDIR + inputRunName + inputCheckpointpostfix\n",
        "      mycheckpoint.restore(save_path=save_path).expect_partial()\n",
        "      tfepochstep  = mycheckpoint.tfepochstep \n",
        "      recordvalloss = mycheckpoint.tfrecordvalloss.numpy().tolist()\n",
        "      recordtrainloss = mycheckpoint.tfrecordtrainloss.numpy().tolist()\n",
        "      trainlen = len(recordtrainloss)\n",
        "      extrainfo = ''\n",
        "      vallen = len(recordvalloss)\n",
        "      SavedTrainLoss = recordtrainloss[trainlen-1]\n",
        "      SavedValLoss = 0.0\n",
        "      if vallen > 0:\n",
        "        extrainfo = ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
        "        SavedValLoss = recordvalloss[vallen-1]\n",
        "      print(startbold + 'Network restored from ' + save_path + '\\nLoss ' + str(round(recordtrainloss[trainlen-1],7)) \n",
        "       + extrainfo + ' Epochs ' + str(tfepochstep.numpy()) + resetfonts )\n",
        "      LSTMTFMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= True, \n",
        "              Restored_path = save_path,  ValidationFraction = UsedLSTMvalidationfrac, SavedTrainLoss = SavedTrainLoss, \n",
        "              SavedValLoss =SavedValLoss)\n",
        "    else:\n",
        "      LSTMTFMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= False, \n",
        "                                       ValidationFraction = UsedLSTMvalidationfrac)\n",
        "\n",
        "# This just does analysis      \n",
        "    if AnalysisOnly:\n",
        "      if OutputNetworkPictures:\n",
        "        outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
        "        outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
        "        tf.keras.utils.plot_model(myLSTMcustommodel.build_graph([Tseq,NpropperseqTOT]), \n",
        "                            show_shapes=True, to_file = outputpicture1,\n",
        "                            show_dtype=True, \n",
        "                            expand_nested=True)\n",
        "        tf.keras.utils.plot_model(myLSTMcustommodel.fullLSTM.build_graph([Tseq,NpropperseqTOT]), \n",
        "                            show_shapes=True, to_file = outputpicture2,\n",
        "                            show_dtype=True, \n",
        "                            expand_nested=True)\n",
        "      if SymbolicWindows:\n",
        "        finalizeDL(myLSTMcustommodel,recordtrainloss, recordvalloss,UsedLSTMvalidationfrac,\n",
        "              ReshapedSequencesTOT, RawInputPredictionsTOT,0,LabelFit = 'Non-sampled LSTM Fit')\n",
        "      else:\n",
        "        finalizeDL(myLSTMcustommodel,recordtrainloss, recordvalloss,UsedLSTMvalidationfrac,\n",
        "              RawInputSequencesTOT, RawInputPredictionsTOT,0,LabelFit = 'Non-sampled LSTM Fit')\n",
        "      return\n",
        "\n",
        "# Initialize progress bars\n",
        "    pbar = notebook.trange(LSTMepochs, desc='Training loop', unit ='epoch')\n",
        "    bbar = notebook.trange(epochsize,  desc='Batch    loop', unit  = 'sample')\n",
        "\n",
        "    train_epoch = 0.0 # Training Loss this epoch\n",
        "    val_epoch = 0.0 # Validation Loss this epoch\n",
        "\n",
        "    Ctime1 = 0.0\n",
        "    Ctime2 = 0.0\n",
        "    Ctime3 = 0.0\n",
        "    GarbageCollect = True\n",
        "\n",
        "    for e in pbar:\n",
        "      myLSTMcustommodel.resetmetrics()\n",
        "      train_lossoverbatch=[]\n",
        "      val_lossoverbatch=[]\n",
        "      \n",
        "      if batchperepoch:\n",
        "        qbar = notebook.trange(epochsize, desc='Batch loop epoch ' +str(e))\n",
        "\n",
        "      for batch, (X_train, y_train, sw_train) in enumerate(train_dataset.take(-1)):\n",
        "        Numinbatch = X_train.shape[0]\n",
        "        # SymbolicWindows X_train is indexed by Batch index, 1(replace by Window), 1 (replace by properties)\n",
        "        if SymbolicWindows:\n",
        "          StopWatch.start('label1')\n",
        "          X_train = X_train.numpy()          \n",
        "          X_train = np.reshape(X_train,Numinbatch)\n",
        "          iseqarray = np.right_shift(X_train,16)\n",
        "          ilocarray = np.bitwise_and(X_train, 0b1111111111111111)\n",
        "          StopWatch.stop('label1')\n",
        "          Ctime1 += StopWatch.get('label1', digits=4)\n",
        "          StopWatch.start('label3')\n",
        "          X_train_withSeq = list()\n",
        "          for iloc in range(0,Numinbatch):\n",
        "            X_train_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
        "#         X_train_withSeq=[ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq] for iloc in range(0,Numinbatch)]\n",
        "          StopWatch.stop('label3')\n",
        "          Ctime3 += StopWatch.get('label3', digits=5)\n",
        "          StopWatch.start('label2')\n",
        "          loss = myLSTMcustommodel.train_step((np.array(X_train_withSeq), y_train, sw_train))\n",
        "          StopWatch.stop('label2')\n",
        "          Ctime2 += StopWatch.get('label2', digits=4)\n",
        "\n",
        "        else:\n",
        "          loss = myLSTMcustommodel.train_step((X_train, y_train, sw_train))\n",
        "\n",
        "        if GarbageCollect:\n",
        "          if SymbolicWindows:\n",
        "            X_train_withSeq = None\n",
        "          X_train = None\n",
        "          y_train = None\n",
        "          sw_train = None\n",
        "          if garbagecollectcall > GarbageCollectionLimit:\n",
        "            garbagecollectcall = 0\n",
        "            gc.collect()\n",
        "          garbagecollectcall += 1\n",
        "\n",
        "        localloss = loss[\"loss\"].numpy()\n",
        "        train_lossoverbatch.append(localloss)\n",
        "\n",
        "        if batchperepoch:\n",
        "          qbar.update(LSTMbatch_size)\n",
        "          qbar.set_postfix(Loss = localloss, Epoch = e)\n",
        "        bbar.update(Numinbatch)\n",
        "        bbar.set_postfix(Loss = localloss, Epoch = e)\n",
        "# End Training step for one batch\n",
        "\n",
        "# Start Validation \n",
        "      if UsedLSTMvalidationfrac > 0.001:\n",
        "        for batch, (X_val, y_val, sw_val) in enumerate(val_dataset.take(-1)):\n",
        "          Numinbatch = X_val.shape[0]\n",
        "          # SymbolicWindows X_val is indexed by Batch index, 1(replace by Window), 1 (replace by properties)\n",
        "          if SymbolicWindows:\n",
        "            StopWatch.start('label1')\n",
        "            X_val = X_val.numpy()          \n",
        "            X_val = np.reshape(X_val,Numinbatch)\n",
        "            iseqarray = np.right_shift(X_val,16)\n",
        "            ilocarray = np.bitwise_and(X_val, 0b1111111111111111)\n",
        "            StopWatch.stop('label1')\n",
        "            Ctime1 += StopWatch.get('label1', digits=4)\n",
        "            StopWatch.start('label3')\n",
        "            X_valFull = list()\n",
        "            for iloc in range(0,Numinbatch):\n",
        "              X_valFull.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
        "            StopWatch.stop('label3')\n",
        "            Ctime3 += StopWatch.get('label3', digits=5)\n",
        "            StopWatch.start('label2')\n",
        "            loss = myLSTMcustommodel.test_step((np.array(X_valFull), y_val, sw_val))\n",
        "            StopWatch.stop('label2')\n",
        "            Ctime2 += StopWatch.get('label2', digits=4)\n",
        "\n",
        "          else:\n",
        "            loss = myLSTMcustommodel.test_step((X_val, y_val, sw_val))\n",
        "\n",
        "          localval = loss[\"val_loss\"].numpy()\n",
        "          val_lossoverbatch.append(localval)\n",
        "          \n",
        "          bbar.update(X_val.shape[0])\n",
        "          bbar.set_postfix(Val_loss = localval, Epoch = e)\n",
        "# End Batch\n",
        "\n",
        "      train_epoch = train_lossoverbatch[-1]\n",
        "      recordtrainloss.append(train_epoch)\n",
        "      mycheckpoint.tfrecordtrainloss = tf.Variable(recordtrainloss)\n",
        "\n",
        "      val_epoch = 0.0\n",
        "      if UsedLSTMvalidationfrac > 0.001:\n",
        "        val_epoch = val_lossoverbatch[-1]\n",
        "        recordvalloss.append(val_epoch)\n",
        "        mycheckpoint.tfrecordvalloss = tf.Variable(recordvalloss)\n",
        "\n",
        "      pbar.set_postfix(Loss = train_epoch, Val = val_epoch)\n",
        "      bbar.reset()\n",
        "      tfepochstep = tfepochstep + 1\n",
        "      mycheckpoint.tfepochstep.assign(tfepochstep)\n",
        "\n",
        "# Decide on best fit\n",
        "      MonitorResult, train_epoch, val_epoch = LSTMTFMonitor.EpochEvaluate(e,train_epoch, val_epoch, \n",
        "          tfepochstep, recordtrainloss, recordvalloss)\n",
        "      if MonitorResult==1:\n",
        "        tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = LSTMTFMonitor.RestoreBestFit() # Restore Best Fit\n",
        "      else:\n",
        "        continue\n",
        "# *********************** End of Epoch Loop\n",
        "\n",
        "# Print Fit details\n",
        "    print(startbold + 'Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)) + resetfonts)\n",
        "    LSTMTFMonitor.PrintEndofFit(LSTMepochs)\n",
        "\n",
        "# Set Best Possible Fit\n",
        "    tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = LSTMTFMonitor.BestPossibleFit()\n",
        "\n",
        "    if Checkpointfinalstate:\n",
        "      savepath = mycheckpoint.save(file_prefix=CHECKPOINTDIR + RunName)\n",
        "      print('Checkpoint at ' + savepath + ' from ' + CHECKPOINTDIR)\n",
        "    trainlen = len(recordtrainloss)\n",
        "    extrainfo = ''\n",
        "    if UsedLSTMvalidationfrac > 0.001:\n",
        "      vallen = len(recordvalloss)\n",
        "      extrainfo = ' Val Epoch ' + str(vallen-1) + ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
        "    print('Train Epoch ' + str(trainlen-1) + ' Train Loss ' + str(round(recordtrainloss[trainlen-1],7)) + extrainfo)\n",
        "\n",
        "\n",
        "  else:\n",
        "    the_callbacks = [TqdmCallback()]\n",
        "    modelresult = myLSTMcustommodel.fit(train_dataset,\n",
        "          validation_data = val_dataset,\n",
        "          epochs=LSTMepochs,\n",
        "          batch_size=None,\n",
        "          verbose = LSTMverbose,\n",
        "          callbacks=the_callbacks\n",
        "          )\n",
        "    recordtrainloss = modelresult.history['loss']\n",
        "    recordvalloss = modelresult.history['val_loss']\n",
        "\n",
        "  myLSTMcustommodel.fullLSTM.summary()\n",
        "  myLSTMcustommodel.summary()\n",
        "  if OutputNetworkPictures:\n",
        "    outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
        "    outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
        "    tf.keras.utils.plot_model(myLSTMcustommodel.build_graph([Tseq,NpropperseqTOT]), \n",
        "                        show_shapes=True, to_file = outputpicture1,\n",
        "                        show_dtype=True, \n",
        "                        expand_nested=True)\n",
        "    tf.keras.utils.plot_model(myLSTMcustommodel.fullLSTM.build_graph([Tseq,NpropperseqTOT]), \n",
        "                        show_shapes=True, to_file = outputpicture2,\n",
        "                        show_dtype=True, \n",
        "                        expand_nested=True)\n",
        "  if SymbolicWindows:\n",
        "    finalizeDL(myLSTMcustommodel,recordtrainloss,recordvalloss,UsedLSTMvalidationfrac,ReshapedSequencesTOT, RawInputPredictionsTOT,0)\n",
        "  else:\n",
        "    finalizeDL(myLSTMcustommodel,recordtrainloss,recordvalloss,UsedLSTMvalidationfrac,RawInputSequencesTOT, RawInputPredictionsTOT,0)\n",
        "  return"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49wk75MekbeT"
      },
      "source": [
        "def StandaloneLSTM():\n",
        "# run pure LSTM with no attention (transformer) with simple Keras sequential model\n",
        "\n",
        "  X_predict, y_predict = setDLinput(Spacetime = False)\n",
        "  X_train, y_train = shuffleDLinput(X_predict, y_predict)\n",
        "\n",
        "  # n_timesteps=n_steps_in=Tseq, \n",
        "  # n_features= NpropperseqTOT,  \n",
        "  # n_outputs = NpredperseqTOT       \n",
        "  InitializeDLforTimeSeries('Keras Sequential Version ',processindex,y_predict)\n",
        "\n",
        "  # define model\n",
        "  StandaloneLSTMmodel = Sequential()\n",
        "  \n",
        "  if(LSTMInitialMLP > 0):\n",
        "      StandaloneLSTMmodel.add(Dense(LSTMInitialMLP, activation=LSTMactivationvalue, input_shape=(Tseq,NpropperseqTOT)))\n",
        "      nextround = LSTMInitialMLP\n",
        "  else:\n",
        "      nextround = NpropperseqTOT\n",
        "\n",
        "  StandaloneLSTMmodel.add(LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
        "                  activation= LSTMactivationvalue , return_sequences=True, recurrent_activation= LSTMrecurrent_activation,\n",
        "                  input_shape=(Tseq, nextround)))\n",
        "\n",
        "  if(LSTMThirdLayer):\n",
        "      StandaloneLSTMmodel.add(LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout2, dropout = LSTMdropout2,\n",
        "                  activation= LSTMactivationvalue , return_sequences=True, recurrent_activation= LSTMrecurrent_activation,\n",
        "                  input_shape=(Tseq, number_LSTMnodes)))\n",
        "      StandaloneLSTMmodel.add(LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout2, dropout = LSTMdropout2,\n",
        "                      activation= LSTMactivationvalue , recurrent_activation=LSTMrecurrent_activation,\n",
        "                      input_shape=(Tseq, number_LSTMnodes)))\n",
        "  else:\n",
        "      StandaloneLSTMmodel.add(LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout2, dropout = LSTMdropout2,\n",
        "              activation= LSTMactivationvalue, recurrent_activation=LSTMrecurrent_activation,\n",
        "              input_shape=(Tseq, number_LSTMnodes)))\n",
        "      \n",
        "  if(LSTMFinalMLP > 0):\n",
        "      StandaloneLSTMmodel.add(Dense(LSTMFinalMLP, activation=LSTMactivationvalue))\n",
        "  StandaloneLSTMmodel.add(Dense(NpredperseqTOT))\n",
        "  \n",
        "  if CustomLoss == 0:\n",
        "      StandaloneLSTMmodel.compile(loss='mse', optimizer= LSTMoptimizer)\n",
        "  if CustomLoss == 1:\n",
        "      StandaloneLSTMmodel.compile(loss= custom_lossGCF1, optimizer= LSTMoptimizer)\n",
        "  if CustomLoss == 4:    \n",
        "      StandaloneLSTMmodel.compile(loss= custom_lossGCF4, optimizer= LSTMoptimizer)\n",
        "\n",
        "  modelsummarystring = get_model_summary(StandaloneLSTMmodel)\n",
        "  if( processindex == 0 ):\n",
        "      print(modelsummarystring)\n",
        "\n",
        "  the_callbacks = [TqdmCallback(),]\n",
        "  if UseClassweights:   \n",
        "      cw = {}\n",
        "      for i in range(0,NpredperseqTOT):\n",
        "        cw[i] = Predictionwgt[i]     \n",
        "      modelresult = StandaloneLSTMmodel.fit(X_train, y_train,\n",
        "            epochs=LSTMepochs,\n",
        "            batch_size=LSTMbatch_size,\n",
        "            class_weight = cw,\n",
        "            verbose=LSTMverbose,\n",
        "            validation_split=UsedLSTMvalidationfrac,\n",
        "            callbacks=the_callbacks\n",
        "            )\n",
        "  else:\n",
        "      modelresult = StandaloneLSTMmodel.fit(X_train, y_train,\n",
        "            epochs=LSTMepochs,\n",
        "            batch_size=LSTMbatch_size,\n",
        "            verbose=LSTMverbose,\n",
        "            validation_split=UsedLSTMvalidationfrac,\n",
        "            callbacks=the_callbacks\n",
        "            )  \n",
        "  \n",
        "  recordtrainloss = modelresult.history['loss']\n",
        "  recordvalloss = modelresult.history['val_loss']\n",
        "  finalizeDL(StandaloneLSTMmodel, recordtrainloss, recordvalloss, UsedLSTMvalidationfrac,\n",
        "             RawInputSequencesTOT, RawInputPredictionsTOT,0, LabelFit = 'Best LSTM Fit')\n",
        "  return\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YowQQ4Cmkc1B"
      },
      "source": [
        "# Run LSTM Only\n",
        "AnalysisOnly = True\n",
        "Dumpoutkeyplotsaspics = True\n",
        "Restorefromcheckpoint = False\n",
        "Checkpointfinalstate = True\n",
        "if AnalysisOnly:\n",
        "  Restorefromcheckpoint = True\n",
        "  Checkpointfinalstate = False\n",
        "if Restorefromcheckpoint:\n",
        "  inputRunName = RunName\n",
        "  inputCHECKPOINTDIR = CHECKPOINTDIR\n",
        "  inputRunName = 'CovidA21-LSTM4'\n",
        "  inputCheckpointpostfix = '-55'\n",
        "  inputCHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + inputRunName + \"dir/\"\n",
        "\n",
        "batchperepoch = False # if True output a batch bar for each epoch\n",
        "GlobalSpacetime = False\n",
        "IncreaseNloc_sample = 1\n",
        "DecreaseNloc_sample = 1\n",
        "SkipDL2F = False\n",
        "\n",
        "# Run Pure LSTM\n",
        "LSTMepochs = 100\n",
        "number_LSTMnodes= 48\n",
        "LSTMFinalMLP = 128\n",
        "LSTMInitialMLP = 128\n",
        "LSTMThirdLayer = False\n",
        "processindex = 0\n",
        "standaloneLSTMrun = False\n",
        "ClassLSTMrun = True\n",
        "CustomTraining = True\n",
        "\n",
        "if ClassLSTMrun and CustomTraining:\n",
        "  FullSetValidation = False\n",
        "  LSTMTFMonitor = TFTrainingMonitor()\n",
        "  if ReadJan2021Covid or ReadApril2021Covid:\n",
        "    LSTMTFMonitor.SetControlParms(SuccessLimit = 3,FailureLimit = 2)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgHnQHVHkel6"
      },
      "source": [
        "def PrintLSTMandBasicStuff(model):\n",
        "  if SymbolicWindows:\n",
        "    print(startbold  + startred + 'Symbolic Windows used to save space'+resetfonts)\n",
        "  else:\n",
        "    print(startbold  + startred + 'Symbolic Windows NOT used'+resetfonts)\n",
        "  print('Training Locations ' + str(TrainingNloc) + ' Validation Locations ' + str(ValidationNloc) +\n",
        "        ' Sequences ' + str(Num_Seq))\n",
        "  if LocationBasedValidation:\n",
        "    print(startbold  + startred + \" Location Based Validation with fraction \" + str(LocationValidationFraction)+resetfonts)\n",
        "    if RestartLocationBasedValidation:\n",
        "      print(startbold  + startred + \" Using Validation set saved in \" + RestartRunName+resetfonts)\n",
        "  print('\\nAre futures predicted ' + str(UseFutures) + ' Custom Loss Pointer ' + str(CustomLoss) + ' Class weights used ' + str(UseClassweights))\n",
        "  \n",
        "  print('\\nProperties per sequence ' + str(NpropperseqTOT))\n",
        "  print('\\n' + startbold +startpurple + 'Properties ' + resetfonts)\n",
        "  labelline = 'Name   '\n",
        "  for propval in range (0,7):\n",
        "    labelline += QuantityStatisticsNames[propval] + '    '\n",
        "  print('\\n' + startbold + labelline + resetfonts)\n",
        "  for iprop in range(0,NpropperseqTOT):\n",
        "    line = startbold + startpurple + str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]] + resetfonts  \n",
        "    jprop = PropertyAverageValuesPointer[iprop]\n",
        "    line += ' Root ' + str(QuantityTakeroot[jprop])\n",
        "    for proppredval in range (0,7):\n",
        "      line += ' ' + str(round(QuantityStatistics[jprop,proppredval],3))\n",
        "    print(line)\n",
        "\n",
        "  print('\\nPredictions per sequence ' + str(NpredperseqTOT))\n",
        "  print('\\n' + startbold +startpurple + 'Predictions ' + resetfonts)\n",
        "  print('\\n' + startbold + labelline + resetfonts)\n",
        "  for ipred in range(0,NpredperseqTOT):\n",
        "    line = startbold + startpurple + str(ipred) + ' ' + Predictionname[ipred] + ' wgt ' + str(round(Predictionwgt[ipred],3)) + resetfonts + ' '\n",
        "    jpred = PredictionAverageValuesPointer[ipred]\n",
        "    line += ' Root ' + str(QuantityTakeroot[jpred])\n",
        "    for proppredval in range (0,7):\n",
        "      line += ' ' + str(round(QuantityStatistics[jpred,proppredval],3))\n",
        "    print(line)\n",
        "  print('\\n')\n",
        "\n",
        "  if model == 0:\n",
        "    print('Number of LSTMworkers ' + str(number_of_LSTMworkers))\n",
        "    print('Number of epochs for each LSTMworker ' + str(LSTMepochs))\n",
        "    print('LSTM Validation Fraction ' +str(LSTMvalidationfrac) + ' Method to chose best solution '+ str(bestmethod))\n",
        "    print('Batch size for LSTM ' + str(LSTMbatch_size))\n",
        "    print('LSTM Optimizer ' + str(LSTMoptimizer))\n",
        "  else:\n",
        "    print('Number of epochs for Transformer ' + str(Transformerepochs))\n",
        "  \n",
        "  print('LSTM Activation Method ' + str(LSTMactivationvalue))\n",
        "  print('LSTM recurrent Activation method ' + str(LSTMrecurrent_activation)) \n",
        "  print('LSTM Dropout Layer 1 ' +str(LSTMdropout1) + 'LSTM Recurrent Dropout Layer 1 ' +str(LSTMrecurrent_dropout1) + ' LSTM Dropout Layer >= 2 ' +str(LSTMdropout2) + 'LSTM Recurrent Dropout Layer >=2 ' +str(LSTMrecurrent_dropout2))\n",
        "  print('Number of hidden LSTM nodes ' + str(number_LSTMnodes) + ' Is there a third LSTM layer? ' + str(LSTMThirdLayer))\n",
        "  print('LSTM Initial Embedding layer ' + str(LSTMInitialMLP) + ' Final LSTM Layer ' + str(LSTMFinalMLP))\n",
        "  print('LSTM Verbose Option ' + str(LSTMverbose))\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f017fb00d9b849ffacf8e3d75545d6b1",
            "ba2e4138179949e682eae8a4d2477110",
            "94c8788c68894931aa617a6756249e18",
            "1b0cc4c9fabd48159a28f4d4e98c5088",
            "520c383d92fb4374a6ae834f84dd9e66",
            "09dfb5208d004d0e83a8b633927c02e4",
            "bdcaabc54eb34954ab3c994279548d2c",
            "c6f36177546b48448d179defff592b3a"
          ]
        },
        "id": "yhhLQNRbkf5E",
        "outputId": "0f2a3c15-e917-46b9-b23c-eaf93162189e"
      },
      "source": [
        "\n",
        "current_time = timenow()\n",
        "runtype = ''\n",
        "if Restorefromcheckpoint:\n",
        "  runtype = 'Restarted '\n",
        "if standaloneLSTMrun or ClassLSTMrun:\n",
        "  print(wraptotext(startbold + startred +  current_time + ' '  + runtype + RunName + ' ' + RunComment + resetfonts))\n",
        "  PrintLSTMandBasicStuff(0)\n",
        "if standaloneLSTMrun:\n",
        "  StandaloneLSTM()\n",
        "if ClassLSTMrun:\n",
        "  if SymbolicWindows:\n",
        "    CustomTraining = True\n",
        "  if CustomTraining:\n",
        "    RunLSTMCustomVersion()\n",
        "  else:\n",
        "    RunLSTMClassVersion()\n",
        "if standaloneLSTMrun or ClassLSTMrun:\n",
        "  print(startbold + startpurple +  'LSTM run completed ' + runtype + RunName + ' ' + RunComment + resetfonts)\n",
        "  sys.exit(0)\n",
        "print(startbold + startpurple +  current_time + ' UTC Start Hybrid Transformer run ' + RunName + ' ' + RunComment + resetfonts)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[31m06/07/2021, 03:20:55 UTC Restarted CovidA21-LSTM4Analyze April 14 Covid 2021 Dataset; Old set of Properties; NO\n",
            "Validation; LSTM; Futures; 500 counties\u001b[0m\n",
            "\u001b[1m\u001b[31mSymbolic Windows NOT used\u001b[0m\n",
            "Training Locations 500 Validation Locations 0 Sequences 396\n",
            "\n",
            "Are futures predicted True Custom Loss Pointer 1 Class weights used True\n",
            "\n",
            "Properties per sequence 23\n",
            "\n",
            "\u001b[1m\u001b[35mProperties \u001b[0m\n",
            "\n",
            "\u001b[1mName   Min    Max    Norm    Mean    Std    Normed Mean    Normed Std    \u001b[0m\n",
            "\u001b[1m\u001b[35m0 Cases\u001b[0m Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m1 Deaths\u001b[0m Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m2 Age Distribution\u001b[0m Root 1 0.248 1.0 1.33 0.507 0.091 0.345 0.121\n",
            "\u001b[1m\u001b[35m3 Air Pollution\u001b[0m Root 1 0.0 1.0 1.0 0.413 0.115 0.413 0.115\n",
            "\u001b[1m\u001b[35m4 Co-morbidities\u001b[0m Root 1 0.029 0.581 1.813 0.285 0.099 0.463 0.179\n",
            "\u001b[1m\u001b[35m5 Demographics\u001b[0m Root 1 0.189 1.0 1.233 0.503 0.112 0.388 0.139\n",
            "\u001b[1m\u001b[35m6 Disease Spread\u001b[0m Root 1 0.0 1.0 1.0 0.234 0.243 0.234 0.243\n",
            "\u001b[1m\u001b[35m7 Health Disparities\u001b[0m Root 1 0.004 0.848 1.184 0.264 0.158 0.308 0.187\n",
            "\u001b[1m\u001b[35m8 Hospital Beds\u001b[0m Root 1 0.0 1.0 1.0 0.456 0.086 0.456 0.086\n",
            "\u001b[1m\u001b[35m9 Mobility\u001b[0m Root 1 0.499 1.0 1.996 0.683 0.07 0.367 0.14\n",
            "\u001b[1m\u001b[35m10 Residential Density\u001b[0m Root 1 0.016 0.994 1.023 0.577 0.264 0.574 0.27\n",
            "\u001b[1m\u001b[35m11 Social Distancing\u001b[0m Root 1 0.0 1.0 1.0 0.904 0.166 0.904 0.166\n",
            "\u001b[1m\u001b[35m12 Testing\u001b[0m Root 1 0.0 1.0 1.0 0.569 0.228 0.569 0.228\n",
            "\u001b[1m\u001b[35m13 Transmissible Cases\u001b[0m Root 1 0.0 1.0 1.0 0.527 0.166 0.527 0.166\n",
            "\u001b[1m\u001b[35m14 voting\u001b[0m Root 1 0.144 0.945 1.249 0.514 0.148 0.462 0.185\n",
            "\u001b[1m\u001b[35m15 LinearSpace\u001b[0m Root 1 0.0 1.0 1.0 0.5 0.289 0.5 0.289\n",
            "\u001b[1m\u001b[35m16 Constant\u001b[0m Root 1 0.5 0.5 1.0 0.5 0.0 0.5 0.0\n",
            "\u001b[1m\u001b[35m17 LinearTime\u001b[0m Root 1 0.0 1.0 1.0 0.5 0.289 0.5 0.289\n",
            "\u001b[1m\u001b[35m18 P2-Time\u001b[0m Root 1 -1.0 1.0 1.0 0.0 0.447 0.0 0.447\n",
            "\u001b[1m\u001b[35m19 P3-Time\u001b[0m Root 1 -1.0 1.0 1.0 0.0 0.378 0.0 0.378\n",
            "\u001b[1m\u001b[35m20 P4-Time\u001b[0m Root 1 -1.0 1.0 1.0 0.0 0.333 0.0 0.333\n",
            "\u001b[1m\u001b[35m21 CosWeekly\u001b[0m Root 1 -1.0 1.0 1.0 0.0 0.707 0.0 0.707\n",
            "\u001b[1m\u001b[35m22 SinWeekly\u001b[0m Root 1 -1.0 1.0 1.0 0.0 0.707 0.0 0.707\n",
            "\n",
            "Predictions per sequence 38\n",
            "\n",
            "\u001b[1m\u001b[35mPredictions \u001b[0m\n",
            "\n",
            "\u001b[1mName   Min    Max    Norm    Mean    Std    Normed Mean    Normed Std    \u001b[0m\n",
            "\u001b[1m\u001b[35m0 Next Cases wgt 1.0\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m1 Next Deaths wgt 1.0\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m2 Casesday2 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m3 Deathsday2 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m4 Casesday3 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m5 Deathsday3 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m6 Casesday4 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m7 Deathsday4 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m8 Casesday5 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m9 Deathsday5 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m10 Casesday6 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m11 Deathsday6 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m12 Casesday7 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m13 Deathsday7 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m14 Casesday8 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m15 Deathsday8 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m16 Casesday9 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m17 Deathsday9 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m18 Casesday10 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m19 Deathsday10 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m20 Casesday11 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m21 Deathsday11 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m22 Casesday12 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m23 Deathsday12 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m24 Casesday13 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m25 Deathsday13 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m26 Casesday14 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m27 Deathsday14 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m28 Casesday15 wgt 0.071\u001b[0m  Root 2 0.0 171.531 0.006 7.735 7.451 0.045 0.043\n",
            "\u001b[1m\u001b[35m29 Deathsday15 wgt 0.071\u001b[0m  Root 2 0.0 39.408 0.025 0.783 1.202 0.02 0.03\n",
            "\u001b[1m\u001b[35m30 LinearSpace wgt 0.25\u001b[0m  Root 1 0.0 1.0 1.0 0.5 0.289 0.5 0.289\n",
            "\u001b[1m\u001b[35m31 Constant wgt 0.25\u001b[0m  Root 1 0.5 0.5 1.0 0.5 0.0 0.5 0.0\n",
            "\u001b[1m\u001b[35m32 LinearTime wgt 0.25\u001b[0m  Root 1 0.0 1.0 1.0 0.5 0.289 0.5 0.289\n",
            "\u001b[1m\u001b[35m33 P2-Time wgt 0.25\u001b[0m  Root 1 -1.0 1.0 1.0 0.0 0.447 0.0 0.447\n",
            "\u001b[1m\u001b[35m34 P3-Time wgt 0.25\u001b[0m  Root 1 -1.0 1.0 1.0 0.0 0.378 0.0 0.378\n",
            "\u001b[1m\u001b[35m35 P4-Time wgt 0.25\u001b[0m  Root 1 -1.0 1.0 1.0 0.0 0.333 0.0 0.333\n",
            "\u001b[1m\u001b[35m36 CosWeekly wgt 0.25\u001b[0m  Root 1 -1.0 1.0 1.0 0.0 0.707 0.0 0.707\n",
            "\u001b[1m\u001b[35m37 SinWeekly wgt 0.25\u001b[0m  Root 1 -1.0 1.0 1.0 0.0 0.707 0.0 0.707\n",
            "\n",
            "\n",
            "Number of LSTMworkers 1\n",
            "Number of epochs for each LSTMworker 100\n",
            "LSTM Validation Fraction 0.0 Method to chose best solution 1\n",
            "Batch size for LSTM 500\n",
            "LSTM Optimizer adam\n",
            "LSTM Activation Method selu\n",
            "LSTM recurrent Activation method sigmoid\n",
            "LSTM Dropout Layer 1 0.2LSTM Recurrent Dropout Layer 1 0.2 LSTM Dropout Layer >= 2 0.2LSTM Recurrent Dropout Layer >=2 0.2\n",
            "Number of hidden LSTM nodes 48 Is there a third LSTM layer? False\n",
            "LSTM Initial Embedding layer 128 Final LSTM Layer 128\n",
            "LSTM Verbose Option 0\n",
            "\u001b[1m06/07/2021, 03:20:55 UTC Class custom  Version \u001b[0m Window Size 13 Number of samples over time that sequence starts\n",
            "at and location:198000 Number input features per sequence:23 Number of predicted outputs per sequence:38 Batch_size:500\n",
            "n_nodes:48 epochs:100\n",
            " is NaN  105000  percent  1.4  not NaN  7419000\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "\u001b[1mNetwork restored from /content/gdrive/My Drive/Colab Datasets/COVIDJuly2020/checkpoints/CovidA21-LSTM4dir/CovidA21-LSTM4-55\n",
            "Loss 0.0023824 Epochs 160\u001b[0m\n",
            "\u001b[1m\u001b[31m06/07/2021, 03:21:06 UTC CovidA21-LSTM4Analyze finalizeDL April 14 Covid 2021 Dataset; Old set of Properties; NO Validation; LSTM; Futures; 500 counties\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[31m06/07/2021, 03:21:07 UTC CovidA21-LSTM4Analyze DLPrediction April 14 Covid 2021 Dataset; Old set of Properties; NO Validation; LSTM; Futures; 500 counties\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f017fb00d9b849ffacf8e3d75545d6b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Predict loop', max=396.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a10502131660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mCustomTraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mCustomTraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mRunLSTMCustomVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mRunLSTMClassVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-504a3398eb92>\u001b[0m in \u001b[0;36mRunLSTMCustomVersion\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         finalizeDL(myLSTMcustommodel,recordtrainloss, recordvalloss,UsedLSTMvalidationfrac,\n\u001b[0;32m--> 196\u001b[0;31m               RawInputSequencesTOT, RawInputPredictionsTOT,0,LabelFit = 'Non-sampled LSTM Fit')\n\u001b[0m\u001b[1;32m    197\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-c246cd93d76c>\u001b[0m in \u001b[0;36mfinalizeDL\u001b[0;34m(ActualModel, recordtrainloss, recordvalloss, validationfrac, X_in, y_in, modelflag, LabelFit)\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0mFitPredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDLprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mActualModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelFit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelFit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdebugfips\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mListofTestFIPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdebugfips\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-fea828b1d0cd>\u001b[0m in \u001b[0;36mDLprediction\u001b[0;34m(Xin, yin, DLmodel, modelflag, LabelFit)\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0mtotalcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mmse1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0myyhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m           \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mNpredperseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mfloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}