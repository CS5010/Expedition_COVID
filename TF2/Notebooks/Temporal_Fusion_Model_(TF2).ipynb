{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCsw1KjFX1PP"
      },
      "source": [
        "# Introduction\n",
        "This is just a notebook version of the TFT model script written in TF2 folder. This works with both the old and new dataset. If you are switching dataset, be sure to change both `TFTdfCurrent.csv` and `config.json` files. Also remove any files from the `checkpoint` folder for a fresh start.\n",
        "\n",
        "It is modified to run on colab. If you want to run on your local machine, you can use the model scripts in the TF2 folder instead.\n",
        "\n",
        "Run the model on GPU or reduce `epochs` in `config.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvhjFLeL6GwE"
      },
      "source": [
        "# Initial Setup\n",
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq2WaGb23m4h",
        "outputId": "93b84f48-52ba-4760-d59d-0bcc15d4e229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Progbar\n",
        "import os, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.training.checkpoint_management import CheckpointManager\n",
        "\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect input data\n",
        "\n",
        "### For new dataset\n",
        "You can collect `TFTdfCurrent.csv` and `config.json` for new dataset in in the `dataset_new` folder. \n",
        "\n",
        "### For the old dataset \n",
        "Collect `TFTdfCurrent.csv` and `config.json` in the `TF2/TFTTF2_ModelDev/data` folder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO8CwyyG6C5L"
      },
      "source": [
        "## Set up drive\n",
        "Create a `TF2` folder in your drive and setup the link here. The `TF2` has the following structure\n",
        "\n",
        "TF2\n",
        "* checkpoints\n",
        "* data\n",
        "  * TFTdfCurrent.csv\n",
        "* config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emPIKC5x40L8"
      },
      "outputs": [],
      "source": [
        "# read in science data \n",
        "COLABROOTDIR=\"/content/drive/My Drive/TF2\"\n",
        "os.environ[\"COLABROOTDIR\"] = COLABROOTDIR\n",
        "\n",
        "# Set up Checkpoints\n",
        "checkPointPath = COLABROOTDIR + \"/checkpoints/\"\n",
        "dataPath = COLABROOTDIR + \"/data/TFTdfCurrent.csv\"\n",
        "configPath = COLABROOTDIR+'/config.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77iIQhnG6b2y"
      },
      "source": [
        "# TFT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glYJdrUr6fAq"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"Defines scaled dot product attention layer.\n",
        "\n",
        "    Attributes:\n",
        "    dropout: Dropout rate to use\n",
        "    activation: Normalisation function for scaled dot product attention (e.g.\n",
        "      softmax by default)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attn_dropout=0.0):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = tf.keras.layers.Dropout(attn_dropout)\n",
        "        self.activation = tf.keras.layers.Activation('softmax')\n",
        "\n",
        "    def call(self, q, k, v, mask):\n",
        "        \"\"\"Applies scaled dot product attention.\n",
        "\n",
        "        Args:\n",
        "          q: Queries\n",
        "          k: Keys\n",
        "          v: Values\n",
        "          mask: Masking if required -- sets softmax to very large value\n",
        "\n",
        "        Returns:\n",
        "          Tuple of (layer outputs, attention weights)\n",
        "        \"\"\"\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        temper = tf.sqrt(tf.cast(tf.shape(k)[-1], dtype='float32'))\n",
        "        attn = matmul_qk / temper\n",
        "\n",
        "        if mask is not None:\n",
        "            mmask = mask * -1e9  # setting to infinity\n",
        "            attn = tf.keras.layers.Add()([attn, mmask])\n",
        "\n",
        "        attn = self.activation(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = tf.matmul(attn, v)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "\n",
        "class InterpretableMultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, n_head, d_model, dropout):\n",
        "        \"\"\"Initialises layer.\n",
        "\n",
        "        Args:\n",
        "        n_head: Number of heads\n",
        "        d_model: TFT state dimensionality\n",
        "        dropout: Dropout discard rate\n",
        "        \"\"\"\n",
        "        super(InterpretableMultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = self.d_v = d_k = d_v = d_model // n_head\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Use same value layer to facilitate interp\n",
        "        vs_layer = tf.keras.layers.Dense(d_v, use_bias=False)\n",
        "\n",
        "        self.qs_layers = [tf.keras.layers.Dense(d_k, use_bias=False) for _ in range(n_head)]\n",
        "        self.ks_layers = [tf.keras.layers.Dense(d_k, use_bias=False) for _ in range(n_head)]\n",
        "        self.vs_layers = [vs_layer for _ in range(n_head)]\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "        self.w_o = tf.keras.layers.Dense(d_model, use_bias=False)\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        n_head = self.n_head\n",
        "\n",
        "        heads = tf.TensorArray(tf.float32, n_head)\n",
        "        attns = tf.TensorArray(tf.float32, n_head)\n",
        "\n",
        "        for i in range(self.n_head):\n",
        "            qs = self.qs_layers[i](q)\n",
        "            ks = self.ks_layers[i](q)\n",
        "            vs = self.vs_layers[i](q)\n",
        "            head, attn = self.attention(qs, ks, vs, mask)\n",
        "\n",
        "            head_dropout = tf.keras.layers.Dropout(self.dropout)(head)\n",
        "            heads = heads.write(i, head_dropout)\n",
        "            attns = attns.write(i, attn)\n",
        "\n",
        "        head = heads.stack()\n",
        "        attn = attns.stack()\n",
        "\n",
        "        outputs = tf.math.reduce_mean(head, axis=0) if n_head > 1 else head\n",
        "        outputs = self.w_o(outputs)\n",
        "        outputs = tf.keras.layers.Dropout(self.dropout)(outputs)  # output dropout\n",
        "\n",
        "        return outputs, attn\n",
        "\n",
        "\n",
        "class HiD_EmbeddingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, time_steps, known_reg_inputs,\n",
        "                 future_inputs, static_inputs, target_loc, unknown_len, hls=64, cat_inputs=None):\n",
        "        super(HiD_EmbeddingLayer, self).__init__()\n",
        "\n",
        "        self.time_steps = time_steps\n",
        "        self.known_locs = known_reg_inputs\n",
        "        self.future_locs = future_inputs\n",
        "        self.static_locs = static_inputs\n",
        "        self.unknown_length = unknown_len\n",
        "\n",
        "        self.target_loc = target_loc\n",
        "\n",
        "        if cat_inputs:\n",
        "            self.cat = cat_inputs\n",
        "\n",
        "        self.hls = hls\n",
        "\n",
        "        self.sd = [tf.keras.layers.Dense(hls) for s in range(len(static_inputs))]\n",
        "\n",
        "        self.real_conversion_unknown = [tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(hls)) \\\n",
        "                                        for i in range(unknown_len)]\n",
        "\n",
        "        self.real_conversion_known = [tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(hls)) \\\n",
        "                                      for i in range(len(known_reg_inputs))]\n",
        "\n",
        "        self.real_conversion_target = [tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(hls)) \\\n",
        "                                       for i in range(len(target_loc))]\n",
        "\n",
        "    def call(self, known_inputs, unknown_inputs, static_inputs):\n",
        "        \"\"\"\n",
        "        Not set up for categorical inputs currently - therefore regular inputs = inputs\n",
        "\n",
        "        First trial we will ignore the targets\n",
        "        \"\"\"\n",
        "\n",
        "        static_inputs = tf.stack([self.sd[sdx](s) for sdx, s in enumerate(static_inputs)], axis=1)\n",
        "\n",
        "        unknown_inputs = tf.stack([self.real_conversion_unknown[udx](u) \\\n",
        "                                   for udx, u in enumerate(unknown_inputs)], axis=-1)\n",
        "\n",
        "        known_inputs = tf.stack([self.real_conversion_known[kdx](k) for kdx, k in enumerate(known_inputs)], axis=-1)\n",
        "\n",
        "        return known_inputs, unknown_inputs, static_inputs  # , target_inputs\n",
        "\n",
        "\n",
        "class LinearLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, size, use_time_distributed, use_bias=True, activation=None):\n",
        "        super(LinearLayer, self).__init__()\n",
        "\n",
        "        self.size = size\n",
        "        self.use_time_distributed = use_time_distributed\n",
        "        self.use_bias = use_bias\n",
        "        self.activation = activation\n",
        "\n",
        "        self.out_linear = tf.keras.layers.Dense(size, activation, use_bias)\n",
        "        if use_time_distributed:\n",
        "            self.out_linear = tf.keras.layers.TimeDistributed(self.out_linear)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = self.out_linear(inputs)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GLU(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, hls=64, use_time_distributed=True, dropout_rate=None, activation=None):\n",
        "\n",
        "        super(GLU, self).__init__()\n",
        "\n",
        "        self.hls = hls\n",
        "        self.use_time_distributed = use_time_distributed\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.activation = activation\n",
        "\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "        if use_time_distributed:\n",
        "            self.activation_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(hls, activation))\n",
        "            self.gate_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(hls, activation='sigmoid'))\n",
        "        else:\n",
        "            self.activation_layer = tf.keras.layers.Dense(hls, activation)\n",
        "            self.gate_layer = tf.keras.layers.Dense(hls, activation='sigmoid')\n",
        "\n",
        "        self.return_layer = tf.keras.layers.Multiply()\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        if self.dropout_rate:\n",
        "            inputs = self.dropout_layer(inputs)\n",
        "\n",
        "        activation_layer = self.activation_layer(inputs)\n",
        "\n",
        "        gate_layer = self.gate_layer(inputs)\n",
        "\n",
        "        return self.return_layer([activation_layer, gate_layer]), gate_layer\n",
        "\n",
        "\n",
        "class GatedResidualNetwork(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    This class is only used for GRN's without added context. For layers which use context see TemporalGatedResidualNetwork\n",
        "    '''\n",
        "\n",
        "    def __init__(self, hls=64, output_size=None,\n",
        "                 dropout_rate=None, use_time_distributed=True,\n",
        "                 return_gate=False, altered_glu=False):\n",
        "\n",
        "        super(GatedResidualNetwork, self).__init__()\n",
        "        self.hls = hls\n",
        "        self.use_time_distributed = use_time_distributed\n",
        "        self.return_gate = return_gate\n",
        "\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.additional_context = None\n",
        "\n",
        "        self.linear = LinearLayer(hls, activation=None,\n",
        "                                  use_time_distributed=self.use_time_distributed)\n",
        "\n",
        "        self.linear_hidden = LinearLayer(hls, activation=None,\n",
        "                                         use_time_distributed=self.use_time_distributed)\n",
        "        self.linear_additional_context = LinearLayer(hls, activation=None,\n",
        "                                                     use_time_distributed=use_time_distributed,\n",
        "                                                     use_bias=False)\n",
        "\n",
        "        if output_size:\n",
        "            self.out_linear = tf.keras.layers.Dense(output_size)\n",
        "            if use_time_distributed:\n",
        "                self.out_linear = tf.keras.layers.TimeDistributed(self.out_linear)\n",
        "\n",
        "        if altered_glu:\n",
        "            self.gate_layer = GLU(output_size, dropout_rate=self.dropout_rate,\n",
        "                                  use_time_distributed=self.use_time_distributed, activation=None)\n",
        "        else:\n",
        "            self.gate_layer = GLU(hls, dropout_rate=self.dropout_rate,\n",
        "                                  use_time_distributed=self.use_time_distributed, activation=None)\n",
        "\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.hidden_activation = tf.keras.layers.Activation('elu')\n",
        "\n",
        "        self.add_layer1 = tf.keras.layers.Add()\n",
        "        self.add_layer2 = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        if self.output_size is None:\n",
        "            output_size = self.hls\n",
        "            skip = inputs\n",
        "        else:\n",
        "            skip = self.out_linear(inputs)\n",
        "\n",
        "        hidden0 = self.linear(inputs)\n",
        "\n",
        "        hidden1 = self.hidden_activation(hidden0)\n",
        "        hidden2 = self.linear_hidden(hidden1)\n",
        "\n",
        "        gating_layer, gate = self.gate_layer(hidden2)\n",
        "\n",
        "        tmp = self.add_layer2([skip, gating_layer])\n",
        "        ann = self.norm(tmp)\n",
        "\n",
        "        if self.return_gate:\n",
        "            return ann, gate\n",
        "        else:\n",
        "            return ann\n",
        "\n",
        "\n",
        "class TemporalGatedResidualNetwork(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, hls=64, output_size=None,\n",
        "                 dropout_rate=None, use_time_distributed=True,\n",
        "                 return_gate=False, altered_glu=False):\n",
        "\n",
        "        super(TemporalGatedResidualNetwork, self).__init__()\n",
        "        self.hls = hls\n",
        "        self.use_time_distributed = use_time_distributed\n",
        "        self.return_gate = return_gate\n",
        "\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.additional_context = None\n",
        "\n",
        "        self.linear = LinearLayer(hls, activation=None,\n",
        "                                  use_time_distributed=self.use_time_distributed)\n",
        "\n",
        "        self.linear_hidden = LinearLayer(hls, activation=None,\n",
        "                                         use_time_distributed=self.use_time_distributed)\n",
        "        self.linear_additional_context = LinearLayer(hls, activation=None,\n",
        "                                                     use_time_distributed=use_time_distributed,\n",
        "                                                     use_bias=False)\n",
        "\n",
        "        if output_size:\n",
        "            self.out_linear = tf.keras.layers.Dense(output_size)\n",
        "            if use_time_distributed:\n",
        "                self.out_linear = tf.keras.layers.TimeDistributed(self.out_linear)\n",
        "\n",
        "        if altered_glu:\n",
        "            self.gate_layer = GLU(output_size, dropout_rate=self.dropout_rate,\n",
        "                                  use_time_distributed=self.use_time_distributed, activation=None)\n",
        "        else:\n",
        "            self.gate_layer = GLU(hls, dropout_rate=self.dropout_rate,\n",
        "                                  use_time_distributed=self.use_time_distributed, activation=None)\n",
        "\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.hidden_activation = tf.keras.layers.Activation('elu')\n",
        "\n",
        "        self.add_layer1 = tf.keras.layers.Add()\n",
        "        self.add_layer2 = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, inputs, context=None):\n",
        "\n",
        "        if self.output_size is None:\n",
        "            output_size = self.hls\n",
        "            skip = inputs\n",
        "        else:\n",
        "            skip = self.out_linear(inputs)\n",
        "\n",
        "        hidden0 = self.linear(inputs)\n",
        "\n",
        "        intermediate = self.linear_additional_context(context)\n",
        "        hidden1 = self.add_layer1([hidden0, intermediate])\n",
        "\n",
        "        hidden2 = self.hidden_activation(hidden1)\n",
        "        hidden3 = self.linear_hidden(hidden2)\n",
        "\n",
        "        gating_layer, gate = self.gate_layer(hidden3)\n",
        "\n",
        "        tmp = self.add_layer2([skip, gating_layer])\n",
        "        ann = self.norm(tmp)\n",
        "\n",
        "        if self.return_gate:\n",
        "            return ann, gate\n",
        "        else:\n",
        "            return ann\n",
        "\n",
        "\n",
        "class StaticVSN(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, hls=64, dropout_rate=.1, num_static=6):\n",
        "\n",
        "        super(StaticVSN, self).__init__()\n",
        "        self.hls = hls\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_static = num_static\n",
        "\n",
        "        self.grn_vsn0 = None\n",
        "\n",
        "        self.sparse_activation = tf.keras.layers.Activation('softmax')\n",
        "\n",
        "        self.es = [GatedResidualNetwork(hls=hls, dropout_rate=dropout_rate, use_time_distributed=False) for i in\n",
        "                   range(num_static)]\n",
        "\n",
        "        self.multiply_layer = tf.keras.layers.Multiply()\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        embedding = inputs\n",
        "\n",
        "        _, num_static, _ = embedding.get_shape().as_list()\n",
        "        flatten = tf.keras.layers.Flatten()(embedding)\n",
        "\n",
        "        if self.grn_vsn0 is None:\n",
        "            self.grn_vsn0 = GatedResidualNetwork(hls=self.hls, output_size=num_static,\n",
        "                                                 dropout_rate=self.dropout_rate,\n",
        "                                                 use_time_distributed=False,\n",
        "                                                 altered_glu=True)\n",
        "\n",
        "        mlp_outputs = self.grn_vsn0(flatten)\n",
        "\n",
        "        sparse_weights = self.sparse_activation(mlp_outputs)\n",
        "        sparse_weights = tf.expand_dims(sparse_weights, axis=-1)\n",
        "\n",
        "        trans_emb_list = tf.TensorArray(tf.float32, size=num_static)\n",
        "        for i in range(self.num_static):\n",
        "            tmp = self.es[i](embedding[:, i:i + 1, :])\n",
        "            trans_emb_list = trans_emb_list.write(i, tmp)\n",
        "\n",
        "        transformed_embedding = trans_emb_list.stack()\n",
        "        transformed_embedding = tf.transpose(transformed_embedding, perm=[2, 1, 0, 3])\n",
        "        transformed_embedding = tf.squeeze(transformed_embedding, axis=0)\n",
        "        combined = self.multiply_layer([sparse_weights, transformed_embedding])\n",
        "\n",
        "        static_vec = tf.math.reduce_sum(combined, axis=1)\n",
        "\n",
        "        return static_vec, sparse_weights\n",
        "\n",
        "\n",
        "class TemporalVSN(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, hls, dropout_rate=.1, num_inputs=None):\n",
        "        super(TemporalVSN, self).__init__()\n",
        "        self.hls = hls\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_inputs = num_inputs\n",
        "\n",
        "        self.grn0 = TemporalGatedResidualNetwork(self.hls, output_size=self.num_inputs,\n",
        "                                                 dropout_rate=self.dropout_rate,\n",
        "                                                 use_time_distributed=True,\n",
        "                                                 return_gate=True, altered_glu=True)\n",
        "\n",
        "        self.sparse_activation_layer = tf.keras.layers.Activation('softmax')\n",
        "\n",
        "        self.et = [GatedResidualNetwork(hls=hls, dropout_rate=dropout_rate, use_time_distributed=True) \\\n",
        "                   for i in range(num_inputs)]\n",
        "\n",
        "        self.mult_layer = tf.keras.layers.Multiply()\n",
        "\n",
        "        self.scv = None\n",
        "\n",
        "    def call(self, inputs, context):\n",
        "        _, time_steps, embedding_dim, num_inputs = inputs.get_shape().as_list()\n",
        "\n",
        "        flatten = tf.reshape(inputs, [-1, time_steps, embedding_dim * num_inputs])\n",
        "\n",
        "        mlp_outputs, static_gate = self.grn0(inputs=flatten, context=context)  # self.scv)\n",
        "\n",
        "        sparse_weights = self.sparse_activation_layer(mlp_outputs)\n",
        "        sparse_weights = tf.expand_dims(sparse_weights, axis=2)\n",
        "\n",
        "        trans_emb_list = tf.TensorArray(tf.float32, size=num_inputs)\n",
        "\n",
        "        for i in range(num_inputs):\n",
        "            grn_output = self.et[i](inputs[Ellipsis, i])\n",
        "            trans_emb_list = trans_emb_list.write(i, grn_output)\n",
        "\n",
        "        transformed_embedding = trans_emb_list.stack()\n",
        "        transformed_embedding = tf.transpose(transformed_embedding, perm=[1, 2, 3, 0])\n",
        "\n",
        "        combined = self.mult_layer([sparse_weights, transformed_embedding])\n",
        "        temporal_ctx = tf.math.reduce_sum(combined, axis=-1)\n",
        "\n",
        "        return temporal_ctx, sparse_weights, static_gate\n",
        "\n",
        "\n",
        "class MLP(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, hls=64, output_size=None, output_activation=None,\n",
        "                 hidden_activation='tanh', use_time_distributed=False):\n",
        "\n",
        "        super(MLP, self).__init__()\n",
        "        self.hls = hls\n",
        "        self.output_size = output_size\n",
        "        self.output_activation = output_activation\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.use_time_distributed = use_time_distributed\n",
        "\n",
        "        if use_time_distributed:\n",
        "            self.hidden1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.hls,\n",
        "                                                                                 activation=hidden_activation))\n",
        "            self.hidden2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.output_size,\n",
        "                                                                                 activation=self.output_activation))\n",
        "        else:\n",
        "            self.hidden1 = tf.keras.layers.Dense(self.hls, activation=hidden_activation)\n",
        "            self.hidden2 = tf.keras.layers.Dense(self.output_size, activation=self.output_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        out1 = self.hidden1(inputs)\n",
        "        out2 = self.hidden2(out1)\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "class TemporalFusionTransformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_heads, input_seq_len, output_size,target_seq_len,\n",
        "                 known_reg_inputs, future_inputs, static_inputs, target_inputs,\n",
        "                 attn_hls=64, final_mlp_hls=128, unknown_inputs=7,\n",
        "                 hls=64, cat_inputs=None,\n",
        "                 rate=.2):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.hls = hls\n",
        "        self.dropout_rate = rate\n",
        "        self.input_seq_len = input_seq_len\n",
        "        self.target_seq_len = target_seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.attn_hls = attn_hls\n",
        "        self.final_mlp_hls = final_mlp_hls\n",
        "\n",
        "        self.static_locs = static_inputs\n",
        "        self.known_locs = known_reg_inputs\n",
        "        self.future_locs = future_inputs\n",
        "        self.target_locs = target_inputs\n",
        "\n",
        "        self.FinalLoopSize = 1\n",
        "\n",
        "        # HiD_EmbeddingLayer should take all the parameters it can from TFT model\n",
        "        self.embedding = HiD_EmbeddingLayer(\n",
        "            time_steps=target_seq_len + input_seq_len, known_reg_inputs=known_reg_inputs, \n",
        "            future_inputs=future_inputs, static_inputs=static_inputs, target_loc=target_inputs,\n",
        "            unknown_len=unknown_inputs, hls=hls, cat_inputs=cat_inputs\n",
        "        )\n",
        "\n",
        "        # add static VSN\n",
        "\n",
        "        self.static_vsn = StaticVSN(hls=hls, dropout_rate=rate, num_static=len(static_inputs))\n",
        "\n",
        "        # add Temporal VSN\n",
        "        self.temporal_vsn1 = TemporalVSN(hls=hls, dropout_rate=rate,\n",
        "                                         num_inputs=len(future_inputs) + unknown_inputs)\n",
        "        self.temporal_vsn2 = TemporalVSN(hls=hls, dropout_rate=rate,\n",
        "                                         num_inputs=len(future_inputs))\n",
        "\n",
        "        self.temporal_layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.enriched_grn = TemporalGatedResidualNetwork(hls=self.hls,\n",
        "                                                         dropout_rate=self.dropout_rate,\n",
        "                                                         use_time_distributed=True,\n",
        "                                                         return_gate=True)\n",
        "\n",
        "        self.mlha = InterpretableMultiHeadAttention(n_head=self.num_heads, d_model=self.attn_hls, dropout=self.dropout_rate)\n",
        "\n",
        "        self.static_grn1 = GatedResidualNetwork(self.hls,\n",
        "                                                dropout_rate=self.dropout_rate,\n",
        "                                                use_time_distributed=False)\n",
        "        self.static_grn2 = GatedResidualNetwork(self.hls,\n",
        "                                                dropout_rate=self.dropout_rate,\n",
        "                                                use_time_distributed=False)\n",
        "        self.static_grn3 = GatedResidualNetwork(self.hls,\n",
        "                                                dropout_rate=self.dropout_rate,\n",
        "                                                use_time_distributed=False)\n",
        "        self.static_grn4 = GatedResidualNetwork(self.hls,\n",
        "                                                dropout_rate=self.dropout_rate,\n",
        "                                                use_time_distributed=False)\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.LSTM(self.hls, return_sequences=True, return_state=True, stateful=False,\n",
        "                                          activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0,\n",
        "                                          unroll=False,\n",
        "                                          use_bias=True)\n",
        "\n",
        "        self.lstm2 = tf.keras.layers.LSTM(self.hls, return_sequences=True, return_state=False, stateful=False,\n",
        "                                          activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0,\n",
        "                                          unroll=False, use_bias=True)\n",
        "\n",
        "        self.lstmGLU = GLU(hls, rate, activation=None)\n",
        "\n",
        "        self.mlp = MLP(self.final_mlp_hls, output_size, output_activation=None, hidden_activation='selu',\n",
        "                       use_time_distributed=True)\n",
        "\n",
        "        self.final_glus = [GLU(self.hls, dropout_rate=self.dropout_rate, activation=None) for i in\n",
        "                           range(self.FinalLoopSize)]\n",
        "        self.final_norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.final_add1 = tf.keras.layers.Add()\n",
        "        self.final_add2 = tf.keras.layers.Add()\n",
        "\n",
        "        self.decoder = GatedResidualNetwork(hls=self.hls, dropout_rate=self.dropout_rate, use_time_distributed=True)\n",
        "        self.decoder_glu = GLU(hls=self.hls, activation=None)\n",
        "        self.final_norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def get_decoder_mask(self, attn_inputs):\n",
        "\n",
        "        len_s = tf.shape(attn_inputs)[1]\n",
        "        bs = tf.shape(attn_inputs)[:1]\n",
        "        mask = tf.math.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
        "        return mask\n",
        "\n",
        "    def process_inputs(self, inputs):\n",
        "\n",
        "        inputs = inputs[0]\n",
        "        times, feature = inputs[0].get_shape().as_list()\n",
        "\n",
        "        static_inputs = [inputs[:, 0, s:s + 1] for s in range(feature) if s in self.static_locs]\n",
        "\n",
        "        unknown_inputs = [inputs[Ellipsis, u:u + 1] for u in range(feature) if u not in self.known_locs]\n",
        "\n",
        "        known_inputs = [inputs[Ellipsis, k:k + 1] for k in self.known_locs if k not in self.static_locs]\n",
        "\n",
        "        return known_inputs, unknown_inputs, static_inputs\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "\n",
        "        known_inputs, unknown_inputs, static_inputs = self.process_inputs(inputs)\n",
        "\n",
        "        known_emb, unknown_emb, static_emb = self.embedding(known_inputs, unknown_inputs, static_inputs)\n",
        "\n",
        "        if unknown_emb is not None:\n",
        "            historical_inputs = tf.concat([unknown_emb[:, :self.input_seq_len, :],\n",
        "                                           known_emb[:, :self.input_seq_len, :]], axis=-1)\n",
        "\n",
        "        future_inputs = known_emb[:, self.input_seq_len:, :]\n",
        "\n",
        "        static_encoder, static_weights = self.static_vsn(static_emb)\n",
        "\n",
        "        scvs = self.static_grn1(static_encoder)\n",
        "        scvs_e = tf.expand_dims(scvs, axis=1)\n",
        "\n",
        "        static_context_enrichment = self.static_grn2(static_encoder)\n",
        "        static_context_state_h = self.static_grn3(static_encoder)\n",
        "        static_context_state_c = self.static_grn4(static_encoder)\n",
        "\n",
        "        # temporal VSN\n",
        "\n",
        "        historical_features, historical_flags, _ = self.temporal_vsn1(historical_inputs, context=scvs_e)\n",
        "        future_features, future_flags, _ = self.temporal_vsn2(future_inputs, context=scvs_e)\n",
        "\n",
        "        history_lstm, state_h, state_c = self.lstm1(historical_features, initial_state=[static_context_state_h,\n",
        "                                                                                        static_context_state_c])\n",
        "\n",
        "        future_lstm = self.lstm2(future_features, initial_state=[state_h, state_c])\n",
        "\n",
        "        lstm_layer = tf.concat([history_lstm, future_lstm], axis=1)\n",
        "\n",
        "        input_embeddings = tf.concat([historical_features, future_features], axis=1)\n",
        "\n",
        "        lstm_layer, _ = self.lstmGLU(lstm_layer)\n",
        "\n",
        "        tmp = tf.keras.layers.add([lstm_layer, input_embeddings])\n",
        "        temporal_feature_layer = self.temporal_layer_norm(tmp)\n",
        "\n",
        "        expanded_static_context = tf.expand_dims(static_context_enrichment, axis=1)\n",
        "\n",
        "        enriched, _ = self.enriched_grn(inputs=temporal_feature_layer, context=expanded_static_context)\n",
        "\n",
        "        mask = self.get_decoder_mask(enriched)\n",
        "\n",
        "        xsve, attn = self.mlha(enriched, enriched, enriched, mask=mask)\n",
        "\n",
        "        if self.FinalLoopSize > 1:\n",
        "            StackLayers = tf.TensorArray(tf.float32, self.FinalLoopSize)\n",
        "        for FinalGatingLoop in range(0, self.FinalLoopSize):\n",
        "            x, _ = self.final_glus[FinalGatingLoop](xsve)\n",
        "            x = self.final_add1([x, enriched])\n",
        "            x = self.final_norm1(x)\n",
        "\n",
        "            decoder = self.decoder(x)\n",
        "\n",
        "            decoder, _ = self.decoder_glu(decoder)\n",
        "\n",
        "            transformer_layer = self.final_add2([decoder, temporal_feature_layer])\n",
        "            transformer_layer = self.final_norm2(transformer_layer)\n",
        "\n",
        "        outputs = self.mlp(transformer_layer[Ellipsis, self.input_seq_len:, :])\n",
        "\n",
        "        attention_weights = {'decoder_self_attn': attn,\n",
        "                             'static_flags': static_weights[Ellipsis, 0],\n",
        "                             'historical_flags': historical_flags[Ellipsis, 0, :],\n",
        "                             'future_flags': future_flags[Ellipsis, 0, :]}\n",
        "\n",
        "        return outputs, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY2OVkjn6tQp"
      },
      "source": [
        "# Parameter manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf2BGFHR6vwr"
      },
      "outputs": [],
      "source": [
        "class ParameterManager:\n",
        "\n",
        "    def __init__(self, config_path):\n",
        "        f = open(config_path)\n",
        "        self.param_dict = json.load(f)\n",
        "\n",
        "        try:\n",
        "            self.tft_params = self.param_dict['TFTparams']\n",
        "        except:\n",
        "            raise ValueError('There are no TFT params key in the config file')\n",
        "\n",
        "        if self.tft_params:\n",
        "            self.attn_params = self.tft_params['attn']\n",
        "            self.optimizer_params = self.tft_params['optimizer']\n",
        "            self.col_mappings = self.tft_params['col_mappings']\n",
        "            self.data_params = self.tft_params['data']['params']\n",
        "            self.support_params = self.tft_params['data']['support']\n",
        "\n",
        "    def print_params(self):\n",
        "\n",
        "        print('TFT Regular Parameters\\nAll loc parameters below indicate matrix location (column) in dataframe')\n",
        "        for i in self.tft_params:\n",
        "            if i != 'attn':\n",
        "                print(i + ': ' + str(self.tft_params[i]))\n",
        "        print('\\nTFT Attention Parameters')\n",
        "        for i in self.attn_params:\n",
        "            print(i + ': ' + str(self.attn_params[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWKexwe97GOj"
      },
      "source": [
        "# Data manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgsgOBko7IyS"
      },
      "outputs": [],
      "source": [
        "class DataManager:\n",
        "\n",
        "    def __init__(self, data_path, total_seq_len, col_mappings, data_params):\n",
        "\n",
        "        self.training = pd.read_csv(data_path)\n",
        "        \n",
        "        # Temporary overwrite\n",
        "        # self.training = self.training.dropna()\n",
        "        \n",
        "        if self.training.isna().sum().any():\n",
        "            raise ValueError('Null values found in your training dataset')\n",
        "\n",
        "        self.tseq_len = total_seq_len\n",
        "        self.col_mappings = col_mappings\n",
        "\n",
        "        self.batch_size = data_params['batch_size']\n",
        "        self.buffer_size = data_params['buffer_size']\n",
        "\n",
        "        self.num_samples = None\n",
        "        self.inference_data = None\n",
        "        self.training_data = None\n",
        "        self.np_inference = None\n",
        "\n",
        "    def batch_data(self, data):\n",
        "\n",
        "        if self.training is None:\n",
        "            return None\n",
        "\n",
        "        def _batch_single_entity(input_data, tseq):\n",
        "            time_steps = len(input_data)\n",
        "            lags = tseq\n",
        "            x = input_data.values\n",
        "            if time_steps >= lags:\n",
        "                return np.stack([x[i:time_steps - (lags - 1) + i, :] for i in range(lags)], axis=1)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        id_col = self.col_mappings['ID']\n",
        "        time_col = self.col_mappings['Time']\n",
        "        target_col = self.col_mappings['Target']\n",
        "        input_cols = self.col_mappings['Known Regular'] + self.col_mappings['Future']\n",
        "\n",
        "        data_map = {}\n",
        "        for _, sliced in data.groupby(id_col):\n",
        "\n",
        "            for k in self.col_mappings:\n",
        "                cols = self.col_mappings[k]\n",
        "                arr = _batch_single_entity(sliced[cols].copy(), self.tseq_len)\n",
        "\n",
        "                if k not in data_map:\n",
        "                    data_map[k] = [arr]\n",
        "                else:\n",
        "                    data_map[k].append(arr)\n",
        "\n",
        "        for k in data_map:\n",
        "            data_map[k] = np.concatenate(data_map[k], axis=0)\n",
        "\n",
        "        data_map['TargetAsInput'] = data_map['Target']\n",
        "        data_map['Target'] = data_map['Target'][:, 13:, :]\n",
        "\n",
        "        active_entries = np.ones_like(data_map['Target'])\n",
        "        if 'active_entries' not in data_map:\n",
        "            data_map['active_entries'] = active_entries\n",
        "        else:\n",
        "            data_map['active_entries'].append(active_entries)\n",
        "\n",
        "        return data_map\n",
        "\n",
        "    def createTFData(self):\n",
        "\n",
        "        batched_data = self.batch_data(self.training)\n",
        "\n",
        "        # TODO fix this line below\n",
        "        self.num_samples = batched_data['Future'].shape[0]\n",
        "\n",
        "        all_inputs = np.concatenate(\n",
        "            (batched_data['Known Regular'], batched_data['Future'], batched_data['TargetAsInput']), axis=2)\n",
        "        tf_data = tf.data.Dataset.from_tensor_slices((all_inputs, batched_data['Target']))\n",
        "\n",
        "        def make_inference_batches(ds):\n",
        "            return ds.cache().batch(self.batch_size)\n",
        "\n",
        "        def make_train_batches(ds):\n",
        "            return ds.cache().shuffle(self.buffer_size).batch(self.batch_size)\n",
        "\n",
        "        self.inference_data = make_inference_batches(tf_data)\n",
        "        self.np_inference = batched_data\n",
        "        self.training_data = make_train_batches(tf_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5slqmnm8ITA"
      },
      "source": [
        "# Training\n",
        "## Ready Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4KJtGpk8MMA"
      },
      "outputs": [],
      "source": [
        "parameterManager = ParameterManager(configPath)\n",
        "\n",
        "tft_params = parameterManager.tft_params\n",
        "attn_params = parameterManager.attn_params\n",
        "optimizer_params = parameterManager.optimizer_params\n",
        "col_mappings = parameterManager.col_mappings\n",
        "data_params = parameterManager.data_params\n",
        "\n",
        "tseq_length = tft_params['input_sequence_length'] + tft_params['target_sequence_length']\n",
        "\n",
        "unk_inputs = tft_params['total_inputs'] - len(tft_params['static_locs']) - len(tft_params['future_locs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghPPr2s8xL_4",
        "outputId": "44513268-704f-431f-edb6-e8249d690397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Static': ['Total Population', 'Population Density', 'Population 55+', '% Fair or Poor Health', '% Adults with Obesity', '% Flu Vaccinated'], 'ID': ['County'], 'Time': ['TimeFromStart'], 'Target': ['Cases'], 'Future': ['LinearSpace', 'Constant', 'LinearTime', 'P2Time', 'P3Time', 'P4Time', 'CosWeekly', 'SinWeekly'], 'Known Regular': ['Total Population', 'Population Density', 'Population 55+', '% Fair or Poor Health', '% Adults with Obesity', '% Flu Vaccinated', 'workplaces_percent_change_from_baseline', 'Administered_Dose1_Recip', 'Series_Complete_Yes', 'Testing']}\n"
          ]
        }
      ],
      "source": [
        "print(col_mappings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj1JEIct8tTo"
      },
      "source": [
        "## Create TFT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3sp58NJ8noo"
      },
      "outputs": [],
      "source": [
        "transformer = TemporalFusionTransformer(input_seq_len=tft_params['input_sequence_length'],\n",
        "      target_seq_len=tft_params['target_sequence_length'],\n",
        "      output_size=tft_params['output_size'],\n",
        "      static_inputs=tft_params['static_locs'],\n",
        "      target_inputs=tft_params['target_loc'],\n",
        "      future_inputs=tft_params['future_locs'],\n",
        "      known_reg_inputs=tft_params['static_locs'] + tft_params['future_locs'],\n",
        "      attn_hls=attn_params['hidden_layer_size'],\n",
        "      num_heads=attn_params['num_heads'],\n",
        "      final_mlp_hls=tft_params['final_mlp_hidden_layer'],\n",
        "      unknown_inputs=unk_inputs,\n",
        "      cat_inputs=tft_params['categorical_loc'], rate=tft_params['dropout_rate']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuTeedmR9Cof"
      },
      "source": [
        "## Set loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYtcqJjk9AZ_"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "if tft_params['loss'].upper() == 'MSE':\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "else:\n",
        "    print('No other losses defined in main method')\n",
        "\n",
        "if optimizer_params['optimizer'].lower() == 'adam':\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=optimizer_params['learning_rate'], clipnorm=optimizer_params['clipnorm'])\n",
        "else:\n",
        "    print('No other optimizers defined in main method')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C2DvX3e93xW"
      },
      "source": [
        "## Data manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIj9FGzO96bW",
        "outputId": "1ed44802-7beb-45b3-c7ea-320df93517b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples 47845 and batches 748\n"
          ]
        }
      ],
      "source": [
        "dataManager = DataManager(dataPath, tseq_length, col_mappings, data_params)\n",
        "dataManager.createTFData()\n",
        "\n",
        "train_batches = dataManager.training_data\n",
        "num_training_samples = dataManager.num_samples\n",
        "batch_size = data_params['batch_size']\n",
        "metrics_names = ['train_loss']\n",
        "print(f'Training samples {num_training_samples} and batches {len(train_batches)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwv8m9Cf-Ga-"
      },
      "source": [
        "## Run epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTl-Eozv-KVQ"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer([inp, tar], training=True)\n",
        "\n",
        "        loss = loss_object(tar, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1dEmuBR9NFn",
        "outputId": "4a2e05d0-e746-4bbd-9ce8-8dec10e825b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running the model for 10 epochs\n",
            "47845/47845 [==============================] - 300s 6ms/step - train_loss: 0.0173\n",
            "\n",
            "Epoch 1 Loss 0.0173\n",
            "Time taken for 1 epoch: 300.06 secs\n",
            "\n",
            "47845/47845 [==============================] - 243s 5ms/step - train_loss: 0.0031\n",
            "\n",
            "Epoch 2 Loss 0.0031\n",
            "Time taken for 1 epoch: 261.96 secs\n",
            "\n",
            "47845/47845 [==============================] - 247s 5ms/step - train_loss: 0.0025\n",
            "\n",
            "Epoch 3 Loss 0.0025\n",
            "Time taken for 1 epoch: 246.83 secs\n",
            "\n",
            "47845/47845 [==============================] - 244s 5ms/step - train_loss: 0.0026\n",
            "\n",
            "Epoch 4 Loss 0.0026\n",
            "Time taken for 1 epoch: 262.26 secs\n",
            "\n",
            "47845/47845 [==============================] - 246s 5ms/step - train_loss: 0.0021\n",
            "\n",
            "Saving checkpoint for epoch 5 at /content/drive/My Drive/TF2/checkpoints/ckpt-1\n",
            "\n",
            "Epoch 5 Loss 0.0021\n",
            "Time taken for 1 epoch: 263.49 secs\n",
            "\n",
            "47845/47845 [==============================] - 243s 5ms/step - train_loss: 0.0019\n",
            "\n",
            "Epoch 6 Loss 0.0019\n",
            "Time taken for 1 epoch: 261.98 secs\n",
            "\n",
            "47845/47845 [==============================] - 243s 5ms/step - train_loss: 0.0018\n",
            "\n",
            "Epoch 7 Loss 0.0018\n",
            "Time taken for 1 epoch: 261.98 secs\n",
            "\n",
            "47845/47845 [==============================] - 243s 5ms/step - train_loss: 0.0017\n",
            "\n",
            "Epoch 8 Loss 0.0017\n",
            "Time taken for 1 epoch: 261.98 secs\n",
            "\n",
            "47845/47845 [==============================] - 245s 5ms/step - train_loss: 0.0017\n",
            "\n",
            "Epoch 9 Loss 0.0017\n",
            "Time taken for 1 epoch: 244.85 secs\n",
            "\n",
            "47845/47845 [==============================] - 243s 5ms/step - train_loss: 0.0016\n",
            "\n",
            "Saving checkpoint for epoch 10 at /content/drive/My Drive/TF2/checkpoints/ckpt-2\n",
            "\n",
            "Epoch 10 Loss 0.0016\n",
            "Time taken for 1 epoch: 243.92 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "checkpoint = tf.train.Checkpoint(model=transformer, optimizer=optimizer)\n",
        "checkpointManager = tf.train.CheckpointManager(checkpoint, checkPointPath, max_to_keep=5)\n",
        "\n",
        "if checkpointManager.latest_checkpoint:\n",
        "    checkpoint.restore(checkpointManager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored: ' + checkpointManager.latest_checkpoint)\n",
        "\n",
        "EPOCHS = tft_params['epochs']\n",
        "print(f'Running the model for {EPOCHS} epochs')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    pb_i = Progbar(num_training_samples, stateful_metrics=metrics_names)\n",
        "    p_counter = 0\n",
        "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "        train_step(inp, tar)\n",
        "        values = [('train_loss', train_loss.result())]\n",
        "        p_counter += inp.shape[0]\n",
        "        pb_i.update(p_counter, values=values)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "        print(\n",
        "            f'\\nEpoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = checkpointManager.save()\n",
        "        print(f'\\nSaving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
        "\n",
        "    print(f'\\nEpoch {epoch + 1} Loss {train_loss.result():.4f}')\n",
        "\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "77iIQhnG6b2y",
        "IY2OVkjn6tQp"
      ],
      "name": "Temporal Fusion Model (TF2).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
