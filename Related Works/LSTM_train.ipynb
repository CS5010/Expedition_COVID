{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LopEAYzbELVA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j724k07rELVB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# disable chained assignments\n",
        "pd.options.mode.chained_assignment = None \n",
        "from pandas import DataFrame, to_timedelta, concat\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras import optimizers, Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "SEED = 7\n",
        "tf.random.set_seed(SEED)\n",
        "SHOW_IMAGE = True\n",
        "VERBOSE = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bGSe5AhJXgd"
      },
      "outputs": [],
      "source": [
        "# https://matplotlib.org/stable/tutorials/introductory/customizing.html#the-default-matplotlibrc-file\n",
        "SMALL_SIZE = 20\n",
        "MEDIUM_SIZE = 24\n",
        "BIGGER_SIZE = 32\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "plt.rc('axes', titlepad=15)\n",
        "\n",
        "# set tick width\n",
        "plt.rcParams['xtick.major.size'] = 15 # default 3.5\n",
        "plt.rcParams['xtick.major.width'] = 2 # default 0.8 \n",
        "\n",
        "plt.rcParams['ytick.major.size'] = 15 # default 3.5\n",
        "plt.rcParams['ytick.major.width'] = 2 # 0.8 \n",
        "\n",
        "plt.rcParams['lines.linewidth'] = 2.5\n",
        "\n",
        "DPI = 200\n",
        "FIGSIZE = (12, 7)\n",
        "DATE_TICKS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CBFPwG4EPNI"
      },
      "source": [
        "## Google Colab\n",
        "If running on colab, make sure to uncomment the following. Also, upload the merged csv file in drive and modify the input file path accordingly in the preprocessing section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfN5rUZRERW2",
        "outputId": "ee565b43-c1ff-458a-c785-0c8e8ca2c047"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/Colab Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Result folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_folder = 'results'\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "@dataclass\n",
        "class Split:\n",
        "    train_start = pd.to_datetime(\"2020-02-29\")\n",
        "    validation_start = pd.to_datetime(\"2021-11-30\")\n",
        "    test_start = pd.to_datetime(\"2021-12-15\")\n",
        "    test_end = pd.to_datetime(\"2021-12-29\")\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    features = [\n",
        "    'AgeDist', 'HealthDisp', \n",
        "    'DiseaseSpread', 'Transmission', 'VaccinationFull', 'SocialDist', \n",
        "    'SinWeekly', 'CosWeekly', 'TimeFromStart'\n",
        "    ]  # note that TimeFromStart is an index feature commonly used by all timeseries models\n",
        "    targets = ['Cases']\n",
        "    group_id = 'FIPS'\n",
        "    selected_columns = features + targets\n",
        "    input_sequence_length = 13\n",
        "    output_sequence_length = 15\n",
        "    batch_size = 128\n",
        "    buffer_size = 1000\n",
        "    epochs = 200\n",
        "    learning_rate = 1e-5\n",
        "    early_stopping_patience = 5\n",
        "    loss = 'mse'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets = Config.targets\n",
        "group_id = Config.group_id\n",
        "input_sequence_length = Config.output_sequence_length\n",
        "output_sequence_length = Config.output_sequence_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlXwxlMELVB"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "s4ZnvkkJELVC",
        "outputId": "b7d0c71d-75d6-4308-ee80-89e029d2f68d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../TFT-pytorch/2022_May_cleaned/Top_100.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gy6DE69ELVD"
      },
      "source": [
        "## Split and scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vcbvD7ZELVD"
      },
      "outputs": [],
      "source": [
        "def split_data(\n",
        "    df:DataFrame, split:dataclass, input_sequence_length:int\n",
        "):\n",
        "    train_df = df[(df['Date']>=split.train_start) & (df['Date']<split.validation_start)]\n",
        "\n",
        "    validation_start = max(split.validation_start - to_timedelta(input_sequence_length, unit='day'), df['Date'].min())\n",
        "    val_df = df[(df['Date']>=validation_start) & (df['Date']<split.test_start)]\n",
        "\n",
        "    test_start = max(split.test_start - to_timedelta(input_sequence_length, unit='day'), df['Date'].min())\n",
        "    test_df = df[(df['Date']>=test_start) & (df['Date']<=split.test_end)]\n",
        "\n",
        "    print(f'Shapes: train {train_df.shape}, validation {val_df.shape}, test {test_df.shape}.')\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def scale_data(train_df, val_df, test_df, features, targets):\n",
        "    feature_scaler = StandardScaler()\n",
        "    train_df[features] = feature_scaler.fit_transform(train_df[features])\n",
        "    val_df[features] = feature_scaler.transform(val_df[features])\n",
        "    test_df[features] = feature_scaler.transform(test_df[features])\n",
        "\n",
        "    target_scaler = StandardScaler()\n",
        "    train_df[targets] = target_scaler.fit_transform(train_df[targets])\n",
        "    val_df[targets] = target_scaler.transform(val_df[targets])\n",
        "    test_df[targets] = target_scaler.transform(test_df[targets])\n",
        "\n",
        "    return train_df, val_df, test_df, feature_scaler, target_scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf0nwz_qELVD",
        "outputId": "23b9687e-e6fc-4867-eb5a-ea8412cf968d"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, test_df = split_data(df, Split, input_sequence_length)\n",
        "train_df, val_df, test_df, feature_scaler, target_scaler = scale_data(\n",
        "    train_df, val_df, test_df, Config.features, targets\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy_Jq3uSELVE"
      },
      "source": [
        "## Window generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6z1tjVkELVE"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def prepare_dataset(\n",
        "    df:DataFrame, config:dataclass, \n",
        "    disable_progress_bar:bool=False\n",
        "    ):\n",
        "    data, labels = [], []\n",
        "    assert df.shape[0] >= (config.input_sequence_length + config.output_sequence_length), f\"Data size ({df.shape[0]}) too small for a complete sequence\"\n",
        "\n",
        "    for (_, county) in tqdm(df.groupby(config.group_id), disable=disable_progress_bar):\n",
        "        feature_df = county[config.selected_columns]\n",
        "        target_df = county[config.targets]\n",
        "\n",
        "        for index in range(config.input_sequence_length, county.shape[0]-config.output_sequence_length+1):\n",
        "            indices = range(index-config.input_sequence_length, index)\n",
        "            data.append(feature_df.iloc[indices])\n",
        "            \n",
        "            indices = range(index, index + config.output_sequence_length)\n",
        "            labels.append(target_df.iloc[indices])\n",
        "\n",
        "    data = np.array(data).reshape((len(data), -1, len(config.selected_columns)))\n",
        "    labels = np.array(labels).reshape((len(labels), -1))\n",
        "    print(f'Shapes: data {data.shape}, labels {labels.shape}.')\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_AYybFIELVE",
        "outputId": "8aa42b3c-460b-4517-ec36-85577287c291"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = prepare_dataset(\n",
        "    train_df, Config, disable_progress_bar=(VERBOSE!=1)\n",
        ")\n",
        "x_val, y_val = prepare_dataset(\n",
        "    val_df, Config, disable_progress_bar=(VERBOSE!=1)\n",
        ")\n",
        "x_test, y_test = prepare_dataset(\n",
        "    test_df, Config, disable_progress_bar=(VERBOSE!=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHY_6InLELVE"
      },
      "source": [
        "## Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cache_data(\n",
        "    x, y, batch_size:int=64, buffer_size:int=None\n",
        "):\n",
        "    data = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    if buffer_size is None:\n",
        "        return data.cache().batch(batch_size)\n",
        "    return data.cache().shuffle(buffer_size).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl7NGbFBELVF"
      },
      "outputs": [],
      "source": [
        "train_data = cache_data(\n",
        "    x_train, y_train, batch_size=Config.batch_size, \n",
        "    buffer_size=Config.buffer_size\n",
        ")\n",
        "val_data = cache_data(\n",
        "    x_val, y_val, batch_size=Config.batch_size, \n",
        ")\n",
        "test_data = cache_data(\n",
        "    x_test, y_test, batch_size=Config.batch_size, \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Im3Z1QELVF"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q8eFRhMELVF"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OleHLkDKELVF"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import StrMethodFormatter, MultipleLocator\n",
        "\n",
        "def plot_train_history(\n",
        "    history, title:str, figure_path:str=None, \n",
        "    figsize=FIGSIZE, base:int=None, show_image:bool=True\n",
        "    ):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    loss_formatter = StrMethodFormatter('{x:,.3f}')\n",
        "    plt.gca().yaxis.set_major_formatter(loss_formatter)\n",
        "\n",
        "    if base is not None:\n",
        "        plt.gca().xaxis.set_major_locator(MultipleLocator(base=base))\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(figure_path, dpi=DPI)\n",
        "\n",
        "    if show_image:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOHTAa2eELVF"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xD0Nwu9ELVF"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def build_LSTM(\n",
        "    input_shape: Tuple[int],  output_size:int, loss:str='mse', \n",
        "    summarize:bool=False, learning_rate:float=1e-5  \n",
        "    ):\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=input_shape, return_sequences=True),\n",
        "        Dropout(0.1),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        LSTM(32),\n",
        "        Dense(32),\n",
        "        Dense(output_size)\n",
        "    ])\n",
        "    \n",
        "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=loss, optimizer=adam)\n",
        "    if summarize:\n",
        "        model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_size = len(targets) * output_sequence_length\n",
        "model = build_LSTM(\n",
        "    x_train.shape[1:], output_size=output_size, loss=Config.loss, \n",
        "    summarize=True, learning_rate=Config.learning_rate\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    patience = Config.early_stopping_patience, \n",
        "    restore_best_weights=True\n",
        ")\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=os.path.join(output_folder, 'model.h5'), \n",
        "    save_best_only=True, save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHDsKOloELVG",
        "outputId": "048a11f0-f64f-4e8f-8973-98571f9cc14b"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_data, epochs=Config.epochs, validation_data=val_data, \n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "gc.collect()\n",
        "model.load_weights(model_checkpoint.filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODQwnffELVG"
      },
      "source": [
        "## History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "4JDLrJgEELVG",
        "outputId": "ce32e695-58f9-43a0-8370-864d421d538e"
      },
      "outputs": [],
      "source": [
        "plot_train_history(\n",
        "    history, title='Multi-Step, Multi-Output Training and Validation Loss', \n",
        "    figure_path=os.path.join(output_folder, 'history.jpg'), show_image=SHOW_IMAGE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UQ_PFbQELVG"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwTPgfG-ELVG"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31XIBe85ELVG"
      },
      "source": [
        "### Process prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOFXuk54ELVG"
      },
      "outputs": [],
      "source": [
        "def process_prediction(\n",
        "    target_df:DataFrame, y_pred:np.ndarray,\n",
        "    config:dataclass\n",
        "):\n",
        "    counties = []\n",
        "    prediction_counter = 0\n",
        "    targets = config.targets\n",
        "    group_id = config.group_id\n",
        "\n",
        "    zeroes_df = DataFrame(np.zeros_like(target_df[targets]), columns=targets)\n",
        "    zeroes_df[group_id] = target_df[group_id].values\n",
        "    \n",
        "    for (fips, county) in zeroes_df.groupby(group_id):\n",
        "        df = county[config.targets].reset_index(drop=True)\n",
        "        # keeps counter of how many times each index appeared in prediction\n",
        "        indices_counter = np.zeros(df.shape[0])\n",
        "\n",
        "        for index in range(config.input_sequence_length, df.shape[0]-config.output_sequence_length+1):\n",
        "            indices = range(index, index + config.output_sequence_length)\n",
        "            df.loc[indices] += y_pred[prediction_counter]\n",
        "            indices_counter[indices] += 1\n",
        "            prediction_counter += 1\n",
        "\n",
        "        for index in range(config.input_sequence_length+1, df.shape[0]-1):\n",
        "            if indices_counter[index] > 0:\n",
        "                df.loc[index] /= indices_counter[index]\n",
        "\n",
        "        df[group_id] = fips\n",
        "        counties.append(df)\n",
        "\n",
        "    prediction_df = concat(counties, axis=0).reset_index(drop=True)\n",
        "    \n",
        "    for target in targets:\n",
        "        # target values here can not be negative\n",
        "        prediction_df[prediction_df[target]<0] = 0\n",
        "        # both case and death can only be an integer number\n",
        "        prediction_df[target] = prediction_df[target].round() \n",
        "\n",
        "        prediction_df.rename(columns={target: f'Predicted_{target}'}, inplace=True)\n",
        "\n",
        "    prediction_df.drop(group_id, axis=1, inplace=True)\n",
        "    # now attach the predictions to the ground truth dataframe along column axis\n",
        "    # need to better generalize for seriality (should join on date/time index too)\n",
        "    prediction_df = concat([target_df, prediction_df], axis=1)\n",
        "\n",
        "    # drop the input_sequence_length timesteps, since prediction starts after that\n",
        "    prediction_start_date = prediction_df['Date'].min()+ to_timedelta(config.input_sequence_length + 1, unit='D')\n",
        "    prediction_df = prediction_df[prediction_df['Date']>prediction_start_date]\n",
        "\n",
        "    return prediction_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0j07JPqELVH"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnZrzPG-ELVH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
        "\n",
        "# https://en.wikipedia.org/wiki/Nash%E2%80%93Sutcliffe_model_efficiency_coefficient\n",
        "def normalized_nash_sutcliffe_efficiency(y_true, y_pred):\n",
        "    NSE = 1 - sum (np.square(y_true - y_pred) ) / sum( np.square(y_true - np.mean(y_true)) )\n",
        "    return 1 / ( 2 - NSE)\n",
        "\n",
        "# https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.metrics.point.SMAPE.html?highlight=smape\n",
        "def symmetric_mean_absolute_percentage(y_true, y_pred):\n",
        "    numerator = 2*abs(y_true - y_pred)\n",
        "    denominator = abs(y_true) + abs(y_pred)\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        result = numerator / denominator\n",
        "        result[denominator == 0] = 0\n",
        "    \n",
        "    return np.mean(result)\n",
        "\n",
        "def calculate_result(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "    smape = symmetric_mean_absolute_percentage(y_true, y_pred)\n",
        "    nnse = normalized_nash_sutcliffe_efficiency(y_true, y_pred)\n",
        "\n",
        "    return mae, rmse, rmsle, smape, nnse\n",
        "\n",
        "def show_result(df: DataFrame, targets:List[str]):    \n",
        "    for target in targets:\n",
        "        predicted_column = 'Predicted_'+ target\n",
        "        y_true, y_pred = df[target].values, df[predicted_column].values\n",
        "\n",
        "        mae, rmse, rmsle, smape, nnse = calculate_result(y_true, y_pred)\n",
        "        print(f'Target {target}, MAE {mae:.5g}, RMSE {rmse:.5g}, RMSLE {rmsle:0.5g}, SMAPE {smape:0.5g}. NNSE {nnse:0.5g}.')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQd0BHtWELVH"
      },
      "source": [
        "### Plot prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1MkMmQeELVH"
      },
      "outputs": [],
      "source": [
        "def plot_predition(\n",
        "    df:DataFrame, target:str, plot_error:bool=True, \n",
        "    figure_path:str=None, figsize=FIGSIZE, show_image:bool=True\n",
        "):\n",
        "    x_major_ticks = DATE_TICKS\n",
        "    predicted = 'Predicted_'+ target\n",
        "\n",
        "    # make sure to do this before the aggregation\n",
        "    mae, rmse, rmsle, smape, nnse = calculate_result(df[target].values, df[predicted].values)\n",
        "    title = f'{target} MAE {mae:0.3g}, RMSE {rmse:0.4g}, RMSLE {rmsle:0.3g}, SMAPE {smape:0.3g}, NNSE {nnse:0.3g}'\n",
        "\n",
        "    df = df.groupby('Date')[\n",
        "        [target, predicted]\n",
        "    ].aggregate('sum').reset_index()\n",
        "    \n",
        "    _, ax = plt.subplots(figsize=figsize)\n",
        "    plt.title(title)\n",
        "    plt.plot(df['Date'], df[target], color='blue', label='Ground Truth')\n",
        "    plt.plot(df['Date'], df[predicted], color='green', label='Predicted')\n",
        "    if plot_error:\n",
        "        plt.plot(df['Date'], abs(df[target] - df[predicted]), color='red', label='Error')\n",
        "    ax.set_ylim(0, ax.get_ylim()[-1]*1.05)\n",
        "\n",
        "    label_text, scale, unit = [], 1e3, 'K'\n",
        "    for loc in plt.yticks()[0]:\n",
        "        if loc == 0:\n",
        "            label_text.append('0')\n",
        "        else:\n",
        "            label_text.append(f'{loc/scale:0.5g}{unit}')\n",
        "        \n",
        "    ax.set_yticks(plt.yticks()[0])\n",
        "    ax.set_yticklabels(label_text)\n",
        "    \n",
        "    plt.ylabel(f'Daily {target}') \n",
        "\n",
        "    x_first_tick = df['Date'].min()\n",
        "    x_last_tick = df['Date'].max()\n",
        "    ax.set_xticks(\n",
        "        [x_first_tick + (x_last_tick - x_first_tick) * i / (x_major_ticks - 1) for i in range(x_major_ticks)]\n",
        "    )\n",
        "\n",
        "    if plot_error:\n",
        "        plt.legend(framealpha=0.3, edgecolor=\"black\", ncol=3)\n",
        "    else:\n",
        "        plt.legend(framealpha=0.3, edgecolor=\"black\", ncol=2)\n",
        "    \n",
        "    if figure_path is not None:\n",
        "        plt.savefig(figure_path, dpi=200)\n",
        "\n",
        "    if show_image:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueA3SdtBELVH"
      },
      "source": [
        "## Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bcuucBBELVH",
        "outputId": "835862cb-acae-4460-9169-3126ffe596fa"
      },
      "outputs": [],
      "source": [
        "print('\\nTrain prediction')\n",
        "train_data = cache_data(\n",
        "    x_train, y_train, batch_size=Config.batch_size, \n",
        ")\n",
        "y_pred = model.predict(train_data, verbose=VERBOSE)\n",
        "\n",
        "# upscale prediction\n",
        "y_pred = target_scaler.inverse_transform(\n",
        "    y_pred.reshape((-1, len(targets)))\n",
        ").reshape((-1, output_sequence_length, len(targets)))\n",
        "\n",
        "# upscale ground truth\n",
        "target_df = train_df[[group_id, 'Date'] + targets].copy().reset_index(drop=True)\n",
        "target_df[targets] = target_scaler.inverse_transform(target_df[targets])\n",
        "\n",
        "# align predictions with ground truth\n",
        "train_prediction_df = process_prediction(target_df, y_pred, Config)\n",
        "print(train_prediction_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "WJNKwGRRELVH",
        "outputId": "9e126fa2-6d19-43b4-f92d-f67b99301a3d"
      },
      "outputs": [],
      "source": [
        "show_result(train_prediction_df, targets)\n",
        "for target in targets:\n",
        "    plot_predition(\n",
        "        train_prediction_df, target, show_image=SHOW_IMAGE,\n",
        "        figure_path=os.path.join(output_folder, f'Summed_{target}_Train.jpg')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHgxG9JCELVI"
      },
      "source": [
        "## Validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "iSGQfl_yELVI",
        "outputId": "3241226f-507e-4558-f373-aad6289e58bf"
      },
      "outputs": [],
      "source": [
        "print('\\nValidation prediction')\n",
        "y_pred = model.predict(val_data, verbose=VERBOSE)\n",
        "\n",
        "# upscale prediction\n",
        "y_pred = target_scaler.inverse_transform(\n",
        "    y_pred.reshape((-1, len(targets)))\n",
        ").reshape((-1, output_sequence_length, len(targets)))\n",
        "\n",
        "# upscale ground truth\n",
        "target_df = val_df[[group_id, 'Date'] + targets].copy().reset_index(drop=True)\n",
        "target_df[targets] = target_scaler.inverse_transform(target_df[targets])\n",
        "\n",
        "# align predictions with ground truth\n",
        "val_prediction_df = process_prediction(target_df, y_pred, Config)\n",
        "print(val_prediction_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "Z0aFF1pqELVI",
        "outputId": "128897be-3c44-41e6-a9b5-10f0f1e6ecdd"
      },
      "outputs": [],
      "source": [
        "show_result(val_prediction_df, targets)\n",
        "for target in targets:\n",
        "    plot_predition(\n",
        "        val_prediction_df, target, show_image=SHOW_IMAGE,\n",
        "        figure_path=os.path.join(output_folder, f'Summed_{target}_Validation.jpg')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPxtoBdGELVI"
      },
      "source": [
        "## Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "eItmvSNLELVI",
        "outputId": "df09dde6-2b61-4a95-a3d3-f28d80db0388"
      },
      "outputs": [],
      "source": [
        "print('\\nTest prediction')\n",
        "y_pred = model.predict(test_data, verbose=VERBOSE)\n",
        "\n",
        "# upscale prediction\n",
        "y_pred = target_scaler.inverse_transform(\n",
        "    y_pred.reshape((-1, len(targets)))\n",
        ").reshape((-1, output_sequence_length, len(targets)))\n",
        "\n",
        "# upscale ground truth\n",
        "target_df = test_df[[group_id, 'Date'] + targets].copy().reset_index(drop=True)\n",
        "target_df[targets] = target_scaler.inverse_transform(target_df[targets])\n",
        "\n",
        "# align predictions with ground truth\n",
        "test_prediction_df = process_prediction(target_df, y_pred, Config)\n",
        "print(test_prediction_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "AJPonPTVELVI",
        "outputId": "57135a40-0dcf-4853-bf97-8c58630a55d5"
      },
      "outputs": [],
      "source": [
        "show_result(test_prediction_df, targets)\n",
        "for target in targets:\n",
        "    plot_predition(\n",
        "        test_prediction_df, target=target, show_image=SHOW_IMAGE,\n",
        "        figure_path=os.path.join(output_folder, f'Summed_{target}_Test.jpg')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jj-oQOzELVJ"
      },
      "source": [
        "## Dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP-eJgv1ELVJ"
      },
      "outputs": [],
      "source": [
        "train_prediction_df['Split'] = 'train'\n",
        "val_prediction_df['Split'] = 'validation'\n",
        "test_prediction_df['Split'] = 'test'\n",
        "merged_df = pd.concat([train_prediction_df, val_prediction_df, test_prediction_df], axis=0)\n",
        "merged_df.to_csv(os.path.join(output_folder, 'predictions.csv'), index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "43fc5fbfa959c1c54ddf7d7acab30a2019a504b895513ba1ba722e7f395657c0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
