{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCsw1KjFX1PP"
   },
   "source": [
    "# Introduction\n",
    "This is modified a notebook version of the TFT model script written in TF2 folder. This works with both the old and new dataset.If you are switching dataset, be sure to change both input csv and `config.json` files. They can be found in the `TF2\\Notebooks\\2022_May` or `TF2\\Notebooks\\2021_Nov` folder.  This notebook used the input files in `TF2\\Notebooks\\2022_May` folder. You can recreate them using the `Data preparation` notebook in this same folder. For a fresh model training, remove any files from the `checkpoint` folder.\n",
    "\n",
    "It is modified to run on colab. If you want to run on your local machine, you can use the model scripts in the TF2 folder instead. Run the model on GPU or reduce `epochs` in `config.json`.\n",
    "\n",
    "The input dataset is splitted into three parts here based on the following dates\n",
    "\n",
    "* `Train`: From `2020-02-28` to `2021-11-22`\n",
    "* `Validation`: From `2021-11-23` to `2022-02-18`\n",
    "* `Test`: From `2022-02-19` to `2022-05-17`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvhjFLeL6GwE"
   },
   "source": [
    "# Initial Setup\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aq2WaGb23m4h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 10:06:46.967832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os, gc, json\n",
    "import pandas as pd\n",
    "from pandas import to_datetime\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Apply the default theme\n",
    "sns.set_theme()\n",
    "sns.set(font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( '..' )\n",
    "from script.utils import train_validation_test_split, scale_back, calculate_result, sumCases\n",
    "from Class.Trainer import Trainer\n",
    "from Class.ParameterManager import ParameterManager\n",
    "from Class.DataProcessor import DataProcessor\n",
    "from Class.Plotter import PlotResults, PlotWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHfi9sgDPca3",
    "outputId": "08eb578e-22f9-4e68-dda1-a86d1f6a55bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 10:07:06.247095: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-21 10:07:06.251029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-06-21 10:07:06.470228: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-21 10:07:06.470289: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (udc-ba27-18): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following if cuda is out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAXjXa01ZxMr"
   },
   "source": [
    "## Adding google drive\n",
    "Set `running_on_colab= True` if running on Google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMLUJkNto9s5",
    "outputId": "45fd13c2-5721-4a9d-9d95-9aa823bfb142"
   },
   "outputs": [],
   "source": [
    "running_on_colab = False\n",
    "\n",
    "if running_on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    %cd /content/drive/My Drive/Projects/Covid/TF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO8CwyyG6C5L"
   },
   "source": [
    "## Setting up folders\n",
    "The current folder has the following structure\n",
    "\n",
    "* 2022_May\n",
    "  * Population_cut.csv\n",
    "  * Total.csv\n",
    "  * Rurality_cut.csv\n",
    "* config_2022_May.json\n",
    "* output\n",
    "  * checkpoints\n",
    "  * figures\n",
    "\n",
    "## For new dataset\n",
    "You can collect either of the csv files for new dataset is in the `TF2\\Notebooks\\2022_May` folder. \n",
    "\n",
    "## For the old dataset \n",
    "Collect the csv files from the `TF2\\Notebooks\\2021_Nov` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class args:\n",
    "    # folder where the cleaned feature file are at\n",
    "    # dataPath = ''../2021_Nov/Population_cut.csv''\n",
    "    dataPath = '../2022_May/Population_cut.csv'\n",
    "\n",
    "    outputPath = '../output/'\n",
    "    configPath = '../config_2022_May.json'\n",
    "    checkpoint = os.path.join(outputPath, 'checkpoints')\n",
    "    restore = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "emPIKC5x40L8"
   },
   "outputs": [],
   "source": [
    "# output paths\n",
    "checkpoint_folder = args.checkpoint\n",
    "figure_folder = os.path.join(args.outputPath, \"figures\")\n",
    "\n",
    "# this eventually creates output folder if it doesn't exist\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(figure_folder):\n",
    "    os.makedirs(figure_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bV4r9ohA0vOW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config.json from ../config_2022_May.json\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading config.json from {args.configPath}')\n",
    "with open(args.configPath) as inputfile:\n",
    "    config = json.load(inputfile)\n",
    "    inputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL-GCKH1ChBC"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HQ0hkih1ulOo",
    "outputId": "35c9b4be-94df-4fdc-94ce-da7b264e59a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data from ../2022_May/Population_cut.csv\n",
      "Input feature file shape (405000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading input data from {args.dataPath}')\n",
    "df = pd.read_csv(args.dataPath)\n",
    "print(f'Input feature file shape {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nhixFDqFTqn5"
   },
   "outputs": [],
   "source": [
    "df['Date'] = to_datetime(df['Date']) \n",
    "df['FIPS'] = df['FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlzdcpMvQxEE"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column mappings: {'Static': ['AgeDist', 'AirPollution', 'HealthDisp'], 'ID': ['FIPS'], 'Time': ['TimeFromStart'], 'Target': ['Cases'], 'Future': ['LinearSpace', 'Constant', 'LinearTime', 'P2Time', 'P3Time', 'P4Time', 'CosWeekly', 'SinWeekly'], 'Known Regular': ['AgeDist', 'AirPollution', 'HealthDisp', 'DiseaseSpread', 'Transmission', 'VaccinationFull', 'SocialDist']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameterManager = ParameterManager(config)\n",
    "print(f'Column mappings: {parameterManager.col_mappings}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation test split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train data is 317000, validation 44000, test 44000\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data, target_scaler = train_validation_test_split(df, parameterManager, scale=True)\n",
    "print(f'Number train data is {train_data.shape[0]}, validation {validation_data.shape[0]}, test {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 10:07:12.359229: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-21 10:07:12.359627: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataProcessor = DataProcessor(\n",
    "    parameterManager.total_sequence_length, parameterManager.col_mappings, parameterManager.data_params\n",
    ")\n",
    "\n",
    "train_batch = dataProcessor.prepare_batch(train_data, train=True)\n",
    "validation_batch = dataProcessor.prepare_batch(validation_data)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history:dict,  figure_path:str=None, show=False):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 8), sharex=True)\n",
    "    x = range(1, len(history['train_loss'])+1)\n",
    "\n",
    "    # label_text   = [f'{int(loc/1000)}k' for loc in plt.yticks()[0]]\n",
    "    # ax.set_yticklabels(label_text)\n",
    "\n",
    "    ax[0].set_title('Training history')\n",
    "    ax[0].plot(x, history['train_loss'], color='blue', label='Train loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Mean Squared Error of daily covid cases')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].set_title('Validation history')\n",
    "    ax[1].plot(x, history['validation_loss'], color='green', label='Validation loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Mean Squared Error of daily covid cases')\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if figure_path:\n",
    "        plt.savefig(figure_path)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(parameterManager, disable_progress=False)\n",
    "model = trainer.create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = parameterManager.optimizer_params\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=optimizer_params['learning_rate'], clipnorm=optimizer_params['clipnorm']\n",
    ")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "checkpointManager = tf.train.CheckpointManager(checkpoint, checkpoint_folder, max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.restore and checkpointManager.latest_checkpoint:\n",
    "    model = trainer.load_from_checkpoint(checkpoint, checkpointManager.latest_checkpoint)\n",
    "    if model is None:\n",
    "        sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.fit(\n",
    "    model, optimizer, train_batch, validation_batch, checkpointManager, early_stopping_patience=parameterManager.early_stopping_patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, os.path.join(figure_folder, 'history.png'), show=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best checkpoint by validation loss\n",
    "model = trainer.load_from_checkpoint(checkpoint, checkpointManager.latest_checkpoint)\n",
    "if model is None:\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = dataProcessor.prepare_batch(train_data)\n",
    "train_preds, train_actuals, train_attn_weights = trainer.predict(model, train_batch)\n",
    "\n",
    "train_actuals = scale_back(train_actuals, target_scaler, parameterManager.target_sequence_length)\n",
    "train_preds = scale_back(train_preds, target_scaler, parameterManager.target_sequence_length)\n",
    "\n",
    "train_mae, train_rmse, train_smape = calculate_result(train_actuals, train_preds)\n",
    "print(f'Train MAE {train_mae}, RMSE {train_rmse}, SMAPE {train_smape}')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_preds, validation_actuals, _ = trainer.predict(model, validation_batch)\n",
    "\n",
    "validation_preds = scale_back(validation_preds, target_scaler, parameterManager.target_sequence_length)\n",
    "validation_actuals = scale_back(validation_actuals,  target_scaler, parameterManager.target_sequence_length)\n",
    "\n",
    "validation_mae, validation_rmse, validation_smape = calculate_result(validation_actuals, validation_preds)\n",
    "print(f'Validation MAE {validation_mae}, RMSE {validation_rmse}, SMAPE {validation_smape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = dataProcessor.prepare_batch(test_data)\n",
    "test_preds, test_actuals, _ = trainer.predict(model, test_batch)\n",
    "\n",
    "test_actuals = scale_back(test_actuals, target_scaler, parameterManager.target_sequence_length) \n",
    "test_preds = scale_back(test_preds, target_scaler, parameterManager.target_sequence_length)\n",
    "\n",
    "test_mae, test_rmse, test_smape = calculate_result(test_actuals, test_preds)\n",
    "print(f'Test MAE {test_mae}, RMSE {test_rmse}, SMAPE {test_smape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_locations = df[parameterManager.col_mappings['ID']].nunique().values[0]\n",
    "print(f'Number of locations {number_of_locations}')\n",
    "locs = df[parameterManager.col_mappings['ID']].iloc[:number_of_locations, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, predictions = sumCases(train_actuals, train_preds, number_of_locations)\n",
    "\n",
    "resultPlotter = PlotResults(targets, predictions, parameterManager.train_start, locs, figure_folder)\n",
    "plot_title = f'Summed plot (train) MAE {train_mae:0.3f}, RMSE {train_rmse:0.3f}'\n",
    "\n",
    "resultPlotter.makeSummedPlot(plot_title, figure_name='Summed plot - train', figsize=(24, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, predictions = sumCases(validation_actuals, validation_preds, number_of_locations)\n",
    "resultPlotter = PlotResults(targets, predictions, parameterManager.validation_start, locs, figure_folder)\n",
    "plot_title = f'Summed plot (Validation) MAE {validation_mae:0.3f}, RMSE {validation_rmse:0.3f}, SMAPE {validation_smape:0.3f}'\n",
    "\n",
    "resultPlotter.makeSummedPlot(plot_title, figure_name='Summed plot - validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, predictions = sumCases(test_actuals, test_preds, number_of_locations)\n",
    "PlotC = PlotResults(targets, predictions, parameterManager.test_start, locs, figure_folder)\n",
    "plot_title = f'Summed plot (Validation) MAE {validation_mae:0.3f}, RMSE {validation_rmse:0.3f}, SMAPE {validation_smape:0.3f}'\n",
    "\n",
    "PlotC.makeSummedPlot(plot_title, figure_name='Summed plot - test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = PlotWeights(parameterManager.col_mappings, train_attn_weights, figure_folder, show=True)\n",
    "\"\"\"## Static variables\"\"\"\n",
    "\n",
    "plotter.plot_static_weights()\n",
    "\n",
    "\"\"\"## Future known input\"\"\"\n",
    "\n",
    "plotter.plot_future_weights()\n",
    "\n",
    "\"\"\"## Observed weights\"\"\"\n",
    "\n",
    "plotter.plotObservedWeights()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "77iIQhnG6b2y",
    "IY2OVkjn6tQp"
   ],
   "name": "TFT on top 500 counties.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ml]",
   "language": "python",
   "name": "conda-env-.conda-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
